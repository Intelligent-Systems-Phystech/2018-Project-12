\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\bibliographystyle{plain}
%\NOREVIEWERNOTES
\title
    [Обучение машинного перевода без параллельных текстов] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Обучение машинного перевода без параллельных текстов}
\author
    [Рябов~Ф.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Рябов~Ф.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление
    [Рябов~Ф.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Работа выполнена при финансовой поддержке РФФИ, проект \No\,00-00-00000.
   Научный руководитель:  Стрижов~В.\,В.
   Авторы: А.В. Грабовой, О.Ю. Бахтеев, В.В. Стрижов, Eric Gaussier, координатор Малиновский Г.С.
    Консультант:  Бахтеев~О.\,Ю.}
%\email
%    {author@site.ru}
\organization
     {$^1$Московский физико-технический институт\par
      $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}

\abstract
    {Данная задача посвящена исследованию алгоритма обучения машинного перевода без параллельных предложений. Использование параллельных текстов для задачи машинного перевода требует слишком большой базы предложений всех переводимых языков, что является ресурсоемкой задачей для некоторых пар непохожих языков. Особенностью исследуемого алгоритма является то, что для перевода используется кодировние и декодирование текста во внутреннем представлении. Данный алгоритм использует единую модель нейронной сети Seq2Seq для перевода с одного языка на другой и обратно. Цель данного исследования заключается в том, чтобы сделать вектора скрытых пространств этих двух моделей как можно более похожими. Для демонстрации работоспособности метода будет использован вычислительный эксперимент машинного перевода между двумя похожими языками: русским и украинским.

    \bigskip
    \textbf{Ключевые слова}: \emph {машинный перевод, нейросеть, Seq2Seq}.}

\begin{document}

\maketitle

\section{Введение}
{Решается задача оптимизации системы машинного перевода без использования параллельных предложений, так как для некоторых пар языков получение таких пар предложений, а также и само обучение является ресурсоемкой задачей \cite{conneau2017word}. 

Существует ряд подходов к построению систем машинного перевода\cite{koehn2007moses,koehn2009statistical,wu2016google}. Предлагается использовать рекуррентные нейронные сети c короткой и долгой памятью и нейронные сети, в которых реализовано механизм внимания (attention). В данном методе используются нейронные сети, которые осуществляют перевод в два этапа: кодирование и декодирование. Такой метода называется Seq2Seq\cite{weiss2017sequence, sutskever2014sequence}.

Данная работа посвящена последнему методу последовательного перевода. Предлагается с помощью первой рекуррентной нейронной сети, основанной на долгой памяти перевести входящую последовательность в вектор, а с помощью второй перевести этот вектор в выходную последовательность на нужном нам языке\cite{cho2014properties}. Предлагается модель, которая оптимизируется таким образом, чтобы скрытые пространства для векторов предложений двух языков совпадали.
 Данный метод позволяет обучить нейронную сеть переводу с одного языка на другой, в связи с использованием ей предыдущего опыта и наличию у нее памяти и внимания.

Эксперименты и анализ качества предложенного метода проводится на паре языков "русский-украинский" с помощью метрик BLEU(Bilingual evaluation understudy) для проверки качества систем машинного перевода\cite{papineni2002bleu}.}


\section{Постановка задачи}

Рассматриваемая модель отображает предложения из обоих языков в одно общее векторное пространство и из этого пространства в предложения первого или второго языка. Она делает это с помощью кодировщика $f$ и декодировщика $g$, в качестве которых выступают рекурентные нейронные сети.

Обозначим через $\mathfrak{D}^{\text{src}} = [\mathbf{s}_1^{\text{src}}, \dots, \mathbf{s}_{m_\text{src}}^{\text{src}}]$ корпус предложений из первого языка, через $\mathfrak{D}^{\text{tgt}} = [\mathbf{s}_1^{\text{tgt}}, \dots, \mathbf{s}_{m_\text{tgt}}^{\text{tgt}}]$ -- корпус предложений из второго языка, не являющиеся параллельными. Так как во время обучения не используются пары параллельных предложений, то требуется добиться того, чтобы автокодировщики вели себя похожим образом. Этого можно добиться, потребовав сходство скрытых векторных пространств. Для этого предлагается использовать сеть-дискриминатор $q$, которая по векторному представлению $h$ некоторого предложения определяет, какому языку оно принадлежит. Функция потерь записывается таким образом, чтобы оптимизировать дискриминатор $r$ для распознавания представлений предложений, а автокодировщик -- для генерации максимально похожих представлений слов из разных языков. Для борьбы с переобучением кодировщика используется механизм зашумления. Пусть $\sigma(x)$ - результат наложения шума на слово x.  Оптимизировать будем  следующую функцию:

$$L_{AE} = ||d(e(\sigma(x)))-x||^2$$

Пусть дана какая-то модель слабого перевода $\hat{g}$. На втором шаге  функция потерь будет иметь вид:

$$L_{TR} = ||d(e(\hat{g}(e(x))) - x||^2$$

Пусть дана модель D, различающая скрытые представления векторов предложений из двух языков. На последнем шаге оптимизируем дискримантор, чтобы он различал представления векторов разных языков в скрытом пространстве:

$$L_{ADV} = \log p(lang = src| Encoder(x)) + \log p(lang = tgt|Encoder(y))$$

В итоге нужно минимизировать следующую функцию:

$$L = a*L_{AE} + b*L_{TR}+c*L{ADV} \longrightarrow min$$

здесь $a,b,c$ - калибруемые гиперпараметры.



\bibliography{bibliography}
%\linenumbers

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
