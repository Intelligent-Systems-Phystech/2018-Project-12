\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\bibliographystyle{plain}
%\NOREVIEWERNOTES
\title
    [Обучение машинного перевода без параллельных текстов] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Обучение машинного перевода без параллельных текстов}
\author
    [Рябов~Ф.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Рябов~Ф.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление
    [Рябов~Ф.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Работа выполнена при финансовой поддержке РФФИ, проект \No\,00-00-00000.
   Научный руководитель:  Стрижов~В.\,В.
   Авторы: А.В. Грабовой, О.Ю. Бахтеев, В.В. Стрижов, Eric Gaussier, координатор Малиновский Г.С.
    Консультант:  Бахтеев~О.\,Ю.}
%\email
%    {author@site.ru}
\organization
     {$^1$Московский физико-технический институт\par
      $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}

\abstract
    {Данная задача посвящена исследованию алгоритма обучения машинного перевода без параллельных предложений. Использование параллельных текстов для задачи машинного перевода требует слишком большой базы предложений всех переводимых языков, что является ресурсоемкой задачей для некоторых пар непохожих языков. Особенностью исследуемого алгоритма является то, что для перевода используется кодировние и декодирование текста во внутреннем представлении. Данный алгоритм использует единую модель нейронной сети Seq2Seq для перевода с одного языка на другой и обратно. Цель данного исследования заключается в том, чтобы сделать вектора скрытых пространств этих двух моделей как можно более похожими. Для демонстрации работоспособности метода будет использован вычислительный эксперимент машинного перевода между двумя похожими языками: русским и украинским.

    \bigskip
    \textbf{Ключевые слова}: \emph {машинный перевод, нейросеть, Seq2Seq}.}

\begin{document}

\maketitle

\section{Введение}
{Решается задача оптимизации системы машинного перевода без использования параллельных предложений, так как для некоторых пар языков получение таких пар предложений, а также и само обучение является ресурсоемкой задачей \cite{conneau2017word}. 

Существует ряд подходов к построению систем машинного перевода\cite{koehn2007moses,koehn2009statistical,wu2016google}. Предлагается использовать рекуррентные нейронные сети c короткой и долгой памятью и нейронные сети, в которых реализовано механизм внимания (attention). В данном методе используются нейронные сети, которые осуществляют перевод в два этапа: кодирование и декодирование. Такой метода называется Seq2Seq\cite{weiss2017sequence, sutskever2014sequence}.

Данная работа посвящена последнему методу последовательного перевода. Предлагается с помощью первой рекуррентной нейронной сети, основанной на долгой памяти перевести входящую последовательность в вектор, а с помощью второй перевести этот вектор в выходную последовательность на нужном нам языке\cite{cho2014properties}. Предлагается модель, которая оптимизируется таким образом, чтобы скрытые пространства для векторов предложений двух языков совпадали.
 Данный метод позволяет обучить нейронную сеть переводу с одного языка на другой, в связи с использованием ей предыдущего опыта и наличию у нее памяти и внимания.

Эксперименты и анализ качества предложенного метода проводится на паре языков "русский-украинский" с помощью метрик BLEU(Bilingual evaluation understudy) для проверки качества систем машинного перевода\cite{papineni2002bleu}.}


\section{Постановка задачи}

Рассматривается модель отображения из двух языков в одно общее векторное пространство и из этого пространства в предложения первого или второго языка. Модель состоит из кодировищка $\mathbf{f}$ и декодировщика $\mathbf{g}$, в качестве которых выступают рекурентные нейронные сети. Есть кодировщики  $f^{\text{src}}$  и $f^{\text{tgt}}$,
и такие же декодеровщики $g^{\text{src}}$ и $g^{\text{tgt}}$. Они имеют общие параметры, но отличаются различными входными словарями.
\vspace{\baselineskip}


Обозначим через $\mathfrak{D}^{\text{src}} = [\mathbf{s}_1^{\text{src}}, \dots, \mathbf{s}_{m_\text{src}}^{\text{src}}]$ корпус предложений из первого языка, через $\mathfrak{D}^{\text{tgt}} = [\mathbf{s}_1^{\text{tgt}}, \dots, \mathbf{s}_{m_\text{tgt}}^{\text{tgt}}]$ -- корпус предложений из второго языка, не являющиеся параллельными. Так как во время проведения оптимизации не используются пары параллельных предложений, то требуется добиться того, чтобы автокодировщики вели себя похожим образом. Этого можно добиться, потребовав сходство скрытых векторных пространств, чтобы модели $f^{\text{src}}$  и $f^{\text{tgt}}$ отображали предложения в общее пространство.
. Для этого предлагается использовать сеть-дискриминатор $\mathbf{q}$, которая по векторному представлению $\mathbf{h}$ некоторого предложения определяет, какому языку оно принадлежит. Функция потерь является составной. Рассмотрим слагаемые минимизируемой функции.\begin{itemize}
\item $L_{AE}$
\item $L_{TR}$
\item $L_{ADV}$
\end{itemize}
 Функция потерь записывается таким образом, чтобы оптимизировать дискриминатор $\mathbf{q}$ для распознавания представлений предложений, а автокодировщик -- для генерации максимально похожих представлений слов из разных языков. Для борьбы с переобучением кодировщика используется механизм зашумления. Пусть $\sigma(s)$ - результат наложения шума на предложение s.  Оптимизировать будем  следующую функцию:

$$L_{AE} = ||g(f(\sigma(s)))-s||^2$$

Пусть дана какая-то модель слабого перевода $\hat{g}$. На втором шаге  функция потерь будет иметь вид:

$$L_{TR} = ||g(f(\hat{g}(f(s))) - s||^2$$

Пусть дана модель q, различающая скрытые представления векторов предложений из двух языков. На последнем шаге оптимизируем дискримантор, чтобы он различал представления векторов разных языков в скрытом пространстве:

$$L_{ADV} = \log p(lang = src| f(x)) + \log p(lang = tgt|f(y))$$

здесь lang - это метка языка предложения, которую выдает дискриминатор.
 	
\vspace{\baselineskip}
Итоговая функция оптимизации выгялдит следуюищм образом:

$$L = a*L_{AE} + b*L_{TR}+c*L{ADV} \longrightarrow min$$

здесь $a,b,c$ - калибруемые гиперпараметры.

\section{Базовый алгоритм}
\subsection{Получение слабого перевода}
Сгенерирован словарь пар слов на основе смежных слов в двух парах словарей "русско-английский" и "англо-украинский" из \cite{conneau2017word}. Данные словари можно считать реальными выборками. Далее строится алгоритм, который делит предложения для перевода на слова и пословно ищет значения в сгенерированном словаре, если значения не находятся, то просто возвращается оригинальное слово.

Оценка работы алгоритма проводится с помощью BLEU-метрики. Результаты работы базового алгоритма на реальной выборке из субтитров к одному фильму на паре языков "русский-украинский" оценен: $$BLEU = 10.86, 27.2/12.9/7.8/5.1 (BP=1.000, ratio=1.010, hyp\_len=3308640, ref\_len=3275742)$$


\subsection{Альтернативный метод}
Берутся два словаря из \cite{conneau2017word}, соотносящие каждому слову некоторое векторное представление, причем векторное пространство общее и синтетически размечено так, что для слов, являющимися реальными переводами друг друга в разных языках, векторное представление приблизительно одинаковое. Мы строим алгоритм, переводящий слово в векторное пространство и в нем ищем наиболее близкие векторные представления из другого языка. Расстояние оценивается по L2 норме.

\newpage
\bibliography{bibliography}
%\linenumbers

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
