{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных\n",
    "\n",
    "Флаг _-nc_ позволяет не скачивать файлы, если они уже есть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘train.lc.norm.tok.fr’ already there; not retrieving.\n",
      "\n",
      "File ‘train.lc.norm.tok.en’ already there; not retrieving.\n",
      "\n",
      "File ‘wiki.multi.en.vec’ already there; not retrieving.\n",
      "\n",
      "File ‘wiki.multi.fr.vec’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/train.lc.norm.tok.fr\n",
    "!wget -nc https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/train.lc.norm.tok.en\n",
    "!wget -nc https://s3.amazonaws.com/arrival/embeddings/wiki.multi.en.vec\n",
    "!wget -nc https://s3.amazonaws.com/arrival/embeddings/wiki.multi.fr.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение вспомогательных функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def read_sentences(path):\n",
    "    lines = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            lines.append(normalizeString(line))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Tracks info about known words, their indices and frequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.dummies = [\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"]\n",
    "        for i, name in enumerate(self.dummies):\n",
    "            self.word2index[name] = i\n",
    "            self.word2count[name] = 0\n",
    "        self.index2word = self.dummies.copy()\n",
    "        self.n_words = len(self.dummies)\n",
    "   \n",
    "    def add_list(self, list):\n",
    "        for s in list:\n",
    "            self.add_sentence(s)\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_seq(self, seq):\n",
    "        for word in seq:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:   \n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word.append(word)\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_format(seq, max_words):\n",
    "    nwords = len(seq)\n",
    "    seq_new = seq + [\"<EOS>\"]\n",
    "    seq_new += [\"<PAD>\" for i in range(max_words - nwords)]\n",
    "    return seq_new\n",
    "\n",
    "def in_lang(seq, lang):\n",
    "    for w in seq:\n",
    "        if not (w in lang.word2index):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def freq_words(seq, lang, freq):\n",
    "    for w in seq:\n",
    "        if lang.word2count[w] < freq:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Leave only known words and given length\n",
    "def prepare_list(list, lang, max_words, freq):\n",
    "    list_seq = [s.split() for s in list]\n",
    "    list_clean = []\n",
    "    for s in list_seq:\n",
    "        if in_lang(s, lang) and len(s) <= max_words and freq_words(s, lang, freq):\n",
    "            list_clean.append(s)\n",
    "        else:\n",
    "            list_clean.append(None)\n",
    "    \n",
    "    list_format = []\n",
    "    for s in list_clean:\n",
    "        if s == None:\n",
    "            list_format.append(s)\n",
    "        else:\n",
    "            list_format.append(seq_format(s, max_words))\n",
    "            \n",
    "    return list_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2vec(seq, lang):\n",
    "    return [lang.word2index[w] for w in seq]\n",
    "\n",
    "def words_list2tensor(list, lang):\n",
    "    ind_list = [seq2vec(x, lang) for x in list]\n",
    "    vec_list = [torch.LongTensor(s) for s in ind_list]\n",
    "    return torch.stack(vec_list)\n",
    "\n",
    "def ind_seq2word_seq(seq, lang):\n",
    "    return [lang.index2word[v] for v in seq]\n",
    "\n",
    "def vec_seq2word_seq(seq, lang):\n",
    "    return [lang.vec2word(v) for v in seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_sents = read_sentences('train.lc.norm.tok.fr')\n",
    "en_sents = read_sentences('train.lc.norm.tok.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fra = Vocabulary('fr')\n",
    "eng = Vocabulary('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fra.add_list(fr_sents)\n",
    "eng.add_list(en_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs left: 3537\n",
      "French non-paired: 937\n",
      "English non-paired: 3052\n",
      "--> Garbage: 24526\n"
     ]
    }
   ],
   "source": [
    "freq = 5\n",
    "max_len = 10\n",
    "\n",
    "# Format sentences\n",
    "fr_clean = prepare_list(fr_sents, fra, max_len, freq)\n",
    "en_clean = prepare_list(en_sents, eng, max_len, freq)\n",
    "\n",
    "fr_pair = []\n",
    "en_pair = []\n",
    "fr_nopair = []\n",
    "en_nopair = []\n",
    "\n",
    "for s_fr, s_en in zip(fr_clean, en_clean):\n",
    "    if s_en == None and s_fr == None:\n",
    "        continue\n",
    "    if s_en == None:\n",
    "        fr_nopair.append(s_fr)\n",
    "    elif s_fr == None:\n",
    "        en_nopair.append(s_en)\n",
    "    else:\n",
    "        fr_pair.append(s_fr) \n",
    "        en_pair.append(s_en)\n",
    "        \n",
    "print('Pairs left:', len(fr_pair))\n",
    "print('French non-paired:', len(fr_nopair))\n",
    "print('English non-paired:', len(en_nopair))\n",
    "print('--> Garbage:', len(fr_clean) - len(fr_pair) - len(fr_nopair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переводчик слово-в-слово с attention и софтмаксом на выходах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassTranslator(nn.Module):\n",
    "    def __init__(self, in_max_len, in_nlabels, out_max_len, out_nlabels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.emb_src = nn.Embedding(in_nlabels, hidden_dim)\n",
    "        self.in_max_len = in_max_len\n",
    "        self.out_max_len = out_max_len\n",
    "        \n",
    "        self.shrink = nn.Linear(self.emb_src.embedding_dim, hidden_dim)\n",
    "        self.cont = nn.Linear(in_max_len, out_max_len)\n",
    "#         self.select_cont = nn.Linear(hidden_dim, 1)\n",
    "        self.reduce = nn.Linear(out_max_len, 1)\n",
    "        self.attn_matr = nn.Linear(hidden_dim, in_max_len*out_max_len)\n",
    "        self.non_lin = nn.ReLU()\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, out_nlabels)\n",
    "        \n",
    "    def forward(self, in_sent):\n",
    "        # Encode\n",
    "        input = self.emb_src(in_sent)\n",
    "        context = self.cont(input.transpose(1, 2))\n",
    "        shrinked = self.non_lin(self.shrink(context.transpose(1, 2)))\n",
    "        \n",
    "        # Decode\n",
    "#         selected = torch.softmax(self.select_cont(shrinked), dim=1)\n",
    "#         attn = self.non_lin(self.attn_matr(shrinked))\n",
    "#         masked = torch.bmm(selected.transpose(1, 2), attn)\n",
    "#         out = torch.squeeze(masked).view(-1, self.out_max_len, self.in_max_len)\n",
    "#         attn = torch.softmax(out, dim=2)\n",
    "#         attn_applied = torch.bmm(attn, shrinked)\n",
    "\n",
    "        reduced = self.non_lin(self.reduce(shrinked.transpose(1, 2)))\n",
    "        attn = self.non_lin(self.attn_matr(reduced.transpose(1, 2)))\n",
    "        out = attn.view(-1, self.out_max_len, self.in_max_len)\n",
    "        attn = torch.softmax(out, dim=2)\n",
    "        attn_applied = torch.bmm(attn, shrinked)\n",
    "    \n",
    "        out = self.classifier(attn_applied)\n",
    "        out = torch.log_softmax(out, dim=2)\n",
    "        \n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ClassTranslator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(9.0293, device='cuda:0')\n",
      "['a', 'soccer', 'player', 'is', 'kicking', 'the', 'ball', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "['wearing', 'wearing', 'wearing', 'wearing', 'wearing', 'wearing', 'wearing', 'wearing', 'wearing', 'wearing', 'wearing']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f489a554c797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 1000\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "X = words_list2tensor(fr_pair, fra)\n",
    "Y = words_list2tensor(en_pair, eng)\n",
    "\n",
    "n_samples, max_len = X.shape\n",
    "hidden_dim = 5\n",
    "\n",
    "LOAD = False\n",
    "\n",
    "if LOAD:\n",
    "    tr = torch.load('class_tr')\n",
    "else:\n",
    "    tr = ClassTranslator(max_len, len(fra.index2word),\n",
    "                         max_len, len(eng.index2word),\n",
    "                         hidden_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(tr.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                 milestones=[], \n",
    "                                                 gamma=0.1)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    X = X.cuda()\n",
    "    Y = Y.cuda()\n",
    "    tr = tr.cuda()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    inds = torch.randperm(n_samples).split(batch_size)\n",
    "    for ind in inds:\n",
    "        X_batch = X[ind]\n",
    "        Y_batch = Y[ind]\n",
    "        \n",
    "        out, attn = tr(X_batch)\n",
    "        \n",
    "        loss = criterion(out.transpose(1, 2), Y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step(loss.data)\n",
    "    if not epoch % 50: \n",
    "        torch.save(tr, 'class_tr')\n",
    "        print(epoch, loss.data)\n",
    "        print(ind_seq2word_seq(Y_batch[0].cpu(), eng))\n",
    "        _, ind = out[0].cpu().topk(1, dim=1)\n",
    "        print(ind_seq2word_seq(ind, eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тупой переводчик: трёхслойная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StupidTranslator(nn.Module):\n",
    "    def __init__(self, in_max_len, in_nlabels, out_max_len, out_nlabels, hidden_dim, tmp_len=5):\n",
    "        super().__init__()\n",
    "        self.emb_src = nn.Embedding(in_nlabels, hidden_dim)\n",
    "        self.in_max_len = in_max_len\n",
    "        self.out_max_len = out_max_len\n",
    "        \n",
    "        self.W = nn.Linear(in_max_len, tmp_len)\n",
    "        self.U = nn.Linear(tmp_len, tmp_len)\n",
    "        self.Q = nn.Linear(tmp_len, out_max_len)\n",
    "        self.non_lin = nn.ReLU()\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, out_nlabels)\n",
    "        \n",
    "    def forward(self, in_sent):\n",
    "        # Encode\n",
    "        input = self.emb_src(in_sent)\n",
    "        \n",
    "        out = self.non_lin(self.W(input.transpose(1, 2)))\n",
    "        out = self.non_lin(self.U(out))\n",
    "        out = self.non_lin(self.Q(out))\n",
    "    \n",
    "        out = self.classifier(out.transpose(1, 2))\n",
    "        out = torch.log_softmax(out, dim=2)\n",
    "        \n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type StupidTranslator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(8.8732, device='cuda:0')\n",
      "['a', 'girl', 'sits', 'on', 'a', 'swing', 'ride', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "['.', 'rock', 'rock', 'rock', 'rock', '.', 'scrolled', 'rock', '.', 'rock', 'scrolled']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d626e29c6d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpad_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpad_ind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_epochs = 1000\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "X = words_list2tensor(fr_pair, fra)\n",
    "Y = words_list2tensor(en_pair, eng)\n",
    "\n",
    "n_samples, max_len = X.shape\n",
    "hidden_dim = 5\n",
    "\n",
    "LOAD = False\n",
    "\n",
    "if LOAD:\n",
    "    tr = torch.load('stupid_tr')\n",
    "else:\n",
    "    tr = StupidTranslator(max_len, len(fra.index2word),\n",
    "                         max_len, len(eng.index2word),\n",
    "                         hidden_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(tr.parameters())\n",
    "pad_ind = eng.word2index['<PAD>']\n",
    "if device.type == \"cuda\":\n",
    "    X = X.cuda()\n",
    "    Y = Y.cuda()\n",
    "    tr = tr.cuda()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    inds = torch.randperm(n_samples).split(batch_size)\n",
    "    for ind in inds:\n",
    "        X_batch = X[ind]\n",
    "        Y_batch = Y[ind]\n",
    "        \n",
    "        out, attn = tr(X_batch)\n",
    "        \n",
    "        out[Y_batch == pad_ind][pad_ind] = 0\n",
    "        loss = criterion(out.transpose(1, 2), Y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if not epoch % 50: \n",
    "        torch.save(tr, 'stupid_tr')\n",
    "        print(epoch, loss.data)\n",
    "        print(ind_seq2word_seq(Y_batch[0].cpu(), eng))\n",
    "        _, ind = out[0].cpu().topk(1, dim=1)\n",
    "        print(ind_seq2word_seq(ind, eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переводчик вектор-в-вектор с натренированными заранее представлениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec(emb_path, max_words=-1):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    it = 0\n",
    "    with open(emb_path) as f:\n",
    "        # Skip first line\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            if max_words != -1 and it > max_words:\n",
    "                break\n",
    "            it += 1\n",
    "            orig_word, vect = line.rstrip().split(' ', 1)\n",
    "            \n",
    "            word = normalizeString(orig_word)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "       \n",
    "            # Words are sorted by frequency, no need to add less \n",
    "            # frequent version of the same word  \n",
    "            if not (word in word2id):\n",
    "                vectors.append(vect)\n",
    "                word2id[word] = len(word2id)\n",
    "\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, embedding_tuple):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.dummies = [\"<SOS>\", \"<EOS>\", \"<PAD>\"]\n",
    "        for i, name in enumerate(self.dummies):\n",
    "            self.word2index[name] = i\n",
    "        self.index2word = self.dummies.copy()\n",
    "        self.embedding_tuple = embedding_tuple    \n",
    "        self.n_words = len(self.dummies) # Count SOS and EOS\n",
    "        self.embedding = None # No initial embedding\n",
    "\n",
    "    def add_list(self, list):\n",
    "        for s in list:\n",
    "            self.addSentence(s)\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:   \n",
    "            if word in self.embedding_tuple[2]:\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word.append(word)\n",
    "                self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def build_embedding(self):\n",
    "        \"\"\"\n",
    "        Получаем матрицу слово -> вектор для всех слов, которые встретились в тексте.\n",
    "        Вектор для начала предложения заменяем нулевым, \n",
    "        для конца предложения --- единичным (можно заменить на случайный вектор).\n",
    "        \"\"\"\n",
    "        dim = self.embedding_tuple[0].shape[1]\n",
    "        # Add 2 dims for start/end\n",
    "        dlen = len(self.dummies)\n",
    "        matrix = np.zeros((self.n_words, dim+dlen))        \n",
    "        for i in range(dlen):\n",
    "            matrix[i, dim+i] = 1\n",
    "\n",
    "        for id, word in enumerate(self.index2word[dlen:]):\n",
    "            id = id+dlen\n",
    "            word_id = self.embedding_tuple[2][word]\n",
    "            vector = self.embedding_tuple[0][word_id]\n",
    "            matrix[id, :-dlen] = vector\n",
    "        \n",
    "        self.embedding = torch.Tensor(matrix)\n",
    "        \n",
    "    def word2vec(self, word):\n",
    "        return self.embedding[self.word2index[word]]\n",
    "    \n",
    "    def vec2word(self, vec):\n",
    "        tmp1 = self.embedding/torch.norm(self.embedding, dim=1, keepdim=True)\n",
    "        tmp2 = vec/torch.norm(vec)\n",
    "        dist = torch.mm(tmp1, tmp2[:, None])\n",
    "        _, ind = torch.topk(dist, 1, dim=0)\n",
    "        return self.index2word[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextTranslator(nn.Module):\n",
    "    def __init__(self, emb_src, emb_tgt, in_max_len, out_max_len, hidden_dim, ncontext):\n",
    "        super().__init__()\n",
    "        self.emb_src = emb_src\n",
    "        self.emb_tgt = emb_tgt\n",
    "        self.in_max_len = in_max_len\n",
    "        self.out_max_len = out_max_len\n",
    "        \n",
    "        self.shrink = nn.Linear(self.emb_src.embedding_dim, hidden_dim)\n",
    "        self.cont = nn.Linear(in_max_len, ncontext)\n",
    "        self.select_cont = nn.Linear(hidden_dim, 1)\n",
    "        self.attn_matr = nn.Linear(hidden_dim, in_max_len*out_max_len)\n",
    "        self.non_lin = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        context = self.cont(input.transpose(1, 2))\n",
    "        \n",
    "        shrinked = self.non_lin(self.shrink(context.transpose(1, 2)))\n",
    "        \n",
    "        selected = torch.softmax(self.select_cont(shrinked), dim=1)\n",
    "        attn = self.non_lin(self.attn_matr(shrinked))\n",
    "        \n",
    "        masked = torch.bmm(selected.transpose(1, 2), attn)\n",
    "        out = torch.squeeze(masked).view(-1, self.out_max_len, self.in_max_len)\n",
    "        attn = torch.softmax(out, dim=2)\n",
    "        attn_applied = torch.bmm(attn, input)\n",
    "        \n",
    "        return attn_applied, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embedding_tuple = load_vec('./wiki.multi.en.vec')\n",
    "fr_embedding_tuple = load_vec('./wiki.multi.fr.vec')\n",
    "fra = Lang('fr', fr_embedding_tuple)\n",
    "eng = Lang('en', en_embedding_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fra.add_list(fr_sents)\n",
    "fra.build_embedding()\n",
    "\n",
    "eng.add_list(en_sents)\n",
    "eng.build_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 5\n",
    "max_len = 10\n",
    "\n",
    "# Format sentences\n",
    "fr_clean = prepare_list(fr_sents, fra, max_len, freq)\n",
    "en_clean = prepare_list(en_sents, eng, max_len, freq)\n",
    "\n",
    "fr_pair = []\n",
    "en_pair = []\n",
    "fr_nopair = []\n",
    "en_nopair = []\n",
    "for s_fr, s_en in zip(fr_clean, en_clean):\n",
    "    if s_en == None and s_fr == None:\n",
    "        continue\n",
    "    if s_en == None:\n",
    "        fr_nopair.append(s_fr)\n",
    "    elif s_fr == None:\n",
    "        en_nopair.append(s_en)\n",
    "    else:\n",
    "        fr_pair.append(s_fr) \n",
    "        en_pair.append(s_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ContextTranslator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0025, device='cuda:0')\n",
      "['a', 'man', 'rollerblading', 'on', 'a', 'metal', 'bar', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "['a', 'a', 'a', 'a', 'roller', 'a', 'a', 'a', 'and', 'a', 'and']\n",
      "50 tensor(0.0022, device='cuda:0')\n",
      "['two', 'men', 'with', 'backpacks', 'wait', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['two', 'men', 'and', 'and', 'and', 'and', 'and', 'and', '<EOS>', '<PAD>', '<PAD>']\n",
      "100 tensor(0.0022, device='cuda:0')\n",
      "['two', 'girls', 'are', 'walking', 'along', 'the', 'street', 'and', 'talking', '.', '<EOS>']\n",
      "['two', 'girls', 'and', 'the', 'the', 'and', 'and', 'and', '.', '<EOS>', '<PAD>']\n",
      "150 tensor(0.0021, device='cuda:0')\n",
      "['a', 'little', 'boy', 'takes', 'a', 'picture', 'at', 'the', 'park', '.', '<EOS>']\n",
      "['a', 'small', 'girl', 'takes', 'a', 'photograph', 'the', 'the', '.', '<EOS>', '<PAD>']\n",
      "200 tensor(0.0021, device='cuda:0')\n",
      "['a', 'man', 'at', 'a', 'food', 'cart', 'is', 'serving', 'corn', '.', '<EOS>']\n",
      "['a', 'man', 'a', 'a', 'the', 'the', 'and', 'but', 'but', '<EOS>', '<EOS>']\n",
      "250 tensor(0.0022, device='cuda:0')\n",
      "['a', 'male', 'in', 'a', 'green', 'striped', 'tank', 'top', 'skateboarding', '.', '<EOS>']\n",
      "['a', 'man', 'and', 'overalls', 'green', 'and', 'and', 'and', '<EOS>', '.', '<EOS>']\n",
      "300 tensor(0.0021, device='cuda:0')\n",
      "['an', 'old', 'fashioned', 'car', 'is', 'coming', 'down', 'the', 'street', '.', '<EOS>']\n",
      "['a', 'old', 'car', 'car', 'the', 'the', 'street', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "350 tensor(0.0021, device='cuda:0')\n",
      "['a', 'man', 'sleeps', 'on', 'a', 'bench', 'behind', 'a', 'fountain', '.', '<EOS>']\n",
      "['a', 'man', 'sleeps', 'a', 'a', 'a', 'a', 'a', 'fountain', '<EOS>', '<EOS>']\n",
      "400 tensor(0.0022, device='cuda:0')\n",
      "['a', 'man', 'is', 'riding', 'a', 'red', 'racing', 'motorcycle', '.', '<EOS>', '<PAD>']\n",
      "['a', 'man', 'driver', 'a', 'motorcycle', 'the', 'the', 'and', '<EOS>', '<PAD>', '<PAD>']\n",
      "450 tensor(0.0022, device='cuda:0')\n",
      "['a', 'big', 'dog', 'catches', 'a', 'ball', 'on', 'his', 'nose', '<EOS>', '<PAD>']\n",
      "['a', 'grand', 'dog', 'a', 'a', 'ball', 'a', 'nose', '.', '<EOS>', '<EOS>']\n",
      "500 tensor(0.0021, device='cuda:0')\n",
      "['a', 'man', 'walking', 'in', 'a', 'white', 'robe', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "['a', 'man', 'hunched', 'and', 'a', 'a', 'white', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "550 tensor(0.0021, device='cuda:0')\n",
      "['a', 'woman', 'is', 'walking', 'and', 'dragging', 'a', 'suitcase', '.', '<EOS>', '<PAD>']\n",
      "['a', 'woman', 'and', 'and', 'and', 'and', 'a', 'suitcase', '.', '<EOS>', '<PAD>']\n",
      "600 tensor(0.0021, device='cuda:0')\n",
      "['two', 'men', 'are', 'sitting', 'working', 'on', 'their', 'computers', '.', '<EOS>', '<PAD>']\n",
      "['two', 'men', 'are', 'and', 'and', 'and', 'and', 'their', '.', '<EOS>', '<EOS>']\n",
      "650 tensor(0.0020, device='cuda:0')\n",
      "['people', 'outside', 'a', 'house', 'in', 'snow', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['folks', 'onlookers', 'which', 'which', 'which', 'and', 'and', '.', '<PAD>', '<PAD>', '<PAD>']\n",
      "700 tensor(0.0020, device='cuda:0')\n",
      "['a', 'dog', 'leaps', 'of', 'a', 'dog', 'jump', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "['a', 'dog', 'leaping', 'by', 'which', 'a', 'and', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "750 tensor(0.0022, device='cuda:0')\n",
      "['a', 'girl', 'on', 'skis', 'in', 'snow', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['which', 'daughter', 'and', 'the', 'the', 'the', 'the', '.', '<EOS>', '<EOS>', '<PAD>']\n",
      "800 tensor(0.0021, device='cuda:0')\n",
      "['a', 'man', 'standing', 'on', 'a', 'green', 'boat', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "['a', 'man', 'seated', 'a', 'a', 'boat', 'green', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "850 tensor(0.0020, device='cuda:0')\n",
      "['a', 'boy', 'climbing', 'a', 'tree', 'smiling', 'for', 'the', 'picture', '.', '<EOS>']\n",
      "['a', 'girl', 'rappelling', 'a', 'tree', 'smiling', 'a', 'the', 'photograph', '.', '<EOS>']\n",
      "900 tensor(0.0021, device='cuda:0')\n",
      "['girls', 'are', 'preparing', 'food', 'to', 'eat', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['girls', 'prepare', 'these', 'these', 'and', 'eat', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "950 tensor(0.0022, device='cuda:0')\n",
      "['a', 'man', 'breakdancing', 'on', 'a', 'tiled', 'floor', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['a', 'man', 'the', 'the', 'a', 'a', 'a', 'underneath', '<EOS>', '<PAD>', '<PAD>']\n",
      "1000 tensor(0.0020, device='cuda:0')\n",
      "['an', 'older', 'woman', 'is', 'checking', 'her', 'watch', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "['a', 'woman', 'woman', 'woman', 'examining', 'her', 'showing', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "1050 tensor(0.0021, device='cuda:0')\n",
      "['many', 'motorcyclists', 'ride', 'along', 'the', 'street', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['many', 'many', 'and', 'the', 'the', 'the', 'and', 'the', '<EOS>', 'street', 'and']\n",
      "1100 tensor(0.0022, device='cuda:0')\n",
      "['little', 'boy', 'enjoying', 'a', 'day', 'at', 'the', 'park', '<EOS>', '<PAD>', '<PAD>']\n",
      "['a', 'small', 'girl', 'cheerful', 'a', 'day', 'the', 'park', '<EOS>', '<PAD>', '<PAD>']\n",
      "1150 tensor(0.0020, device='cuda:0')\n",
      "['an', 'asian', 'themed', 'restaurant', 'with', 'a', 'large', 'mounted', 'fish', '.', '<EOS>']\n",
      "['a', 'restaurant', 'restaurant', 'a', 'a', 'a', 'arched', '.', '<EOS>', '<EOS>', '<EOS>']\n",
      "1200 tensor(0.0020, device='cuda:0')\n",
      "['the', 'surfer', 'is', 'in', 'the', 'wave', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['the', 'surfer', 'is', 'the', 'the', 'the', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1250 tensor(0.0020, device='cuda:0')\n",
      "['a', 'crowd', 'of', 'boys', 'riding', 'a', 'vehicle', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "['a', 'crowd', 'the', 'girls', 'in', 'a', 'vehicle', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "1300 tensor(0.0021, device='cuda:0')\n",
      "['a', 'dog', 'is', 'chewing', 'on', 'a', 'metal', 'pole', '.', '<EOS>', '<PAD>']\n",
      "['a', 'dog', 'dog', 'a', 'a', 'a', 'strainer', 'aluminum', '.', '<EOS>', '<PAD>']\n",
      "1350 tensor(0.0020, device='cuda:0')\n",
      "['people', 'sing', 'with', 'large', 'maroon', 'books', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['folks', 'sings', 'and', 'and', 'and', 'and', 'and', '<EOS>', '<EOS>', '<PAD>', '<PAD>']\n",
      "1400 tensor(0.0021, device='cuda:0')\n",
      "['a', 'man', 'touches', 'the', 'side', 'of', 'his', 'face', '.', '<EOS>', '<PAD>']\n",
      "['a', 'man', 'which', 'the', 'the', 'the', 'the', 'face', '.', '<EOS>', '<PAD>']\n",
      "1450 tensor(0.0021, device='cuda:0')\n",
      "['two', 'young', 'males', 'working', 'in', 'a', 'garden', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "['two', 'youngsters', 'men', 'working', 'in', 'a', 'garden', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "1500 tensor(0.0020, device='cuda:0')\n",
      "['two', 'men', 'are', 'building', 'something', 'together', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['two', 'men', 'build', 'but', 'something', 'that', 'which', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1550 tensor(0.0021, device='cuda:0')\n",
      "['some', 'people', 'are', 'waiting', 'in', 'a', 'line', '.', '<EOS>', '<PAD>', '<PAD>']\n",
      "['folks', 'arrive', 'the', 'the', 'the', 'file', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1600 tensor(0.0020, device='cuda:0')\n",
      "['the', 'baseball', 'player', 'is', 'running', 'after', 'the', 'ball', '.', '<EOS>', '<PAD>']\n",
      "['the', 'the', 'player', 'the', 'player', 'the', 'the', 'the', '.', '<EOS>', '<PAD>']\n",
      "1650 tensor(0.0021, device='cuda:0')\n",
      "['bicycle', 'riders', 'in', 'number', 'riding', 'down', 'in', 'a', 'parade', '.', '<EOS>']\n",
      "['bicyclists', 'and', 'and', 'which', 'the', 'the', 'the', '.', '<EOS>', '<PAD>', '<PAD>']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-8c14002fb08e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_epochs = 1000\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "X = words_list2tensor(fr_pair, fra)\n",
    "Y = words_list2tensor(en_pair, eng)\n",
    "\n",
    "n_samples, max_len = X.shape\n",
    "hidden_dim = 5\n",
    "ncontext = 5\n",
    "\n",
    "en_emb = torch.nn.Embedding.from_pretrained(eng.embedding)\n",
    "fr_emb = torch.nn.Embedding.from_pretrained(fra.embedding)\n",
    "\n",
    "LOAD = False\n",
    "\n",
    "if LOAD:\n",
    "    tr = torch.load('cont_tr')\n",
    "else:\n",
    "    tr = ContextTranslator(fr_emb, en_emb, max_len, max_len, hidden_dim, ncontext)\n",
    "\n",
    "optimizer = torch.optim.Adam(tr.parameters())\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    en_emb = en_emb.cuda()\n",
    "    fr_emb = fr_emb.cuda()\n",
    "    X = X.cuda()\n",
    "    Y = Y.cuda()\n",
    "    tr = tr.cuda()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    inds = torch.randperm(n_samples).split(batch_size)\n",
    "    for ind in inds:\n",
    "        X_batch = fr_emb(X[ind])\n",
    "        Y_batch = en_emb(Y[ind])\n",
    "        \n",
    "        out, attn = tr(X_batch)\n",
    "        loss = criterion(out, Y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if not epoch % 50: \n",
    "        torch.save(tr, 'cont_tr')\n",
    "        print(epoch, loss.data)\n",
    "        print(vec_seq2word_seq(Y_batch[0].cpu(), eng))\n",
    "        print(vec_seq2word_seq(out[0].cpu(), eng))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
