{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Демо по мотивам статьи \n",
    "## Unsupervised Machine Translation Using Monolingual Corpora Only\n",
    "\n",
    "Основные отличия от алгоритма, описанного в статье:\n",
    "\n",
    "1. Небольшие различия в архитектуре (GRU вместо LSTM), другой оптимизатор, более простая модель дискриминатора\n",
    "2. Отстутсвие Attention в Decoder\n",
    "3. Не добавлял шум в автокодировщик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем параллельные предложения из multi30K сразу в предобработанном виде.\n",
    "\n",
    "Основная предобработка: слова и знаки препинания разделены пробелом, и некоторая работа со спецсимволами (типа '&')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/train.lc.norm.tok.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/train.lc.norm.tok.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head train.lc.norm.tok.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем MUSE-вектора, т.е. вектора для слов на разных языках, выровненные таким образом, чтобы косинусное расстояние между схожими словами на разных языках было невелико. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/arrival/embeddings/wiki.multi.en.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/arrival/embeddings/wiki.multi.fr.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт основных библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import codecs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из обучалки torch Seq2Seq: удаляем всё, кроме латинских букв и знаков препинания. \n",
    "    \n",
    "Диакритические знаки (черточки над буквами) также удаляем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizeString(u\"bonjour  je suis élève de l'institut'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция загрузки векторов MUSE из файла.\n",
    "Словари векторов содержат много мусорных слов (типа хэштегов), поэтому используем следующую логику:\n",
    "    \n",
    "1. Предобрабатываем слова, удаляем нелатинские символы\n",
    "2. Если после предобработки в словаре встретился дубликат:\n",
    "        a. Если текущее слово не изменилось после предобработки (т.е. оно скорее всего \"хорошее\"), то заменяем для данного слова в словаре вектор на текущий\n",
    "        b. Иначе пропускаем и идем дальше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec(emb_path):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            orig_word, vect = line.rstrip().split(' ', 1)\n",
    "            \n",
    "            word = normalizeString(orig_word)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            if word in word2id:\n",
    "                print u'word found twice: {0} ({1})'.format(word, orig_word)\n",
    "                if orig_word==word:\n",
    "                    id = word2id[word]\n",
    "                    vectors[id] = vect\n",
    "                    print 'rewriting'\n",
    "                    continue\n",
    "                else:\n",
    "                    continue\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            \n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embedding_tuple = load_vec('./wiki.multi.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_embedding_tuple = load_vec('./wiki.multi.fr.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция из MUSE: находим ближайшие вектора из другого языка. \n",
    "\n",
    "Посмотрим, насколько хорошо работает выравнивание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['cat','dog','human','student','computer']:\n",
    "    get_nn(word, en_embedding_tuple[0], en_embedding_tuple[1], fr_embedding_tuple[0], fr_embedding_tuple[1], K=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс Lang отвечает за обработку языка.\n",
    "\n",
    "SOS_token --- идентификатор начала предложения.\n",
    "\n",
    "EOS_token --- идентификатор конца предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, embedding_tuple):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = [ \"SOS\", \"EOS\"]\n",
    "        self.embedding_tuple = embedding_tuple    \n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:   \n",
    "            if word in self.embedding_tuple[2]:\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word.append(word)\n",
    "                self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def get_matrix(self):\n",
    "        \"\"\"\n",
    "        Получаем матрицу слово -> вектор для всех слов, которые встретились в тексте.\n",
    "        Вектор для начала предложения заменяем нулевым, \n",
    "        для конца предложения --- единичным (можно заменить на случайный вектор).\n",
    "        \"\"\"\n",
    "        dim = self.embedding_tuple[0].shape[1]\n",
    "        matrix = np.zeros((self.n_words, dim))        \n",
    "        matrix[0] = np.zeros(dim)\n",
    "        matrix[1] = np.ones(dim)\n",
    "        for id, word in enumerate(self.index2word[2:]):\n",
    "            id = id+2\n",
    "            word_id = self.embedding_tuple[2][word]\n",
    "            vector = self.embedding_tuple[0][word_id]\n",
    "            matrix[id] = vector\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предложение представляется как набор идентификаторов слов. \n",
    "# Чтобы поместить несколько предложений в одну матрицу, нужно дополнить каждое предложение токенами конца предложения.\n",
    "def pad_seq(seq, length):\n",
    "    \n",
    "    seq += [EOS_token for i in range(length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции-утилиты для считывания текста.\n",
    "\n",
    "В дальнейшем ограничимся только предложениями длины <= 10 для экономии памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, emb1, emb2,  prefix):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines1 = codecs.open(prefix+lang1, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    lines2 = codecs.open(prefix+lang2, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    lines1 = [normalizeString(s) for s in lines1]\n",
    "    lines2 = [normalizeString(s) for s in lines2]\n",
    "   \n",
    "    input_lang = Lang(lang1, emb1)\n",
    "    output_lang = Lang(lang2, emb2)\n",
    "\n",
    "    return input_lang, output_lang, lines1, lines2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "\n",
    "def filter_line(line):\n",
    "    return len(line.split(' ')) < MAX_LENGTH\n",
    "    \n",
    "def filter_lines(lines):\n",
    "    return [line for line in lines if filter_line(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, emb1, emb2,  prefix):\n",
    "    \"\"\"\n",
    "    Возвращает два объекта-языка, предложения для обучения\n",
    "    и пары параллельных предложений для промежуточной валидации результата\n",
    "    \"\"\"\n",
    "    input_lang, output_lang, lines1, lines2 = readLangs(lang1, lang2,  emb1, emb2, prefix)\n",
    "    print(\"Read %s sentence pairs\" % len(lines1))\n",
    "    pairs = [(l1, l2) for l1, l2 in zip(lines1, lines2) if filter_line(l1) and filter_line(l2)]\n",
    "    \n",
    "    # по условиями эксперимента у нас нет параллельных предложений. Для чистоты эксперимента  перемешаем их.\n",
    "    np.random.shuffle(lines1)\n",
    "    np.random.shuffle(lines2)\n",
    "\n",
    "    lines1 = filter_lines(lines1)\n",
    "    lines2 = filter_lines(lines2)\n",
    "    \n",
    "    min_lines = min(len(lines1), len(lines2))\n",
    "    lines1, lines2 = lines1[:min_lines], lines2[:min_lines]\n",
    "    \n",
    "    print(\"Trimmed to %s sentence pairs\" % min_lines)\n",
    "    print(\"Counting words...\")\n",
    "    for l1, l2 in zip(lines1, lines2):\n",
    "        input_lang.addSentence(l1)\n",
    "        output_lang.addSentence(l2)\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, lines1, lines2,  pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, lines1, lines2, pairs = prepareData('fr', 'en', fr_embedding_tuple, en_embedding_tuple, \n",
    "                                             'train.lc.norm.tok.')\n",
    "# в качестве щашумленного перевода предложений пока просто возьмем непараллельные пары\n",
    "tr_lines1,tr_lines2 = lines2[:], lines1[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции кодирования предложений в последовательность идентификаторов слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ') if word in lang.word2index]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "# функция возвращает матрицы случайных предложений и их зашумленных версий в pytorch-формате.\n",
    "# batch_size --- количество предложений.\n",
    "def random_batch(batch_size):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    tr_input_seqs = []\n",
    "    tr_target_seqs = []\n",
    "    \n",
    "    # Choose random pairs\n",
    "    for i in range(batch_size):\n",
    "        id1 = random.choice(xrange(len(lines1)))\n",
    "        id2 = random.choice(xrange(len(lines2)))\n",
    "        line1 = lines1[id1]\n",
    "        line2 = lines2[id2]\n",
    "        tr_line1 = tr_lines1[id1]\n",
    "        tr_line2 = tr_lines2[id2]\n",
    "        \n",
    "        \n",
    "        input_seqs.append(indexesFromSentence(input_lang, line1))\n",
    "        target_seqs.append(indexesFromSentence(output_lang, line2))\n",
    "        \n",
    "        tr_input_seqs.append(indexesFromSentence(output_lang, tr_line1))\n",
    "        tr_target_seqs.append(indexesFromSentence(input_lang, tr_line2))\n",
    "        \n",
    "        \n",
    "    input_length = max([len(s) for s in input_seqs])\n",
    "    target_length = max([len(s) for s in target_seqs])\n",
    "    tr_input_length = max([len(s) for s in tr_input_seqs])\n",
    "    tr_target_length = max([len(s) for s in tr_target_seqs])\n",
    "    \n",
    "    \n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length    \n",
    "    input_padded = [pad_seq(s, input_length) for s in input_seqs]    \n",
    "    target_padded = [pad_seq(s, target_length) for s in target_seqs]\n",
    "    \n",
    "    tr_input_padded = [pad_seq(s, tr_input_length) for s in tr_input_seqs]    \n",
    "    tr_target_padded = [pad_seq(s, tr_target_length) for s in tr_target_seqs]\n",
    "\n",
    "    \n",
    "    # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "    input_var = torch.tensor(input_padded, dtype=torch.long,device=device).transpose(0, 1)\n",
    "    target_var = torch.tensor(target_padded, dtype=torch.long,device=device).transpose(0, 1)\n",
    "    tr_input_var = torch.tensor(tr_input_padded, dtype=torch.long,device=device).transpose(0, 1)\n",
    "    tr_target_var = torch.tensor(tr_target_padded, dtype=torch.long,device=device).transpose(0, 1)\n",
    "    \n",
    "    \n",
    "    return input_var, target_var, tr_input_var, tr_target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = random_batch(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим матрицы для слов из каждого языка для использования в Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_matrix = torch.FloatTensor(input_lang.get_matrix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_matrix = torch.FloatTensor(output_lang.get_matrix())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простой класс однослойной нейросети с 1000 нейронов на скрытом слое, будет использоваться как дискриминатор.\n",
    "\n",
    "Входная размерность --- 300, совпадает с размерностью слов и размерностью скрытого пространства Seq2Seq,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.fc1 = nn.Linear(300,1000)\n",
    "        self.fc2 = nn.Linear(1000, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        y = torch.nn.functional.sigmoid(self.fc2(x))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "класс Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, matrix, gru = None):\n",
    "        \"\"\"\n",
    "        gru -- если не None, берет готовую модель gru и использует для своего языка.\n",
    "        Параметр требуется, чтобы использовать одну и ту же GRU-модель для двух энкодеров с разных языков.\n",
    "        \"\"\"\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(matrix)\n",
    "        self.embedding.requires_grad = False\n",
    "        \n",
    "        if not gru:\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        else:\n",
    "            self.gru = gru\n",
    "            \n",
    "\n",
    "    def forward(self, input, batch_size, hidden):\n",
    "        embedded = self.embedding(input).view(1, batch_size, self.hidden_size)\n",
    "        output = embedded\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder. \n",
    "\n",
    "Важно: в отличие от статьи, я не использовал Attention. \n",
    "\n",
    "Attention можно взять из официальной обучалки pytorch seq2seq, но он там работает с одним предложением за одну операцию.\n",
    "Лучше погуглить \"seq2seq pytorch batch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, matrix, gru = None):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(matrix)\n",
    "        self.embedding.requires_grad = False\n",
    "        if not gru:\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        else:\n",
    "            self.gru = gru\n",
    "            \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, batch_size):\n",
    "        output = self.embedding(input).view(1, batch_size, self.hidden_size)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции для проведения одной итерации оптимизации.\n",
    "\n",
    "teacher_forcing_ratio отвечает вероятность, что в качестве слов при декоде мы будем получать подсказку, а не будем использовать те слова, что были раскодированы декодером самостоятельно на предыдущих шагах. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def encode_decode(input_tensor,  encoder, decoder, target_tensor=None):\n",
    "    # раскодирует и декодирует.\n",
    "    # input_tensor, target_tensor --- матрицы размером\n",
    "    #    <Количество предложений в батче> * <Максимальная длина предложения в батче>\n",
    "    # возвращает последовательность идентификаторов слов от декодера и скрытый вектор от энкодера\n",
    "    batch_size = input_tensor.size(1)\n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    if target_tensor is None:\n",
    "        target_tensor = input_tensor\n",
    "    \n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(input_length, batch_size, encoder.hidden_size, device=device)\n",
    "\n",
    "    for ei in range(input_length):\n",
    "    \n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], batch_size, encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]*batch_size], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    outputs = []\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, batch_size)        \n",
    "        outputs.append(decoder_output)        \n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing            \n",
    "        else:\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            #if decoder_input == EOS_token:\n",
    "            #    break\n",
    "    return outputs,  encoder_hidden\n",
    "\n",
    "\n",
    "def train(source_tensor, target_tensor, translated_source_tensor, translated_target_tensor, \n",
    "          encoder_source, encoder_target,\n",
    "          decoder_source, decoder_target, discriminator,  \n",
    "          optimizer,   discriminator_optimizer, \n",
    "          criterion, cross_entropy, ae_coef = 1.0, translate_coef = 0.0, disc_coef = 1.0,   max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Одна итерация оптимизации\n",
    "    source_tensor --- матрица предложений первого языка\n",
    "    target_tensor --- матрица предложений второго языка\n",
    "    translated_source_tensor --- зашумленный перевод первого языка\n",
    "    translated_target_tensor --- зашумленный перевод второго языка\n",
    "    encoder_target, decoder_source, encoder_target, decoder_target --- seq2seq кодировщики\n",
    "    discriminator --- дискриминаторная сеть\n",
    "    optimizer, discriminator_optimizer --- оптимизаторы Seq2Seq и дискриминатора\n",
    "    criterion --- функция ошибки для Seq2Seq. Принимает на вход предложения и матрицу ответа от декодера\n",
    "    cross_entropy --- функция ошибки для дискриминатора\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = source_tensor.size(1)\n",
    "    \n",
    "    # автокодировщики\n",
    "    source_to_source, source_hidden = encode_decode(source_tensor, encoder_source, decoder_source )\n",
    "    target_to_target, target_hidden = encode_decode(target_tensor, encoder_target, decoder_target)\n",
    "    \n",
    "    # зануляем градиент от ошибки Seq2Seq\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # считаем ошибку от первого автокодировщика\n",
    "    for di in range(len(source_to_source)):\n",
    "        decoder_output = source_to_source[di]\n",
    "        loss += criterion(decoder_output, source_tensor[di])*ae_coef\n",
    "    \n",
    "    \n",
    "    #считаем ошибку от второго автокодировщика\n",
    "    for di in range(len(target_to_target)):\n",
    "        decoder_output = target_to_target[di]\n",
    "        loss += criterion(decoder_output, target_tensor[di])*ae_coef\n",
    "    \n",
    "    # будем считать, что объекты из ПЕРВОГО языка принадлежат  классу \"1\", \n",
    "    # объекты ВТОРОГО языка --- классу \"0\"\n",
    "    classes = torch.tensor(np.array([1.0]*batch_size + [0.0]*batch_size), dtype=torch.float, device=device)\n",
    "    \n",
    "    # Смотрим, что предсказал дискриминатор\n",
    "    source_hidden_predict = discriminator(torch.stack([target_hidden, source_hidden]))\n",
    "    # На данном этапе мы хотим обмануть дискриминатор, поэтому будем минимизировать долю правильных ответов,\n",
    "    # т.е. минимизировать ошибку между ответами дискриминатора и НЕПРАВИЛЬНЫМИ классами.\n",
    "    loss += cross_entropy(source_hidden_predict, classes)*disc_coef\n",
    "    \n",
    "    # аналогично автокодирощикам, считаем ошибку на зашумленном переводе\n",
    "    if translate_coef > 0.0:\n",
    "        translated_source_to_source, _ = encode_decode(translated_source_tensor, encoder_target, decoder_source, target_tensor=source_tensor)\n",
    "        translated_target_to_target, _ = encode_decode(translated_target_tensor, encoder_source, decoder_target, target_tensor=target_tensor)\n",
    "        \n",
    "        for di in range(min(len(translated_source_to_source), len(source_tensor))):\n",
    "            decoder_output = translated_source_to_source[di]\n",
    "            loss += criterion(decoder_output, source_tensor[di])*ae_coef\n",
    "\n",
    "        for di in range(min(len(translated_target_to_target), len(target_tensor))):\n",
    "            decoder_output = translated_target_to_target[di]\n",
    "            loss += criterion(decoder_output, target_tensor[di])*ae_coef\n",
    "\n",
    "\n",
    "\n",
    "    # подсчет градиента от ошибки\n",
    "    loss.backward()\n",
    "\n",
    "    # запуск оптимизации в сторону антиградиента\n",
    "    optimizer.step()\n",
    "    \n",
    "    # информация для отладки\n",
    "    avg_len = (len(target_tensor)+len(source_tensor))/2\n",
    "    \n",
    "    # обнуляем градиент и ошибку для дискриминатора\n",
    "    d_loss = 0     \n",
    "    discriminator_optimizer.zero_grad()\n",
    "    \n",
    "    _, source_hidden = encode_decode(source_tensor, encoder_source, decoder_source)\n",
    "    _, target_hidden = encode_decode(target_tensor, encoder_target, decoder_target)\n",
    "    \n",
    "    \n",
    "    # теперь минимизируем ошибку между предсказанным и правильным классами\n",
    "    source_hidden_predict = discriminator(torch.stack([source_hidden, target_hidden]))    \n",
    "    d_loss += cross_entropy(source_hidden_predict, classes)\n",
    "\n",
    "    d_loss.backward()\n",
    "\n",
    "    discriminator_optimizer.step()\n",
    "    \n",
    "              \n",
    "    \n",
    "    return loss.item() / avg_len, d_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def trainIters(encoder_source,encoder_target,  decoder_source,  decoder_target,discriminator, \n",
    "               n_iters, print_every=1000,  learning_rate=0.001):\n",
    "    # глобальная процедура оптимизации \n",
    "    \n",
    "    \n",
    "    global tr_lines1, tr_lines2\n",
    "    \n",
    "    optimizer = optim.Adam(list(encoder_source.parameters()) +  list(decoder_source.parameters())+\\\n",
    "                            list(encoder_target.parameters())+  list(decoder_target.parameters()),\n",
    "                           lr=learning_rate)\n",
    "    \n",
    "    optimizer2 = optim.Adam(discriminator.parameters(),\n",
    "                           lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    criterion2 = nn.BCELoss()\n",
    "    print_loss_total = [0, 0]\n",
    "\n",
    "    for iter in range(1, n_iters + 1):    \n",
    "        batch = random_batch(25)\n",
    "        \n",
    "        # уровень доверия к зашумленному переводу будет увеличиваться в процессе оптимизации. \n",
    "        # поскольку в первое время мы не обладаем никаким переводом, то изначально коэффициент будет нулевым.\n",
    "        if iter < print_every:\n",
    "            tr_coef = 0.0\n",
    "        else:\n",
    "            tr_coef = 1.0/n_iters\n",
    "        loss = train(batch[0], batch[1], batch[2], batch[3],  encoder_source,encoder_target,  decoder_source,  decoder_target,\n",
    "                     discriminator, optimizer, optimizer2, criterion, criterion2, translate_coef = tr_coef)\n",
    "        print_loss_total[0] += loss[0]\n",
    "        print_loss_total[1] += loss[1]\n",
    "        \n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            # выводи примеры перевода, среднюю ошибку и делаем новый шумный перевод, \n",
    "            # каждую 'print_every' итерацию.\n",
    "            print_loss_avg0 = print_loss_total[0] / print_every\n",
    "            print_loss_avg1 = print_loss_total[1] / print_every\n",
    "            print_loss_total = [0,0]\n",
    "            print iter, print_loss_avg0, print_loss_avg1\n",
    "            print '_'*10\n",
    "            evaluateRandomly(encoder_source, decoder_target,n=3)\n",
    "            print '_'*10\n",
    "            evaluateRandomly(encoder_source, decoder_source,langid=0, n=3)\n",
    "            print '_'*10\n",
    "            evaluateRandomly(encoder_target, decoder_target, langid=1, n=3)\n",
    "            \n",
    "            tr_lines1, tr_lines2 = make_translation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_translation():\n",
    "    \"\"\"\n",
    "    Построение зашуменного перевода. \n",
    "    Можно существенно ускорить, если переводить батчем.\n",
    "    \"\"\"\n",
    "    max_length = MAX_LENGTH\n",
    "    tr_lines1, tr_lines2 = [],[]\n",
    "    id = 0\n",
    "    for line in lines1:\n",
    "        id+=1\n",
    "        if id%1000 == 0:\n",
    "            print 'translating source', id\n",
    "        input_tensor = tensorFromSentence(input_lang, line)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder_source.initHidden(1)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder_source.hidden_size, device=device)\n",
    "\n",
    "        \n",
    "        for ei in range(min(MAX_LENGTH, input_length)):\n",
    "            encoder_output, encoder_hidden = encoder_source(input_tensor[ei],1,\n",
    "                                                     encoder_hidden)\n",
    "            \n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        \n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder_target(\n",
    "                decoder_input, decoder_hidden, 1)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:                \n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        tr_lines1.append(u' '.join(decoded_words))\n",
    "    \n",
    "    # можно устранить дублирование кода, не дошли руки, прим. Олег\n",
    "    id  = 0 \n",
    "    for line in lines2:\n",
    "        id+=1\n",
    "        if id%1000 == 0:\n",
    "            print 'translation target', id\n",
    "        input_tensor = tensorFromSentence(output_lang, line)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder_target.initHidden(1)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder_target.hidden_size, device=device)\n",
    "\n",
    "        \n",
    "        for ei in range(min(MAX_LENGTH, input_length)):\n",
    "            encoder_output, encoder_hidden = encoder_target(input_tensor[ei],1,\n",
    "                                                     encoder_hidden)\n",
    "            \n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        \n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder_source(\n",
    "                decoder_input, decoder_hidden, 1)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                \n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(input_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        tr_lines2.append(u' '.join(decoded_words))\n",
    "        \n",
    "    \n",
    "    return tr_lines1, tr_lines2\n",
    "#tr_lines1, tr_lines2 = make_translation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, encoded_lang,  decoder_lang, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Процедура промежуточной валидации перевода\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(encoded_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden(1)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(min(MAX_LENGTH, input_length)):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],1,\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        \n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, 1)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(decoder_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "    \n",
    "def evaluateRandomly(encoder, decoder, langid = -1,  n=10):\n",
    "    \"\"\"\n",
    "    Берем n предложений и смотрим качество на них.\n",
    "    Если langid == -1 --- смотрим качество перевода.\n",
    "    Если langid == 0 или == 1, смотирм качество восстановления автокодировщиком.\n",
    "    \"\"\"\n",
    "    for i in range(n):\n",
    "        if langid == -1:\n",
    "            id0 = 0\n",
    "            id1 = 1\n",
    "            enc_lang= input_lang\n",
    "            dec_lang = output_lang\n",
    "        else:            \n",
    "            id0 = langid\n",
    "            if langid == 0:\n",
    "                enc_lang = input_lang\n",
    "                dec_lang = input_lang\n",
    "            else:\n",
    "                enc_lang = output_lang\n",
    "                dec_lang = output_lang\n",
    "        if langid==-1:\n",
    "                \n",
    "            pair = random.choice(pairs)\n",
    "        else:\n",
    "            pair = [random.choice(lines1), random.choice(lines2)]\n",
    "        print   '>', pair[id0]\n",
    "        if langid==-1:\n",
    "            print '=', pair[id1]\n",
    "        \n",
    "            \n",
    "        output_words = evaluate(encoder, decoder, pair[id0], enc_lang, dec_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print '<', output_sentence, '\\n'\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание сетей и запуск обучения.\n",
    "\n",
    "Для информации ниже: пример вывода валидации на последних 10000 итераций.\n",
    "\n",
    "Видно, что перевод работает, хотя и не всегда корректно.\n",
    "\n",
    "Зато почти идеально выполняется восстановление предложений.\n",
    "\n",
    "Добавление шума в автокодировщик (как в статье) исправит положение."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> un colley descend une glissiere en courant .\n",
    "= a collie runs down a slide .\n",
    "< a cabin shot of a very crowded airplane <EOS> \n",
    "\n",
    "> un homme regardant par sa fenetre .\n",
    "= a man looking out of his window .\n",
    "< a woman walking with groceries walking past graffiti <EOS> \n",
    "\n",
    "> l apos homme balaie avec un balai rouge .\n",
    "= the man is sweeping with a red broom .\n",
    "< the man is and doing a trick . <EOS> \n",
    "\n",
    "__________\n",
    "> des femmes regardent la mascotte de rio negro .\n",
    "< des femmes regardent la mascotte de rio negro . <EOS> \n",
    "\n",
    "> un basketteur athletique vient de smasher .\n",
    "< un basketteur athletique vient de smasher . <EOS> \n",
    "\n",
    "> un vetetiste saute par dessus un obstacle .\n",
    "< un saute par dessus un obstacle . <EOS> \n",
    "\n",
    "__________\n",
    "> a piece of wall art that includes a tree\n",
    "< a piece of wall art that includes a tree <EOS> \n",
    "\n",
    "> a man and a woman are smiling\n",
    "< a man and a woman are smiling <EOS> \n",
    "\n",
    "> three people are harvesting grass from muddy water .\n",
    "< three people are harvesting grass from muddy water . <EOS> \n",
    "\n",
    "translating source 1000\n",
    "translating source 2000\n",
    "translating source 3000\n",
    "translation target 1000\n",
    "translation target 2000\n",
    "translation target 3000\n",
    "100000 0.671771921184 0.617039271778\n",
    "__________\n",
    "> un lanceur de base ball lance une balle .\n",
    "= a baseball pitcher throws a ball .\n",
    "< a baseball player is fielding a ball . <EOS> \n",
    "\n",
    "> un homme faisant tournoyer un baton enflamme .\n",
    "= a man twirling a flaming baton .\n",
    "< a man at work butchering a cow . <EOS> \n",
    "\n",
    "> une fille saute dans l apos herbe .\n",
    "= a girl jumps in the grass .\n",
    "< brown dog crouching in grass looking up <EOS> \n",
    "\n",
    "__________\n",
    "> deux ouvriers nettoient une structure de nuit .\n",
    "< deux ouvriers nettoient une structure de nuit . <EOS> \n",
    "\n",
    "> une femme avec un manteau bleu court dehors .\n",
    "< une femme avec un manteau bleu court dehors . <EOS> \n",
    "\n",
    "> trois policiers surveillant un magasin .\n",
    "< trois policiers surveillant un magasin . <EOS> \n",
    "\n",
    "__________\n",
    "> man puts his feet up on a desk .\n",
    "< man puts his feet up on a desk . <EOS> \n",
    "\n",
    "> a soccer game or a football game\n",
    "< a soccer game or a football game <EOS> \n",
    "\n",
    "> two men pouring dirt in a field .\n",
    "< two men pouring dirt in a field . <EOS> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "hidden_size = 300\n",
    "\n",
    "encoder_source = EncoderRNN(input_lang.n_words, hidden_size, fr_matrix).to(device)\n",
    "encoder_target = EncoderRNN(output_lang.n_words, hidden_size, en_matrix, gru = encoder_source.gru).to(device)\n",
    "\n",
    "decoder_source = DecoderRNN(hidden_size, input_lang.n_words, fr_matrix).to(device)\n",
    "decoder_target = DecoderRNN(hidden_size, output_lang.n_words, en_matrix, gru = decoder_source.gru).to(device)\n",
    "\n",
    "disc = Net1()\n",
    "disc.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIters(encoder_source,encoder_target,  decoder_source,  decoder_target, disc, 100000, print_every=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder_source.gru.state_dict(), 'monolingual_seq2seq_fr_en_enc')\n",
    "torch.save(decoder_source.gru.state_dict(), 'monolingual_seq2seq_fr_en_dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
