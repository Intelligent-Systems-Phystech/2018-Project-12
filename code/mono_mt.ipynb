{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mono_mt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "q3xgRhtW3cmg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mounting Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "5_l9L8pBjwhE",
        "colab_type": "code",
        "outputId": "d1e8b998-e5fa-44a7-c841-21263b7f85e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "gdrive = 'gdrive/My\\ Drive/colab_storage/'\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w97gdn8fMR5V",
        "colab_type": "code",
        "outputId": "9b12fe59-ccf9-4438-aa43-ca393f3c9a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade torch nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already up-to-date: nltk in /usr/local/lib/python3.6/dist-packages (3.4)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: singledispatch in /usr/local/lib/python3.6/dist-packages (from nltk) (3.4.0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4kaEePeOLNHH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Загрузка данных\n",
        "\n",
        "Флаг _-nc_ позволяет не скачивать файлы, если они уже есть. "
      ]
    },
    {
      "metadata": {
        "id": "c5-bwtVlLNHM",
        "colab_type": "code",
        "outputId": "be63709a-def9-4a7d-9133-7b2d5462b999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "!wget -nc https://raw.githubusercontent.com/Intelligent-Systems-Phystech/2018-Project-12/master/data/opus/samples.ru\n",
        "!wget -nc https://raw.githubusercontent.com/Intelligent-Systems-Phystech/2018-Project-12/master/data/opus/samples.uk\n",
        "!wget -nc https://s3.amazonaws.com/arrival/embeddings/wiki.multi.ru.vec\n",
        "!wget -nc https://s3.amazonaws.com/arrival/embeddings/wiki.multi.uk.vec"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘samples.ru’ already there; not retrieving.\n",
            "\n",
            "File ‘samples.uk’ already there; not retrieving.\n",
            "\n",
            "File ‘wiki.multi.ru.vec’ already there; not retrieving.\n",
            "\n",
            "File ‘wiki.multi.uk.vec’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SAiT9LdLLNHY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Определения\n",
        "\n",
        "После первого запуска имеет смысл установить __LOAD_PICKLED = True__, это позволит загрузить переведённый датасет, а не переводить всё заново."
      ]
    },
    {
      "metadata": {
        "id": "Fp_MEHfYLNHc",
        "colab_type": "code",
        "outputId": "102dfe6a-5e4a-4f37-b201-aa246082e3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "LOAD_PICKLED = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_SvOu_FLLNHm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Вспомогательные функции"
      ]
    },
    {
      "metadata": {
        "id": "Ndrxt2bdi-vQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "def BLEU(ref_list, hyp_list):\n",
        "  chencherry = SmoothingFunction()\n",
        "  ref_lists = [[r] for r in ref_list]\n",
        "  return corpus_bleu(ref_lists, hyp_list, smoothing_function=chencherry.method1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIUN6jBbLNHo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-ZА-ЯҐЄІЇа-яґєії.!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "def read_sentences(path):\n",
        "    lines = []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            lines.append(normalizeString(line))\n",
        "    return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GxAYk5IjLNHt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seq_format(seq, max_words):\n",
        "    nwords = len(seq)\n",
        "    seq_new = seq + [\"<EOS>\"]\n",
        "    seq_new += [\"<PAD>\" for i in range(max_words - nwords)]\n",
        "    return seq_new\n",
        "\n",
        "def freq_filter(seq, lang, freq):\n",
        "    if freq == -1:\n",
        "        return True\n",
        "    for w in seq:\n",
        "        if lang.word2count[w] < freq:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def len_filter(seq, max_len):\n",
        "    if max_len == -1:\n",
        "        return True\n",
        "    else:\n",
        "        return len(seq) <= max_len\n",
        "\n",
        "def prepare_list(list, lang, max_words, freq):\n",
        "    list_seq = [s.split() for s in list]\n",
        "    list_clean = []\n",
        "    for s in list_seq:\n",
        "        if len_filter(s, max_words) and freq_filter(s, lang, freq):\n",
        "            list_clean.append(s)\n",
        "        else:\n",
        "            list_clean.append(None)\n",
        "            \n",
        "    return list_clean\n",
        "\n",
        "def seq2ind(seq, lang):\n",
        "    return [lang.word2index[w] for w in seq]\n",
        "\n",
        "def noise(seq, drop_prob=0.1, shuffle_len=3):\n",
        "    n = len(seq)\n",
        "    ind = np.argsort(np.arange(0, n) + np.random.uniform(0, shuffle_len, n))\n",
        "    drop_mask = np.random.binomial(1, 1-drop_prob, n).astype(np.bool)\n",
        "    ind = ind[drop_mask]\n",
        "    res = []\n",
        "    for i in ind:\n",
        "        res.append(seq[i])\n",
        "    return res\n",
        "  \n",
        "def ind2words(ind_seq, vocab):\n",
        "    \"\"\"Translate word indices to words.\n",
        "\n",
        "        Arguments:\n",
        "        ind_seq  -- sequence of indices\n",
        "        lang     -- corresponding vocabulary\n",
        "    \"\"\"\n",
        "    return list(map(lambda x: vocab.index2word[x], ind_seq))\n",
        "\n",
        "def words2sent(words):\n",
        "    \"\"\"Translate word indices to sentence.\n",
        "\n",
        "        Arguments:\n",
        "        words  -- sequence of words\n",
        "        lang     -- corresponding vocabulary\n",
        "    \"\"\"\n",
        "    try:\n",
        "        end = words.index('<EOS>')\n",
        "    except ValueError:\n",
        "        end = len(words)\n",
        "    return ' '.join([w for w in words[:end] if w not in ['<PAD>', '<SOS>', '<EOS>']])\n",
        "\n",
        "def ind2sent(ind_seq, vocab):\n",
        "    return words2sent(ind2words(ind_seq, vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yHrH-PzzLNHz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Словарь\n",
        "\n",
        "Собирает статистику по словам в тексте и умеет выдавать идекс каждого слова."
      ]
    },
    {
      "metadata": {
        "id": "wuFRGD7qQ62O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_vec(emb_path, max_words=-1):\n",
        "    vectors = []\n",
        "    word2id = {}\n",
        "    it = 0\n",
        "    with open(emb_path) as f:\n",
        "        nvec, ndim = [int(k) for k in f.readline().split()]\n",
        "        for line in f:\n",
        "            if max_words != -1 and it > max_words:\n",
        "                break\n",
        "            it += 1\n",
        "            orig_word, vect = line.rstrip().split(' ', 1)\n",
        "            \n",
        "            word = normalizeString(orig_word)\n",
        "            vect = np.fromstring(vect, sep=' ')\n",
        "       \n",
        "            # Words are sorted by frequency, no need to add less \n",
        "            # frequent version of the same word  \n",
        "            if not (word in word2id):\n",
        "                vectors.append(vect)\n",
        "                word2id[word] = len(word2id)\n",
        "\n",
        "    id2word = {v: k for k, v in word2id.items()}\n",
        "    embeddings = np.vstack(vectors)\n",
        "    return embeddings, id2word, word2id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXOlJ9iPLNH0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "    Tracks info about known words, their indices and frequences.\n",
        "    \"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = []\n",
        "        self.n_words = 0\n",
        "        self.add_seq(self.get_dummies())\n",
        "   \n",
        "    def add_list(self, list):\n",
        "        for s in list:\n",
        "            self.add_sentence(s)\n",
        "    \n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_seq(self, seq):\n",
        "        for word in seq:\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:   \n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word.append(word)\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "        \n",
        "    @staticmethod\n",
        "    def get_dummies():\n",
        "        return [\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"]\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_dummy_ind(w):\n",
        "        return Vocabulary.get_dummies().index(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QXpgGTHuLNH8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Датасет\n",
        "\n",
        "Хранит данные, отвечает за генерацию перевода, тренировочной и валидационной выборок. "
      ]
    },
    {
      "metadata": {
        "id": "ogfh_r9PLNH_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class Dataset:\n",
        "    \"\"\"\n",
        "    Data storage and preprocessing.\n",
        "    \"\"\"\n",
        "    def __init__(self, lang_info, max_len=-1, min_freq=-1, val_ratio=0.1):\n",
        "        \"\"\"\n",
        "        Arguments: \n",
        "        lang_info   -- dictionary with following info:\n",
        "                      --> name = language name (str)\n",
        "                      --> corpus_path = path to file with sentences (str)\n",
        "        max_length  -- maximum sentence length (-1 for no limit)\n",
        "        min_freq    -- minimum word appearing frequency (-1 for no limit)\n",
        "        val_ration -- fraction of sentences to use for validation\n",
        "        \"\"\"\n",
        "        self.max_len = max_len\n",
        "        self.min_freq = min_freq\n",
        "        \n",
        "        # Sentences lists\n",
        "        self.s_list = {}\n",
        "        \n",
        "        if len(lang_info) != 2:\n",
        "            raise ValueError('Only pairs of languages are supported, but {} was passed.'.format(len(lang_info)))\n",
        "        self.names = []\n",
        "        for l, path in lang_info.items():\n",
        "            self.names.append(l)\n",
        "            self.s_list[l] = read_sentences(path)\n",
        "            \n",
        "        # Vocabularies \n",
        "        self.v_list = {}\n",
        "        for l in self.names:\n",
        "            self.v_list[l] = Vocabulary(l)\n",
        "            self.v_list[l].add_list(self.s_list[l])\n",
        "    \n",
        "        # Filter sentences\n",
        "        self.seq_list = {} \n",
        "        nsents = []\n",
        "        \n",
        "        seq_list = []\n",
        "        seq_names = []\n",
        "        for l in self.names:\n",
        "            tmp = prepare_list(self.s_list[l], self.v_list[l], \n",
        "                                           max_len, min_freq)\n",
        "            nsents.append(len(tmp))\n",
        "            seq_list.append(tmp)\n",
        "            seq_names.append(l)\n",
        "\n",
        "        if len(set(nsents)) != 1:\n",
        "            raise Warning('Numbers of sentences are not equal for the languages.')\n",
        "        \n",
        "        # Combaine parallel and non-parallel data\n",
        "        nopair = {l:[] for l in seq_names}\n",
        "        pair = {l:[] for l in seq_names}\n",
        "        nfiltered = [0, 0]\n",
        "        npairs = 0\n",
        "        for s in zip(*seq_list):\n",
        "            if s[0] == None and s[1] == None:\n",
        "                continue\n",
        "            if s[1] == None:\n",
        "                nfiltered[0] += 1\n",
        "                nopair[seq_names[0]].append(s[0])\n",
        "            elif s[0] == None:\n",
        "                nfiltered[1] += 1\n",
        "                nopair[seq_names[1]].append(s[1])\n",
        "            else:\n",
        "                nfiltered[0] += 1\n",
        "                nfiltered[1] += 1\n",
        "                npairs += 1\n",
        "                for i in range(2):\n",
        "                    pair[seq_names[i]].append(s[i])\n",
        "       \n",
        "        # Form test and train sentences\n",
        "        wanted_len = int(val_ratio*min(nfiltered))\n",
        "        if wanted_len > npairs:\n",
        "            raise Warning('Asked for {} test samples, but only {} can be provided.'.format(wanted_len, npairs))\n",
        "        res_len = min(npairs, wanted_len)\n",
        "        self.test_list = {}\n",
        "        self.seq_list = {}\n",
        "        self.val_size = res_len\n",
        "        for l in self.names:\n",
        "            self.seq_list[l] = pair[l][res_len:] + nopair[l]\n",
        "            self.test_list[l] = pair[l][:res_len]\n",
        "        # No translated version present\n",
        "        self.seq_tr_list = {}\n",
        "        for l in self.names:\n",
        "            self.seq_tr_list[l] = None\n",
        "        \n",
        "        # No initial embeddings present\n",
        "        self.emb = {}\n",
        "        for l in self.names:\n",
        "            self.emb[l] = None\n",
        "        \n",
        "    def translate(self, translator, info_timeout=30):\n",
        "        \"\"\"Build translation of stored sentences.\n",
        "        \n",
        "            Arguments:\n",
        "            translator   -- an object that has translate_seq(seq, from_lang, to_lang) function,\n",
        "                            where: seq -- sequence of words\n",
        "                                 from_lang, to_lang -- strings\n",
        "            info_timeout -- time between printing info \n",
        "        \"\"\"\n",
        "        other = dict(zip(self.names, self.names[::-1]))\n",
        "        start = time.time()\n",
        "        for l, seq_list in self.seq_list.items():\n",
        "            n = len(seq_list)\n",
        "            self.seq_tr_list[l] = []\n",
        "            for i, s in enumerate(seq_list):\n",
        "                seq_tr = translator.translate_seq(s, l, other[l])\n",
        "                self.seq_tr_list[l].append(seq_tr)\n",
        "                self.v_list[other[l]].add_seq(seq_tr)\n",
        "                if not i % 100:\n",
        "                    end = time.time()\n",
        "                if end - start > info_timeout: \n",
        "                    start = end\n",
        "                    print('[{}] {:.1f}% done'.format(l, i/n*100))\n",
        "                  \n",
        "    def get_train(self, batch_size=1):\n",
        "        \"\"\"Get train data.\n",
        "          \n",
        "          Returns:\n",
        "           X_auto{'en', 'fr'}   --  indexed noisy src sentences\n",
        "           Y_auto{'en', 'fr'}   --  indexed clean src sentences\n",
        "           X_cross{'en', 'fr'}   --  indexed translated noisy src sentences\n",
        "           Y_cross{'en', 'fr'}   --  indexed clean src sentences\n",
        "        \"\"\"\n",
        "        X_auto = {}\n",
        "        Y_auto = {}\n",
        "        \n",
        "        X_cross = {}\n",
        "        Y_cross = {}\n",
        "        other = dict(zip(self.names, self.names[::-1]))\n",
        "        for l, lang in self.v_list.items():\n",
        "            # Autoencoders train\n",
        "            batch_ind = np.random.choice(range(len(self.seq_list[l])), batch_size, replace=False)\n",
        "            seq_list_tmp = [self.seq_list[l][i] for i in batch_ind]\n",
        "            \n",
        "            X_auto_tmp = list(map(noise, seq_list_tmp))\n",
        "            Y_auto_tmp = seq_list_tmp\n",
        "            \n",
        "            # Cross-domain train\n",
        "            batch_ind = np.random.choice(range(len(self.seq_tr_list[l])), batch_size, replace=False)\n",
        "            seq_list_tmp = [self.seq_list[l][i] for i in batch_ind]\n",
        "            seq_tr_list_tmp = [self.seq_tr_list[l][i] for i in batch_ind]\n",
        "            \n",
        "            X_cross_tmp = list(map(noise, seq_tr_list_tmp))\n",
        "            Y_cross_tmp = seq_list_tmp\n",
        "            \n",
        "            vocabs = 3*[self.v_list[l]] + [self.v_list[other[l]]]\n",
        "            seq_lists = [X_auto_tmp, Y_auto_tmp, Y_cross_tmp, X_cross_tmp]\n",
        "            ind_lists = []\n",
        "            for lang, seq_list in zip(vocabs, seq_lists):\n",
        "                max_len = max(list(map(len, seq_list)))\n",
        "                formatted = list(map(lambda x: seq_format(x, max_len), seq_list))\n",
        "                inds = torch.tensor(list(map(lambda x: seq2ind(x, lang), formatted))) \n",
        "                ind_lists.append(inds)\n",
        "                \n",
        "            X_auto[l], Y_auto[l], Y_cross[l], X_cross[l] = ind_lists\n",
        "                \n",
        "        return X_auto, Y_auto, X_cross, Y_cross\n",
        "    \n",
        "    def get_test(self, nsamples=-1):\n",
        "        \"\"\"Get test data.\n",
        "        \n",
        "        Returns:\n",
        "        X{'fr', 'en'} -- pairs of translated sentences \n",
        "        \"\"\"\n",
        "        X = {}\n",
        "        if nsamples==-1:\n",
        "            nsamples = self.val_size\n",
        "        inds = np.random.choice(range(self.val_size), nsamples, replace=False)\n",
        "        for l, lang in self.v_list.items():\n",
        "            test_list_tmp = [self.test_list[l][i] for i in inds]\n",
        "            max_len = max(list(map(len, test_list_tmp)))\n",
        "            formatted = list(map(lambda x: seq_format(x, max_len), test_list_tmp))\n",
        "            X[l] = torch.tensor(list(map(lambda x: seq2ind(x, lang), formatted))) \n",
        "        return X\n",
        "        \n",
        "    def build_initial_embedding(self, lang_info):\n",
        "        \"\"\"\n",
        "        Arguments: \n",
        "        lang_info  -- dictionary with following info:\n",
        "                      --> name = language name (str)\n",
        "                      --> emb_path = path to file with embeddings (str)\n",
        "        \"\"\"   \n",
        "        for l, path in lang_info.items():\n",
        "            embeddings, id2word, word2id = load_vec(path)\n",
        "            cur_voc = self.v_list[l]\n",
        "            \n",
        "            dum_len = len(cur_voc.get_dummies())\n",
        "            emb = torch.zeros((cur_voc.n_words, embeddings.shape[1]+dum_len))\n",
        "            # One-hot for dummies\n",
        "            for i in range(dum_len):\n",
        "              emb[i, -dum_len+i] = 1\n",
        "            # Copy for others\n",
        "            for i in range(dum_len, cur_voc.n_words):\n",
        "              w = cur_voc.index2word[i]\n",
        "              if w in word2id:\n",
        "                emb[i, :-dum_len] = torch.from_numpy(embeddings[word2id[w]])\n",
        "              else:\n",
        "                # To avoid zero\n",
        "                x = torch.rand(embeddings.shape[1]) + 1e-8\n",
        "                emb[i, :-dum_len] = x/x.norm()\n",
        "        \n",
        "            self.emb[l] = emb\n",
        "    def ind2sent(self, ind_seq, lang):\n",
        "        \"\"\"Translate word indices to sentence.\n",
        "        \n",
        "            Arguments:\n",
        "            ind_seq  -- sequence of indices\n",
        "            lang     -- corresponding language ('en', 'fr', ...)\n",
        "        \"\"\"\n",
        "        return ind2sent(ind_seq, self.v_list[lang])\n",
        "      \n",
        "    def ind2words(self, ind_seq, lang):\n",
        "        \"\"\"Translate word indices to words.\n",
        "        \n",
        "            Arguments:\n",
        "            ind_seq  -- sequence of indices\n",
        "            lang     -- corresponding language ('en', 'fr', ...)\n",
        "        \"\"\"\n",
        "        return ind2words(ind_seq, self.v_list[lang])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yjvDcOuqLNII",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Переводчик\n",
        "\n",
        "Нулевое приближение перевода, используя натренированные представления слов. Близким словам соответствуют близкие векторы, переводим вектор в вектор."
      ]
    },
    {
      "metadata": {
        "id": "axEcHUWfLNIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NaiveTranslator:\n",
        "    \"\"\"Naive word-by-word translation with caching.\n",
        "    \"\"\"\n",
        "    def __init__(self, lang_info, max_words=-1):\n",
        "        \"\"\"\n",
        "        Arguments: \n",
        "        lang_info  -- dictionary with following info:\n",
        "                      --> name = language name (str)\n",
        "                      --> emb_path = path to file with embeddings (str)\n",
        "        max_words -- maximum number of embeddings to load (sorted by frequency)\n",
        "        \"\"\"   \n",
        "        self.emb = {}\n",
        "        self.id2word = {}\n",
        "        self.word2id = {}\n",
        "        self.names = []\n",
        "        for l, path in lang_info.items():\n",
        "            self.names.append(l)\n",
        "            self.emb[l], self.id2word[l], self.word2id[l] = load_vec(path, \n",
        "                                                                     max_words)\n",
        "            \n",
        "        self.cache = {l: {} for l in self.names}\n",
        "        # Add dummies\n",
        "        for l in self.names:\n",
        "            for w in [\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"]:\n",
        "                self.cache[l][w] = w\n",
        "        \n",
        "    def translate(self, word, from_lang, to_lang):\n",
        "        if word in self.cache[from_lang]:\n",
        "            return self.cache[from_lang][word]\n",
        "        else:\n",
        "            # Handle unknown\n",
        "            if word in self.word2id[from_lang]:\n",
        "                id = self.word2id[from_lang][word]\n",
        "            else:\n",
        "                self.cache[from_lang][word] = \"<UNK>\"\n",
        "                return \"<UNK>\"\n",
        "            \n",
        "            vec = self.emb[from_lang][id]\n",
        "            dist = np.dot(self.emb[to_lang], vec)\n",
        "            ind = np.asscalar(np.argmax(dist, axis=0))\n",
        "            tr = self.id2word[to_lang][ind]\n",
        "            self.cache[from_lang][word] = tr    \n",
        "            return tr\n",
        "    \n",
        "    def translate_sent(self, sent, from_lang, to_lang):\n",
        "        new_sent = ' '.join([self.translate(w, from_lang, to_lang) for w in sent.split()])\n",
        "        return new_sent\n",
        "    \n",
        "    def translate_seq(self, seq, from_lang, to_lang):\n",
        "        return [self.translate(w, from_lang, to_lang) for w in seq]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZV1yY4thLNIW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Энкодер\n",
        "\n",
        "Обычный GRU. Параметры:\n",
        "* __embeddings__  -- оптимизируемые представления слов\n",
        "* __hidden_size__ -- размерность векторов в скрытом пространстве (предложений), куда отображает энкодер. Совпадает с размерностью вектора состояния RNN (энкодера)  "
      ]
    },
    {
      "metadata": {
        "id": "-MrYuTcaLNIY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        super().__init__()\n",
        "        self.emb = embeddings\n",
        "        self.hidden_size = hidden_size\n",
        "        for emb in embeddings.values():\n",
        "            self.input_size = emb.embedding_dim\n",
        "            \n",
        "        self.gru = nn.GRU(self.input_size, self.hidden_size, batch_first=True,\n",
        "                          bidirectional=True)\n",
        "\n",
        "    def step(self, input, hidden, from_lang):\n",
        "#         print('from', from_lang)\n",
        "#         print('input', input)\n",
        "#         print('hidden', hidden)\n",
        "\n",
        "        embedded = self.emb[from_lang](input)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "    \n",
        "    def forward(self, ind_batch, nsteps, from_lang):\n",
        "#         print('>>Encoder:step')\n",
        "#         encoder_outputs = torch.zeros((ind_batch.shape[0], nsteps, hidden_size), \n",
        "#                                       device=ind_batch.device)\n",
        "        encoder_hidden = torch.zeros((2, ind_batch.shape[0], self.hidden_size), \n",
        "                                     device=ind_batch.device)\n",
        "# #         print('encoder_outputs', encoder_outputs.shape)\n",
        "#         for i in range(nsteps):\n",
        "# #             print('step:', i)\n",
        "#             encoder_output, encoder_hidden = self.step(ind_batch[:, [i]], encoder_hidden, from_lang)\n",
        "# #             print('encoder_output', encoder_output.shape)\n",
        "#             encoder_outputs[:, i, :] += encoder_output.squeeze()\n",
        "        embedded = self.emb[from_lang](ind_batch)\n",
        "        encoder_outputs, encoder_hidden = self.gru(embedded, encoder_hidden)  \n",
        "#         print('encoder_outputs', encoder_outputs.shape)\n",
        "#         print('encoder_hidden', encoder_hidden.shape)\n",
        "#         print('<<Encoder:step')\n",
        "        \n",
        "        return encoder_outputs, encoder_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XSWTZEO3LNId",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Сеть внимания\n",
        "\n",
        "По вектору скрытого пространства и вектору состояния декодера говорит, какое внимание должно быть уделено первому.\n",
        "\n",
        "* __input_size__ -- размер вектора в скрытом пространстве предложений\n",
        "* __state_size__ -- размер вектора скрытого состояния\n",
        "* __inner_size__ -- размер внутреннего слоя"
      ]
    },
    {
      "metadata": {
        "id": "Ipt4OdBJLNIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Линейный вариант"
      ]
    },
    {
      "metadata": {
        "id": "YsTwcRXGLNIf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttnLinear(torch.nn.Module):\n",
        "    def __init__(self, input_size, state_size, inner_size = 10):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(input_size + state_size, inner_size)\n",
        "        self.v = nn.Linear(inner_size, 1)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "#         print('>>AttnLinear')\n",
        "#         print('input', input.shape)\n",
        "#         print('hidden', hidden.shape)\n",
        "        expanded = hidden.expand(-1, input.shape[1], -1)\n",
        "#         print('expanded', expanded.shape)\n",
        "#         print('<<AttnLinear')\n",
        "        return torch.relu(self.v(self.W(torch.cat((input, expanded), dim=2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nT7ydfuULNIl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Вариант в виде сети"
      ]
    },
    {
      "metadata": {
        "id": "BMJ7YD3iLNIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttnNet(torch.nn.Module):\n",
        "    def __init__(self, input_size, state_size, inner_size=10):\n",
        "        super().__init__()\n",
        "        self.v = nn.Linear(inner_size, 1, bias=False)\n",
        "        self.W = nn.Linear(input_size, inner_size, bias=False)\n",
        "        self.U = nn.Linear(state_size, inner_size, bias=False)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "#         print('>>AttnNet')\n",
        "#         print('input', input.shape)\n",
        "#         print('hidden', hidden.shape)\n",
        "#         print('<<AttnNet')\n",
        "        return torch.relu(self.v(self.W(input) + self.U(hidden)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pc4Zk7kaLNIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Декодер\n",
        "\n",
        "Умеет по представлению предложения в скрытом пространстве (последовательность векторов размерности __hidden_size__) получать последовательность слов для одного из языков.\n",
        "\n",
        "* __embeddings__  -- оптимизируемые представления слов\n",
        "* __hidden_size__ -- размер вектора в скрытом пространстве (предложений)\n",
        "* __state_size__  -- размер вектора состояния RNN (декодера)\n",
        "* __inner_size__  -- размер внутреннего слоя сети внимания "
      ]
    },
    {
      "metadata": {
        "id": "xGqr_7DILNIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttnDecoder(torch.nn.Module):\n",
        "    def __init__(self, embeddings, hidden_size, state_size, attn_size):\n",
        "        super().__init__()\n",
        "        self.emb = embeddings\n",
        "        for emb in embeddings.values():\n",
        "            self.emb_size = emb.embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attn_size = attn_size\n",
        "        self.state_size = state_size\n",
        "\n",
        "        self.attn = AttnLinear(hidden_size, state_size, attn_size)\n",
        "        self.gru = nn.GRU(hidden_size + self.emb_size, state_size, batch_first=True)\n",
        "        self.out = nn.ModuleDict()\n",
        "        for l, emb in embeddings.items():\n",
        "            self.out[l] = nn.Linear(state_size, emb.num_embeddings)\n",
        "        \n",
        "    def step(self, ind, hidden, encoder_outputs, to_lang):\n",
        "#         print('hidden', hidden.shape)\n",
        "        input = self.emb[to_lang](ind)\n",
        "#         print('encoder_outputs', encoder_outputs.shape)\n",
        "        attn_weights = torch.softmax(\n",
        "            self.attn(encoder_outputs, hidden.transpose(0, 1)), dim=1)\n",
        "#         print('attn_weights', attn_weights.shape)\n",
        "        attn_applied = torch.bmm(attn_weights.transpose(1, 2), encoder_outputs)\n",
        "#         print('attn_applied', attn_applied.shape)\n",
        "        gru_input = torch.cat((input, attn_applied), dim=2)\n",
        "#         print('gru_input', gru_input.shape)\n",
        "        output, hidden = self.gru(gru_input, hidden)\n",
        "        output = self.out[to_lang](output)\n",
        "        output = torch.log_softmax(output, dim=2)\n",
        "        \n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def forward(self, encoder_outputs, nsteps, to_lang):\n",
        "#         print('>>AttnDecoder')\n",
        "        decoder_outputs = torch.zeros((encoder_outputs.shape[0], nsteps,\n",
        "                                       self.emb[to_lang].num_embeddings), \n",
        "                                       device=encoder_outputs.device)\n",
        "#         print('decoder_outputs', decoder_outputs.shape)\n",
        "        # Load encoded hidden state\n",
        "#         decoder_hidden = encoder_outputs[:, -1, :].unsqueeze(0).contiguous()\n",
        "\n",
        "        decoder_hidden = torch.zeros((1, encoder_outputs.shape[0], self.state_size), \n",
        "                                     device=encoder_outputs.device)\n",
        "        # Get SOS (start of sentence)\n",
        "        input = torch.full((encoder_outputs.shape[0], 1),\n",
        "                           Vocabulary.get_dummy_ind('<SOS>'),\n",
        "                           dtype=torch.long)\n",
        "        input = input.to(encoder_outputs.device)\n",
        "        for i in range(nsteps):\n",
        "#             print('step', i)\n",
        "            decoder_output, decoder_hidden, attn_weights = self.step(input, \n",
        "                                                       decoder_hidden, \n",
        "                                                       encoder_outputs,\n",
        "                                                       to_lang)\n",
        "#             print('decoder_output', decoder_output.shape)\n",
        "            decoder_outputs[:, [i], :] += decoder_output\n",
        "            _, input = decoder_output.topk(1, dim=2)\n",
        "#             print('input', input.shape)\n",
        "            input = input.view(encoder_outputs.shape[0], 1)\n",
        "        \n",
        "#         print('<<AttnDecoder')\n",
        "        return decoder_outputs, decoder_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBXiq9QxLNI1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Дискриминатор\n",
        "\n",
        "Умеет по представлению предложения в скрытом пространстве (последовательность векторов размерности __hidden_size__) говорить, какому из двух языков (__0__ или __1__) она принадлежит.\n",
        "\n",
        "* __hidden_size__ -- размер вектора в скрытом пространстве (предложений)\n",
        "* __hidden_len__  -- максимальная длина последовательности векторов в скрытом пространстве; она же максимальная длина входный предложений\n",
        "* __hidden_layer_size__  -- размер скрытого слоя"
      ]
    },
    {
      "metadata": {
        "id": "qhVOswe3LNI3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, hidden_size, hidden_layer_size, smooth_coef=1e-1):\n",
        "        super().__init__()\n",
        "        self.smooth_coef = smooth_coef\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.hid = nn.Linear(hidden_size, hidden_layer_size)\n",
        "        self.hid2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
        "        self.out = nn.Linear(hidden_layer_size, 1)\n",
        "        \n",
        "    def forward(self, input):\n",
        "#         print('>>Discriminator')\n",
        "        input = torch.relu(input)\n",
        "        out = torch.relu(self.hid(input))\n",
        "        out = torch.sigmoid(self.out(torch.relu(self.hid2(out)))).squeeze()\n",
        "      \n",
        "        # Add smoothing\n",
        "        smooth = torch.eye(out.shape[-1], device=input.device)*self.smooth_coef\n",
        "        smooth[0, 0] = 1\n",
        "        for i in range(1, out.shape[-1]):\n",
        "          smooth[:i, i] = (1-self.smooth_coef)*smooth[:i, i-1]\n",
        "#         print('<<Discriminator')\n",
        "        return torch.mm(out, smooth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B2K_1iL9LNI-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Обёртка\n",
        "\n",
        "Управляет всеми частями модели. Параметры соответствуют описанным выше."
      ]
    },
    {
      "metadata": {
        "id": "7OuJz3TvLNI_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Wrapper(nn.Module):\n",
        "    def __init__(self, decoder_size, attn_size, discr_size, dataset):\n",
        "        super().__init__()\n",
        "        self.emb = nn.ModuleDict()\n",
        "        self.names = []\n",
        "        for name, emb in dataset.emb.items():\n",
        "            self.emb[name] = nn.Embedding.from_pretrained(emb, freeze=False)\n",
        "            self.names.append(name)\n",
        "            self.hidden_size = self.emb[name].embedding_dim\n",
        "        self.decoder_size = decoder_size\n",
        "        self.attn_size = attn_size\n",
        "        self.discr_size = discr_size\n",
        "        \n",
        "        #Bidirectional encoder\n",
        "        self.enc = Encoder(self.emb, self.hidden_size)\n",
        "        self.dec = AttnDecoder(self.emb, 2*self.hidden_size, decoder_size, attn_size)\n",
        "        self.discr = Discriminator(2*self.hidden_size, discr_size)\n",
        "        \n",
        "    def encode(self, ind_batch, from_lang):\n",
        "        return self.enc(ind_batch, ind_batch.shape[1], from_lang)\n",
        "    \n",
        "    def decode(self, encoder_outputs, output_len, to_lang):\n",
        "        return self.dec(encoder_outputs, output_len, to_lang)\n",
        "    \n",
        "    def encode_decode(self, ind_batch, from_lang, to_lang, out_len=None):\n",
        "        if out_len == None:\n",
        "            out_len = ind_batch.shape[1]\n",
        "        \n",
        "        encoder_outputs, encoder_hidden = self.encode(ind_batch, from_lang)\n",
        "        decoder_outputs, decoder_hidden = self.decode(encoder_outputs, out_len, to_lang)\n",
        "        \n",
        "        return encoder_outputs, decoder_outputs\n",
        "    \n",
        "    def discriminate(self, encoder_outputs):\n",
        "        return self.discr(encoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QZmIwM7yjers",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Translator:\n",
        "    def __init__(self, wrapper, vocabs, max_len):\n",
        "        self.wrapper = wrapper\n",
        "        self.vocabs = vocabs\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def translate_seq(self, seq, from_lang, to_lang):\n",
        "        ind_seq = seq2ind(seq, self.vocabs[from_lang]) +\\\n",
        "                              [self.vocabs[from_lang].get_dummy_ind('<EOS>')]\n",
        "        device = self.wrapper.enc.gru.weight_hh_l0.device\n",
        "        ind_seq = torch.LongTensor(ind_seq).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs, decoder_outputs =\\\n",
        "                           self.wrapper.encode_decode(ind_seq, from_lang, \n",
        "                                                      to_lang, self.max_len)\n",
        "        _, ind = decoder_outputs.squeeze().topk(1, dim=1)\n",
        "        return ind2words(ind.squeeze().cpu().tolist(), self.vocabs[to_lang])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k7MUAZ6ZskT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class IndTranslator:\n",
        "    def __init__(self, wrapper, max_len):\n",
        "        self.wrapper = wrapper\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def translate(self, ind_batch, from_lang, to_lang):\n",
        "        device = self.wrapper.enc.gru.weight_hh_l0.device\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs, decoder_outputs =\\\n",
        "                           self.wrapper.encode_decode(ind_batch, from_lang, \n",
        "                                                      to_lang, self.max_len)\n",
        "        _, ind = decoder_outputs.squeeze().topk(1, dim=1)\n",
        "        return ind.squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SCyFaOvnLNJF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Загрузка датасета\n",
        "\n",
        "Если был указан параметр __LOAD_PICKLED = True__, то загружается из файла. Иначе создаётся заново: процесс не очень быстрый."
      ]
    },
    {
      "metadata": {
        "id": "ZLsXCdlYLNJH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if LOAD_PICKLED:\n",
        "    !cp $gdrive\"\"NaiveTranslator ./\n",
        "    with open('NaiveTranslator', 'rb') as f:\n",
        "        tr = pickle.load(f)\n",
        "else:\n",
        "    lang_info = {'ru': 'wiki.multi.ru.vec',\n",
        "                 'uk': 'wiki.multi.uk.vec'}\n",
        "\n",
        "    tr = NaiveTranslator(lang_info)\n",
        "    with open('NaiveTranslator', 'wb') as f:\n",
        "        pickle.dump(tr, f)\n",
        "        \n",
        "    !cp NaiveTranslator $gdrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHvt10BeLNJN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0c92b6fa-8884-4a54-c100-a536e6d26c8b"
      },
      "cell_type": "code",
      "source": [
        "max_in_len = 20\n",
        "min_in_freq = 5\n",
        "\n",
        "if LOAD_PICKLED:\n",
        "    !cp $gdrive\"\"Dataset ./\n",
        "    with open('Dataset', 'rb') as f:\n",
        "        D = pickle.load(f)\n",
        "else:\n",
        "    lang_info = {'ru': 'samples.ru',\n",
        "                 'uk': 'samples.uk'}\n",
        "\n",
        "    print('-> Loading dataset')\n",
        "    D = Dataset(lang_info, max_in_len, min_in_freq)\n",
        "    print('-> Translating')\n",
        "    D.translate(tr)\n",
        "    print('-> Building embedding')\n",
        "    lang_info = {'ru': 'wiki.multi.ru.vec',\n",
        "                 'uk': 'wiki.multi.uk.vec'}\n",
        "    D.build_initial_embedding(lang_info)\n",
        "   \n",
        "    with open('Dataset', 'wb') as f:\n",
        "        pickle.dump(D, f)\n",
        "        \n",
        "    !cp Dataset $gdrive"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-> Loading dataset\n",
            "-> Translating\n",
            "[ru] 17.0% done\n",
            "[uk] 2.9% done\n",
            "[uk] 47.1% done\n",
            "-> Building embedding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DGgI1oQULNJU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Пример генерации обучающей/тестовой выборки."
      ]
    },
    {
      "metadata": {
        "id": "bqk2n83MLNJY",
        "colab_type": "code",
        "outputId": "98c1fadb-c6a4-49c6-ac3d-6a5e49bafc2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "train = D.get_train(1)\n",
        "\n",
        "for l in ['ru', 'uk']:\n",
        "  print('{} train samples: {}'.format(l, len(D.seq_list[l])))\n",
        "  print('{} test samples: {}'.format(l, len(D.test_list[l])))\n",
        "  \n",
        "print('\\nExamples:')\n",
        "l = 'ru'\n",
        "other = 'uk'\n",
        "\n",
        "print('X_auto:', D.ind2sent(train[0][l][0], l))\n",
        "print('Y_auto:', D.ind2sent(train[1][l][0], l))\n",
        "print('X_cross:', D.ind2sent(train[2][l][0], other))\n",
        "print('Y_cross:', D.ind2sent(train[3][l][0], l))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ru train samples: 7052\n",
            "ru test samples: 778\n",
            "uk train samples: 7008\n",
            "uk test samples: 778\n",
            "\n",
            "Examples:\n",
            "X_auto: чего для я создана .\n",
            "Y_auto: вот для чего я создана .\n",
            "X_cross: не мене було з батьком він . помер\n",
            "Y_cross: меня не было рядом с отцом когда он умер .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-VSzVMZ7LNJk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Тренировка модели"
      ]
    },
    {
      "metadata": {
        "id": "HjmLrgU-hPil",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88e0456d-8193-4ef6-8664-2a394a29697f"
      },
      "cell_type": "code",
      "source": [
        "# Load from saved iteration\n",
        "# -1 for new model\n",
        "LOAD_ITER = -1\n",
        "\n",
        "param = gdrive+f'checkpoint.{LOAD_ITER}' + ' ./'\n",
        "!cp $param"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'gdrive/My Drive/colab_storage/checkpoint.-1': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "K3r8fO7vLNJo",
        "colab_type": "code",
        "outputId": "dc60b1d0-1988-47a5-ba93-a5b2eea834e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 27227
        }
      },
      "cell_type": "code",
      "source": [
        "niters = 100000\n",
        "batch_size = 32\n",
        "\n",
        "# Number of iterations between validations\n",
        "val_per = 500\n",
        "# Batch size for validation (-1 == use full test set)\n",
        "val_size = -1\n",
        "# Number of iterations between printing current iteration\n",
        "it_per = 50\n",
        "# Number of iterations between model saves\n",
        "save_per = 500\n",
        "\n",
        "# Decoder hidden state size\n",
        "decoder_size = 300\n",
        "# Attention net size\n",
        "attn_size = 10\n",
        "# Discriminator size\n",
        "discr_size= 300\n",
        "# Start embeddings optimisation only when loss is sufficiently small \n",
        "border_loss = 11\n",
        "\n",
        "# Ability to translate\n",
        "tr_crit = nn.NLLLoss()\n",
        "# Ability to fool the discriminator\n",
        "tr_fake_crit = nn.BCELoss()\n",
        "# Ability to predict language correctly\n",
        "discr_crit = nn.BCELoss()\n",
        "\n",
        "wr = Wrapper(decoder_size, attn_size, discr_size, D).to(device)\n",
        "\n",
        "if LOAD_ITER != -1:\n",
        "    checkpoint = torch.load(f'checkpoint.{LOAD_ITER}')\n",
        "    wr.load_state_dict(checkpoint['state_dict'])\n",
        "else:\n",
        "    LOAD_ITER = 0\n",
        "\n",
        "# Discriminator optimizer\n",
        "d_opt = torch.optim.RMSprop(wr.discr.parameters(), lr=0.0005)\n",
        "# Encoder and decoder optimizer\n",
        "tr_opt = torch.optim.Adam(list(wr.enc.parameters()) + list(wr.dec.parameters()),\n",
        "                          lr=0.0003, betas=(0.5, 0.999))\n",
        "\n",
        "# Assign classes to languages\n",
        "class_num = {name:cl for cl, name in enumerate(D.names)}\n",
        "# Dict with lang pairs\n",
        "other = dict(zip(D.names, D.names[::-1]))\n",
        "# Get index of PAD \n",
        "pad_ind = Vocabulary.get_dummy_ind('<PAD>')\n",
        "\n",
        "start = time.time()\n",
        "avg_d_loss = 0\n",
        "avg_tr_loss = 0\n",
        "\n",
        "\n",
        "# Don't touch embeddings at the beginning \n",
        "for name in D.names:\n",
        "  wr.emb[name].weight.requires_grad = False\n",
        "\n",
        "for it in range(LOAD_ITER, LOAD_ITER + niters):\n",
        "    # Refresh gradients\n",
        "    tr_opt.zero_grad()\n",
        "    d_opt.zero_grad()\n",
        "    \n",
        "    # Get training batch\n",
        "    X_auto, Y_auto, X_cross, Y_cross = D.get_train(batch_size)\n",
        "    \n",
        "    # Losses to be accumulated\n",
        "    d_loss = 0\n",
        "    tr_loss = 0\n",
        "    # Train autoencoders\n",
        "    for l in D.names:\n",
        "        # Noisy sentences \n",
        "        X_auto[l] = X_auto[l].to(device)\n",
        "        # Clean sentences\n",
        "        Y_auto[l] = Y_auto[l].to(device)\n",
        "        # Noisy translation to other language \n",
        "        X_cross[l] = X_cross[l].to(device)\n",
        "        # Source sentences \n",
        "        Y_cross[l] = Y_cross[l].to(device)\n",
        "\n",
        "        ## AUTOENCODER PHASE\n",
        "        \n",
        "        encoder_outputs, decoder_outputs =\\\n",
        "                           wr.encode_decode(X_auto[l], \n",
        "                                            l,\n",
        "                                            l,\n",
        "                                            Y_auto[l].shape[1])\n",
        "        \n",
        "        # Dont penalize padding\n",
        "        if torch.any(Y_auto[l] == pad_ind):\n",
        "            decoder_outputs[Y_auto[l] == pad_ind][:, pad_ind] = 0\n",
        "        tr_loss += tr_crit(decoder_outputs.transpose(1, 2), Y_auto[l])\n",
        "        # We want to predict wrong class labels (fool the discriminator)\n",
        "        predicted = wr.discriminate(encoder_outputs)\n",
        "        wanted = torch.full_like(predicted, class_num[other[l]], device=device)\n",
        "        tr_loss += tr_fake_crit(predicted, wanted)\n",
        "        \n",
        "        # And predict correct classes by discriminator\n",
        "        # .detach() allows us to ignore subgraph, \n",
        "        # connected with encoder+decoder\n",
        "        correct = torch.full_like(predicted, class_num[l], device=device)\n",
        "        predicted_det = wr.discriminate(encoder_outputs.detach())\n",
        "        d_loss += discr_crit(predicted_det, correct)\n",
        "        \n",
        "        ## CROSS-DOMAIN PHASE\n",
        "        ## Repeats previous phase with the only difference of source\n",
        "        ## language change\n",
        "        \n",
        "        encoder_outputs, decoder_outputs =\\\n",
        "                       wr.encode_decode(X_cross[l], \n",
        "                                        other[l], # language changed here\n",
        "                                        l,\n",
        "                                        Y_cross[l].shape[1])\n",
        "        if torch.any(Y_cross[l] == pad_ind):\n",
        "            decoder_outputs[Y_cross[l] == pad_ind][:, pad_ind] = 0\n",
        "        tr_loss += tr_crit(decoder_outputs.transpose(1, 2), Y_cross[l])\n",
        "        \n",
        "        # Language is changed here too\n",
        "        predicted = wr.discriminate(encoder_outputs)\n",
        "        wanted = torch.full_like(predicted, class_num[l], device=device)\n",
        "        tr_loss += tr_fake_crit(predicted, wanted)\n",
        "        \n",
        "        correct = torch.full_like(predicted, class_num[other[l]], device=device)\n",
        "        predicted_det = wr.discriminate(encoder_outputs.detach())\n",
        "        d_loss += discr_crit(predicted_det, correct)\n",
        "        \n",
        "    ## STATISTICS\n",
        "    \n",
        "    avg_d_loss += d_loss.item()\n",
        "    avg_tr_loss += tr_loss.item()\n",
        "    \n",
        "    ## BACKPROPAGATION PHASE\n",
        "    \n",
        "    # Activate embedding training only when loss is low enough\n",
        "    for name in D.names:\n",
        "        if tr_loss.item() < border_loss:\n",
        "            wr.emb[name].weight.requires_grad = True\n",
        "        else:\n",
        "            wr.emb[name].weight.requires_grad = False\n",
        "        \n",
        "    ## OPTIMISATION STEP\n",
        "    \n",
        "    # We don't want Discriminator weights to be affected\n",
        "    for p in wr.discr.parameters():\n",
        "        p.requires_grad = False\n",
        "    # Computed encoder and decoder gradients\n",
        "    tr_loss.backward()\n",
        "    # Undo the changes\n",
        "    for p in wr.discr.parameters():\n",
        "        p.requires_grad = True\n",
        "    tr_opt.step()\n",
        "    \n",
        "    # Now calculate the Discriminator loss\n",
        "    d_loss.backward()\n",
        "    d_opt.step()\n",
        "\n",
        "    if not it % it_per and it != LOAD_ITER:\n",
        "        end = time.time()\n",
        "        print('Iterations: {} ({} sec/iter)'.format(it, (end-start)/it_per))\n",
        "        print('Average losses:')\n",
        "        print('\\td_loss:', avg_d_loss/it_per)\n",
        "        print('\\ttr_loss:', avg_tr_loss/it_per)\n",
        "        avg_tr_loss = 0\n",
        "        avg_d_loss = 0\n",
        "        start = time.time()\n",
        "        \n",
        "    if not it % save_per and it != LOAD_ITER:\n",
        "        path = f'checkpoint.{it}'\n",
        "        torch.save({'state_dict': wr.state_dict()}, path)\n",
        "        print('Saved model to:', path)\n",
        "        param = f'checkpoint.{it}' + ' ' + gdrive\n",
        "        !cp $param\n",
        "        \n",
        "    if not it % val_per and it != LOAD_ITER:\n",
        "        print('Last loss:')\n",
        "        print('  d_loss =', d_loss.data)\n",
        "        print('  tr_loss =', tr_loss.data)\n",
        "        print(f'\\n[{it}] Validation:')\n",
        "        \n",
        "        wr_tr = IndTranslator(wr, max_in_len)\n",
        "        X = D.get_test(val_size)\n",
        "        ref_list = []\n",
        "        hyp_list = []\n",
        "        print('Sample translations:')\n",
        "        for l in D.names:\n",
        "            print('\\t', other[l], ' --> ', l)\n",
        "            \n",
        "            # Save reference sentences\n",
        "            ind_list = X[l].cpu().tolist()\n",
        "            sent_list = [D.ind2sent(inds, l) for inds in ind_list]    \n",
        "            print('\\t<< ', D.ind2sent(X[other[l]][0].cpu().tolist(), other[l]))\n",
        "            print('\\t== ', sent_list[0])\n",
        "            ref_list += sent_list\n",
        "            \n",
        "            # Translate them\n",
        "            X_cur = X[other[l]].to(device)\n",
        "            X_tr = wr_tr.translate(X_cur, other[l], l)\n",
        "\n",
        "            # Save translations\n",
        "            ind_list = X_tr.cpu().tolist()\n",
        "            sent_list = [D.ind2sent(inds, l) for inds in ind_list]\n",
        "            print('\\t>> ', sent_list[0])\n",
        "            hyp_list += sent_list\n",
        "        \n",
        "        print('BLEU: {:.2f}\\n'.format(BLEU(ref_list, hyp_list)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations: 50 (0.5599467897415161 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.833861594200134\n",
            "\ttr_loss: 21.72158000946045\n",
            "Iterations: 100 (0.5649371433258057 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7738406801223756\n",
            "\ttr_loss: 13.943929901123047\n",
            "Iterations: 150 (0.5644050407409668 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.769741401672363\n",
            "\ttr_loss: 13.35061674118042\n",
            "Iterations: 200 (0.5646068239212036 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.778872332572937\n",
            "\ttr_loss: 12.72068670272827\n",
            "Iterations: 250 (0.563067479133606 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.774490375518799\n",
            "\ttr_loss: 12.622583751678468\n",
            "Iterations: 300 (0.5657293653488159 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7706520462036135\n",
            "\ttr_loss: 12.325673599243164\n",
            "Iterations: 350 (0.5780575847625733 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7767705392837523\n",
            "\ttr_loss: 12.078471393585206\n",
            "Iterations: 400 (0.5646663570404052 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.770696983337402\n",
            "\ttr_loss: 12.12285629272461\n",
            "Iterations: 450 (0.582655668258667 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7750833225250244\n",
            "\ttr_loss: 11.724210910797119\n",
            "Iterations: 500 (0.5699185895919799 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7611397886276245\n",
            "\ttr_loss: 11.846295337677002\n",
            "Saved model to: checkpoint.500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7447, device='cuda:0')\n",
            "  tr_loss = tensor(11.5338, device='cuda:0')\n",
            "\n",
            "[500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  і точно не тут .\n",
            "\t==  и точно не здесь .\n",
            "\t>>  третии бессмысленно <UNK> <UNK> и <UNK>\n",
            "\t ru  -->  uk\n",
            "\t<<  и точно не здесь .\n",
            "\t==  і точно не тут .\n",
            "\t>>  третіи не <UNK> <UNK> <UNK> і нарешті <UNK> <UNK>\n",
            "BLEU: 0.03\n",
            "\n",
            "Iterations: 550 (0.7114699411392212 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7513392639160155\n",
            "\ttr_loss: 11.964034957885742\n",
            "Iterations: 600 (0.5650030660629273 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7664196634292604\n",
            "\ttr_loss: 11.660244388580322\n",
            "Iterations: 650 (0.564346375465393 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7713236522674563\n",
            "\ttr_loss: 11.595221557617187\n",
            "Iterations: 700 (0.5655163049697876 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7770177268981935\n",
            "\ttr_loss: 11.687092037200928\n",
            "Iterations: 750 (0.5601567125320435 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.767608027458191\n",
            "\ttr_loss: 11.659310150146485\n",
            "Iterations: 800 (0.5891028213500976 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7773260354995726\n",
            "\ttr_loss: 11.472116165161133\n",
            "Iterations: 850 (0.6010419797897338 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7742609691619875\n",
            "\ttr_loss: 11.150825519561767\n",
            "Iterations: 900 (0.577212986946106 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7730748891830443\n",
            "\ttr_loss: 11.273188591003418\n",
            "Iterations: 950 (0.5774024200439453 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7723744201660154\n",
            "\ttr_loss: 11.405488910675048\n",
            "Iterations: 1000 (0.5676471567153931 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773519744873047\n",
            "\ttr_loss: 11.437376689910888\n",
            "Saved model to: checkpoint.1000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7668, device='cuda:0')\n",
            "  tr_loss = tensor(11.4423, device='cuda:0')\n",
            "\n",
            "[1000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  а хто він ?\n",
            "\t==  но кто он ?\n",
            "\t>>  и название <UNK>\n",
            "\t ru  -->  uk\n",
            "\t<<  но кто он ?\n",
            "\t==  а хто він ?\n",
            "\t>>  і не <UNK>\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 1050 (0.6979939603805542 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771897106170654\n",
            "\ttr_loss: 11.440014896392823\n",
            "Iterations: 1100 (0.5934838390350342 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7735066556930543\n",
            "\ttr_loss: 11.192739677429199\n",
            "Iterations: 1150 (0.5868667411804199 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773192205429077\n",
            "\ttr_loss: 11.158269329071045\n",
            "Iterations: 1200 (0.592650318145752 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7736068773269653\n",
            "\ttr_loss: 11.028249492645264\n",
            "Iterations: 1250 (0.5813799142837525 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7709817934036254\n",
            "\ttr_loss: 11.086367683410645\n",
            "Iterations: 1300 (0.5932249498367309 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7751628589630126\n",
            "\ttr_loss: 11.018024501800538\n",
            "Iterations: 1350 (0.5896533393859863 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773596477508545\n",
            "\ttr_loss: 10.900077590942383\n",
            "Iterations: 1400 (0.5838971996307373 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.77368793964386\n",
            "\ttr_loss: 10.839020595550537\n",
            "Iterations: 1450 (0.5532968378067017 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7739753484725953\n",
            "\ttr_loss: 11.187629146575928\n",
            "Iterations: 1500 (0.5663301992416382 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7722438621520995\n",
            "\ttr_loss: 10.932560367584228\n",
            "Saved model to: checkpoint.1500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7753, device='cuda:0')\n",
            "  tr_loss = tensor(11.0048, device='cuda:0')\n",
            "\n",
            "[1500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  ну так але цього разу все буде як я сказав .\n",
            "\t==  ну да но в этот раз все будет как я сказал .\n",
            "\t>>  и которая бессмысленно наконец <UNK> явит собои и и\n",
            "\t ru  -->  uk\n",
            "\t<<  ну да но в этот раз все будет как я сказал .\n",
            "\t==  ну так але цього разу все буде як я сказав .\n",
            "\t>>  сама не\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 1550 (0.6978404903411866 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7743582677841188\n",
            "\ttr_loss: 11.01502384185791\n",
            "Iterations: 1600 (0.5802430772781372 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7732935428619383\n",
            "\ttr_loss: 10.793191776275634\n",
            "Iterations: 1650 (0.5633089542388916 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7727396965026854\n",
            "\ttr_loss: 10.827859935760499\n",
            "Iterations: 1700 (0.5554372882843017 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773917942047119\n",
            "\ttr_loss: 10.920677299499511\n",
            "Iterations: 1750 (0.5586255836486816 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7738803672790526\n",
            "\ttr_loss: 10.716759510040283\n",
            "Iterations: 1800 (0.5649645805358887 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7739750289916993\n",
            "\ttr_loss: 10.576576652526855\n",
            "Iterations: 1850 (0.5606438446044922 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7730082511901855\n",
            "\ttr_loss: 10.710419788360596\n",
            "Iterations: 1900 (0.5659935903549195 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7743078088760376\n",
            "\ttr_loss: 10.46526216506958\n",
            "Iterations: 1950 (0.5614473915100098 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7726165533065794\n",
            "\ttr_loss: 10.516472969055176\n",
            "Iterations: 2000 (0.5587069654464721 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7729230642318727\n",
            "\ttr_loss: 10.510780925750732\n",
            "Saved model to: checkpoint.2000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7595, device='cuda:0')\n",
            "  tr_loss = tensor(10.7163, device='cuda:0')\n",
            "\n",
            "[2000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  може не треба ?\n",
            "\t==  оно того стоит ?\n",
            "\t>>  и название <UNK> <UNK>\n",
            "\t ru  -->  uk\n",
            "\t<<  оно того стоит ?\n",
            "\t==  може не треба ?\n",
            "\t>>  і такого\n",
            "BLEU: 0.03\n",
            "\n",
            "Iterations: 2050 (0.7145419406890869 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7744330072402956\n",
            "\ttr_loss: 10.45103515625\n",
            "Iterations: 2100 (0.5643277978897094 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772889046669006\n",
            "\ttr_loss: 10.445422172546387\n",
            "Iterations: 2150 (0.5809295129776001 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7730557918548584\n",
            "\ttr_loss: 10.31548261642456\n",
            "Iterations: 2200 (0.6008379507064819 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7739753484725953\n",
            "\ttr_loss: 10.159121685028076\n",
            "Iterations: 2250 (0.5832636499404907 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7740344905853274\n",
            "\ttr_loss: 10.312045917510986\n",
            "Iterations: 2300 (0.5863424491882324 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773305506706238\n",
            "\ttr_loss: 10.184544563293457\n",
            "Iterations: 2350 (0.5858695602416992 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7729048252105715\n",
            "\ttr_loss: 10.273138751983643\n",
            "Iterations: 2400 (0.5831642389297486 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7731254482269287\n",
            "\ttr_loss: 10.273101406097412\n",
            "Iterations: 2450 (0.5854123258590698 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7734064197540285\n",
            "\ttr_loss: 10.081824150085449\n",
            "Iterations: 2500 (0.5987039279937744 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773026428222656\n",
            "\ttr_loss: 9.938701419830322\n",
            "Saved model to: checkpoint.2500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7749, device='cuda:0')\n",
            "  tr_loss = tensor(8.5512, device='cuda:0')\n",
            "\n",
            "[2500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  але вони знаишли дещо у будинку .\n",
            "\t==  но они нашли кое что в доме .\n",
            "\t>>  \n",
            "\t ru  -->  uk\n",
            "\t<<  но они нашли кое что в доме .\n",
            "\t==  але вони знаишли дещо у будинку .\n",
            "\t>>  це не\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 2550 (0.737679238319397 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7727692461013795\n",
            "\ttr_loss: 10.062330207824708\n",
            "Iterations: 2600 (0.5824830198287964 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7717709493637086\n",
            "\ttr_loss: 9.901359310150147\n",
            "Iterations: 2650 (0.5911531639099121 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7746881437301636\n",
            "\ttr_loss: 9.937833042144776\n",
            "Iterations: 2700 (0.5958675718307496 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773022322654724\n",
            "\ttr_loss: 9.83219871520996\n",
            "Iterations: 2750 (0.6079831504821778 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7727878046035768\n",
            "\ttr_loss: 9.806563358306885\n",
            "Iterations: 2800 (0.6121605491638183 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7722429847717285\n",
            "\ttr_loss: 9.85982089996338\n",
            "Iterations: 2850 (0.6069342565536499 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7720060300827027\n",
            "\ttr_loss: 9.608388252258301\n",
            "Iterations: 2900 (0.5988776922225952 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7720044565200808\n",
            "\ttr_loss: 9.580016269683838\n",
            "Iterations: 2950 (0.5878201150894165 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7716207838058473\n",
            "\ttr_loss: 9.59588451385498\n",
            "Iterations: 3000 (0.5856724214553833 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7729109382629393\n",
            "\ttr_loss: 9.405827560424804\n",
            "Saved model to: checkpoint.3000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7955, device='cuda:0')\n",
            "  tr_loss = tensor(9.6183, device='cuda:0')\n",
            "\n",
            "[3000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  що ж це відбувається ?\n",
            "\t==  что ж это происходит ?\n",
            "\t>>  и бессмысленно <UNK> <UNK>\n",
            "\t ru  -->  uk\n",
            "\t<<  что ж это происходит ?\n",
            "\t==  що ж це відбувається ?\n",
            "\t>>  і не\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 3050 (0.7286216831207275 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773606562614441\n",
            "\ttr_loss: 9.314023342132568\n",
            "Iterations: 3100 (0.6136513233184815 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772110118865967\n",
            "\ttr_loss: 9.383107051849365\n",
            "Iterations: 3150 (0.6188156652450562 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7725430154800415\n",
            "\ttr_loss: 9.312363567352294\n",
            "Iterations: 3200 (0.5979436683654785 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7705346632003782\n",
            "\ttr_loss: 9.25515064239502\n",
            "Iterations: 3250 (0.5977514505386352 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7740134477615355\n",
            "\ttr_loss: 9.245749378204346\n",
            "Iterations: 3300 (0.57774808883667 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7729522228240966\n",
            "\ttr_loss: 9.164638061523437\n",
            "Iterations: 3350 (0.5843432760238647 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771488456726074\n",
            "\ttr_loss: 9.08202169418335\n",
            "Iterations: 3400 (0.5913805532455444 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7741815757751467\n",
            "\ttr_loss: 8.928453435897827\n",
            "Iterations: 3450 (0.57630126953125 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.77232271194458\n",
            "\ttr_loss: 8.964281558990479\n",
            "Iterations: 3500 (0.5852151823043823 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772488865852356\n",
            "\ttr_loss: 8.899676380157471\n",
            "Saved model to: checkpoint.3500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7569, device='cuda:0')\n",
            "  tr_loss = tensor(8.4492, device='cuda:0')\n",
            "\n",
            "[3500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  ні в чому .\n",
            "\t==  не в чем .\n",
            "\t>>  <UNK> и бессмысленно <UNK>\n",
            "\t ru  -->  uk\n",
            "\t<<  не в чем .\n",
            "\t==  ні в чому .\n",
            "\t>>  і . <UNK> <UNK>\n",
            "BLEU: 0.03\n",
            "\n",
            "Iterations: 3550 (0.7262614822387695 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7721129989624025\n",
            "\ttr_loss: 8.769883785247803\n",
            "Iterations: 3600 (0.5703285980224609 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772339382171631\n",
            "\ttr_loss: 8.861670360565185\n",
            "Iterations: 3650 (0.5958648920059204 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7737850522994996\n",
            "\ttr_loss: 8.716324939727784\n",
            "Iterations: 3700 (0.5977400398254394 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7717337083816527\n",
            "\ttr_loss: 8.505793132781982\n",
            "Iterations: 3750 (0.5956472015380859 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7724324607849122\n",
            "\ttr_loss: 8.501217918395996\n",
            "Iterations: 3800 (0.6084476518630981 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.77289692401886\n",
            "\ttr_loss: 8.543490133285523\n",
            "Iterations: 3850 (0.6281320476531982 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772378296852112\n",
            "\ttr_loss: 8.467816352844238\n",
            "Iterations: 3900 (0.6040722179412842 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7723770999908446\n",
            "\ttr_loss: 8.522505655288697\n",
            "Iterations: 3950 (0.6088064527511596 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7719295501708983\n",
            "\ttr_loss: 8.379061660766602\n",
            "Iterations: 4000 (0.6235491132736206 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7739981746673585\n",
            "\ttr_loss: 8.261457328796388\n",
            "Saved model to: checkpoint.4000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7652, device='cuda:0')\n",
            "  tr_loss = tensor(7.5779, device='cuda:0')\n",
            "\n",
            "[4000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  иого там не було .\n",
            "\t==  его там не было .\n",
            "\t>>  наконец название <UNK> <UNK>\n",
            "\t ru  -->  uk\n",
            "\t<<  его там не было .\n",
            "\t==  иого там не було .\n",
            "\t>>  нарешті твору <UNK>\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 4050 (0.7246726560592651 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7738177013397216\n",
            "\ttr_loss: 8.16872971534729\n",
            "Iterations: 4100 (0.5953720760345459 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7715547132492064\n",
            "\ttr_loss: 8.225474796295167\n",
            "Iterations: 4150 (0.6065176153182983 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7731860876083374\n",
            "\ttr_loss: 8.148948631286622\n",
            "Iterations: 4200 (0.5890604114532471 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7725809144973756\n",
            "\ttr_loss: 8.177819519042968\n",
            "Iterations: 4250 (0.5860116338729858 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7709715509414674\n",
            "\ttr_loss: 8.002915391921997\n",
            "Iterations: 4300 (0.572458291053772 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.775016927719116\n",
            "\ttr_loss: 7.999157342910767\n",
            "Iterations: 4350 (0.5922770690917969 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772249436378479\n",
            "\ttr_loss: 7.893258304595947\n",
            "Iterations: 4400 (0.5965375804901123 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7741496706008912\n",
            "\ttr_loss: 7.7618239212036135\n",
            "Iterations: 4450 (0.5701197004318237 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772058243751526\n",
            "\ttr_loss: 7.840570402145386\n",
            "Iterations: 4500 (0.58696542263031 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773016862869263\n",
            "\ttr_loss: 7.819141149520874\n",
            "Saved model to: checkpoint.4500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7804, device='cuda:0')\n",
            "  tr_loss = tensor(7.9083, device='cuda:0')\n",
            "\n",
            "[4500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  вона в себе .\n",
            "\t==  она не пришла .\n",
            "\t>>  и бессмысленно <UNK> <UNK>\n",
            "\t ru  -->  uk\n",
            "\t<<  она не пришла .\n",
            "\t==  вона в себе .\n",
            "\t>>  і . <UNK> <UNK>\n",
            "BLEU: 0.01\n",
            "\n",
            "Iterations: 4550 (0.6847054290771485 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772584056854248\n",
            "\ttr_loss: 7.729552850723267\n",
            "Iterations: 4600 (0.580478401184082 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7734520387649537\n",
            "\ttr_loss: 7.720133848190308\n",
            "Iterations: 4650 (0.57919189453125 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7713294553756715\n",
            "\ttr_loss: 7.681772117614746\n",
            "Iterations: 4700 (0.6020826387405396 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7729567384719847\n",
            "\ttr_loss: 7.59323655128479\n",
            "Iterations: 4750 (0.6038839197158814 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7722051763534545\n",
            "\ttr_loss: 7.564569282531738\n",
            "Iterations: 4800 (0.6214740133285522 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7723089170455935\n",
            "\ttr_loss: 7.574340839385986\n",
            "Iterations: 4850 (0.629916033744812 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7712876176834107\n",
            "\ttr_loss: 7.453014574050903\n",
            "Iterations: 4900 (0.6139209270477295 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772250728607178\n",
            "\ttr_loss: 7.404128322601318\n",
            "Iterations: 4950 (0.6064106702804566 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.774870529174805\n",
            "\ttr_loss: 7.470361251831054\n",
            "Iterations: 5000 (0.6157374906539917 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.774848074913025\n",
            "\ttr_loss: 7.34217755317688\n",
            "Saved model to: checkpoint.5000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7809, device='cuda:0')\n",
            "  tr_loss = tensor(7.3256, device='cuda:0')\n",
            "\n",
            "[5000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  вони повинні будуть взяти мене .\n",
            "\t==  они должны меня принять . это их правила .\n",
            "\t>>  третии существует и <UNK> и <UNK> <UNK> наконец наконец\n",
            "\t ru  -->  uk\n",
            "\t<<  они должны меня принять . это их правила .\n",
            "\t==  вони повинні будуть взяти мене .\n",
            "\t>>  <UNK> музика твору\n",
            "BLEU: 0.01\n",
            "\n",
            "Iterations: 5050 (0.7212166881561279 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7726143836975097\n",
            "\ttr_loss: 7.334625854492187\n",
            "Iterations: 5100 (0.5837847137451172 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7741385221481325\n",
            "\ttr_loss: 7.292897996902465\n",
            "Iterations: 5150 (0.593810133934021 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7681227207183836\n",
            "\ttr_loss: 7.27731653213501\n",
            "Iterations: 5200 (0.5916344118118286 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7737805604934693\n",
            "\ttr_loss: 7.09311276435852\n",
            "Iterations: 5250 (0.5983000135421753 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773742980957031\n",
            "\ttr_loss: 7.160302572250366\n",
            "Iterations: 5300 (0.5917214393615723 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7724831771850584\n",
            "\ttr_loss: 7.194852342605591\n",
            "Iterations: 5350 (0.595252447128296 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7751630544662476\n",
            "\ttr_loss: 7.092131299972534\n",
            "Iterations: 5400 (0.5939271879196167 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7732240533828736\n",
            "\ttr_loss: 7.072420845031738\n",
            "Iterations: 5450 (0.5908042621612549 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7708236932754517\n",
            "\ttr_loss: 6.9652739524841305\n",
            "Iterations: 5500 (0.5903196668624878 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7723301887512206\n",
            "\ttr_loss: 6.986109313964843\n",
            "Saved model to: checkpoint.5500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7756, device='cuda:0')\n",
            "  tr_loss = tensor(6.7422, device='cuda:0')\n",
            "\n",
            "[5500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  мабуть таки ні .\n",
            "\t==  да не особенно .\n",
            "\t>>  и себя\n",
            "\t ru  -->  uk\n",
            "\t<<  да не особенно .\n",
            "\t==  мабуть таки ні .\n",
            "\t>>  і музика\n",
            "BLEU: 0.01\n",
            "\n",
            "Iterations: 5550 (0.6897482395172119 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773713026046753\n",
            "\ttr_loss: 6.9958936214447025\n",
            "Iterations: 5600 (0.6086709880828858 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7724574184417725\n",
            "\ttr_loss: 6.91539361000061\n",
            "Iterations: 5650 (0.6096334648132324 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773935194015503\n",
            "\ttr_loss: 6.878384914398193\n",
            "Iterations: 5700 (0.6368601655960083 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.774013137817383\n",
            "\ttr_loss: 6.7757487297058105\n",
            "Iterations: 5750 (0.6239099979400635 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.77147340297699\n",
            "\ttr_loss: 6.727888536453247\n",
            "Iterations: 5800 (0.6349130821228027 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7735484981536866\n",
            "\ttr_loss: 6.741604270935059\n",
            "Iterations: 5850 (0.632667293548584 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771187129020691\n",
            "\ttr_loss: 6.76085132598877\n",
            "Iterations: 5900 (0.6472651529312133 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.77407931804657\n",
            "\ttr_loss: 6.652530708312988\n",
            "Iterations: 5950 (0.6247276544570923 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772495069503784\n",
            "\ttr_loss: 6.67509048461914\n",
            "Iterations: 6000 (0.6103583955764771 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7730093145370485\n",
            "\ttr_loss: 6.633326930999756\n",
            "Saved model to: checkpoint.6000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7677, device='cuda:0')\n",
            "  tr_loss = tensor(6.5235, device='cuda:0')\n",
            "\n",
            "[6000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  може . . . може б мене тут не було . . .\n",
            "\t==  я не был бы здесь .\n",
            "\t>>  существует бессмысленно тип тип тип\n",
            "\t ru  -->  uk\n",
            "\t<<  я не был бы здесь .\n",
            "\t==  може . . . може б мене тут не було . . .\n",
            "\t>>  третіи себе <UNK> і <UNK> <UNK> і <UNK> і і тип нарешті третіи\n",
            "BLEU: 0.01\n",
            "\n",
            "Iterations: 6050 (0.7128678369522095 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772312889099121\n",
            "\ttr_loss: 6.637421522140503\n",
            "Iterations: 6100 (0.5840449953079223 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7723381185531615\n",
            "\ttr_loss: 6.707612829208374\n",
            "Iterations: 6150 (0.5741834163665771 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773086223602295\n",
            "\ttr_loss: 6.596471042633056\n",
            "Iterations: 6200 (0.586582956314087 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772134189605713\n",
            "\ttr_loss: 6.603776044845581\n",
            "Iterations: 6250 (0.5881501388549805 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7713599586486817\n",
            "\ttr_loss: 6.505963973999023\n",
            "Iterations: 6300 (0.5957170438766479 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.774152307510376\n",
            "\ttr_loss: 6.503085870742797\n",
            "Iterations: 6350 (0.596434030532837 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7746530199050903\n",
            "\ttr_loss: 6.452764892578125\n",
            "Iterations: 6400 (0.6084680318832397 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7731635856628416\n",
            "\ttr_loss: 6.35923752784729\n",
            "Iterations: 6450 (0.6124419689178466 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7730415439605713\n",
            "\ttr_loss: 6.335666790008545\n",
            "Iterations: 6500 (0.6100589036941528 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7719576358795166\n",
            "\ttr_loss: 6.356646080017089\n",
            "Saved model to: checkpoint.6500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7923, device='cuda:0')\n",
            "  tr_loss = tensor(6.2834, device='cuda:0')\n",
            "\n",
            "[6500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  що ти сказав ?\n",
            "\t==  что ты сказал ?\n",
            "\t>>  наконец и собои <UNK> наконец <UNK> наконец наконец <UNK> <UNK> наконец наконец наконец наконец наконец наконец наконец наконец\n",
            "\t ru  -->  uk\n",
            "\t<<  что ты сказал ?\n",
            "\t==  що ти сказав ?\n",
            "\t>>  нарешті і музика <UNK> нарешті <UNK> нарешті нарешті нарешті <UNK> <UNK>\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 6550 (0.7295236825942993 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7733829689025877\n",
            "\ttr_loss: 6.255384254455566\n",
            "Iterations: 6600 (0.6169857263565064 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7721199703216555\n",
            "\ttr_loss: 6.2941617679595945\n",
            "Iterations: 6650 (0.5799712896347046 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7717988204956057\n",
            "\ttr_loss: 6.260896253585815\n",
            "Iterations: 6700 (0.586162486076355 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7730790472030638\n",
            "\ttr_loss: 6.297813367843628\n",
            "Iterations: 6750 (0.5853223371505737 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7725205183029176\n",
            "\ttr_loss: 6.1774560165405275\n",
            "Iterations: 6800 (0.5844353485107422 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772911128997803\n",
            "\ttr_loss: 6.175243272781372\n",
            "Iterations: 6850 (0.5839102506637573 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772225260734558\n",
            "\ttr_loss: 6.256945199966431\n",
            "Iterations: 6900 (0.5919880199432374 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772331156730652\n",
            "\ttr_loss: 6.166651439666748\n",
            "Iterations: 6950 (0.5945151948928833 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773599486351013\n",
            "\ttr_loss: 6.087115411758423\n",
            "Iterations: 7000 (0.5853413677215576 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7701828050613404\n",
            "\ttr_loss: 6.134455633163452\n",
            "Saved model to: checkpoint.7000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.8221, device='cuda:0')\n",
            "  tr_loss = tensor(6.0404, device='cuda:0')\n",
            "\n",
            "[7000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  і до того ж ми адже иого ще не знаишли чи не так ?\n",
            "\t==  и к тому же мы ведь его еще не нашли не правда ли ?\n",
            "\t>>  \n",
            "\t ru  -->  uk\n",
            "\t<<  и к тому же мы ведь его еще не нашли не правда ли ?\n",
            "\t==  і до того ж ми адже иого ще не знаишли чи не так ?\n",
            "\t>>  сама не що і що і і що і тип що існує що тип це музика третіи\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 7050 (0.6928698110580445 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773999662399292\n",
            "\ttr_loss: 6.1009996509552\n",
            "Iterations: 7100 (0.5839844131469727 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7726965141296387\n",
            "\ttr_loss: 6.09964015007019\n",
            "Iterations: 7150 (0.5816636419296265 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.77298050403595\n",
            "\ttr_loss: 6.00683406829834\n",
            "Iterations: 7200 (0.5827312469482422 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772748489379883\n",
            "\ttr_loss: 5.970629568099976\n",
            "Iterations: 7250 (0.5729119873046875 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772215085029602\n",
            "\ttr_loss: 6.0357076454162595\n",
            "Iterations: 7300 (0.5588407945632935 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7724675798416136\n",
            "\ttr_loss: 5.967752571105957\n",
            "Iterations: 7350 (0.5632871055603027 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7718787240982055\n",
            "\ttr_loss: 5.922312879562378\n",
            "Iterations: 7400 (0.5872278261184692 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7733187866210938\n",
            "\ttr_loss: 5.936201324462891\n",
            "Iterations: 7450 (0.568784794807434 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7711618614196776\n",
            "\ttr_loss: 5.936235303878784\n",
            "Iterations: 7500 (0.5782277727127075 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7735380506515503\n",
            "\ttr_loss: 5.941195011138916\n",
            "Saved model to: checkpoint.7500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7913, device='cuda:0')\n",
            "  tr_loss = tensor(5.7060, device='cuda:0')\n",
            "\n",
            "[7500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  ти це про що ?\n",
            "\t==  ты это о чем ?\n",
            "\t>>  <UNK> наконец название\n",
            "\t ru  -->  uk\n",
            "\t<<  ты это о чем ?\n",
            "\t==  ти це про що ?\n",
            "\t>>  <UNK> нарешті себе <UNK> нарешті\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 7550 (0.6662274265289306 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771690673828125\n",
            "\ttr_loss: 5.880314693450928\n",
            "Iterations: 7600 (0.580895938873291 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.77434588432312\n",
            "\ttr_loss: 5.827897806167602\n",
            "Iterations: 7650 (0.5619248342514038 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.77231436252594\n",
            "\ttr_loss: 5.920604934692383\n",
            "Iterations: 7700 (0.5614774942398071 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772696781158447\n",
            "\ttr_loss: 5.928883838653564\n",
            "Iterations: 7750 (0.5733809471130371 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7720597648620604\n",
            "\ttr_loss: 5.777434043884277\n",
            "Iterations: 7800 (0.5653093433380127 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7716059827804567\n",
            "\ttr_loss: 5.768945093154907\n",
            "Iterations: 7850 (0.5749187088012695 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7721180629730227\n",
            "\ttr_loss: 5.797885227203369\n",
            "Iterations: 7900 (0.5792268323898315 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7726693677902223\n",
            "\ttr_loss: 5.7286439037323\n",
            "Iterations: 7950 (0.5726247262954712 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772146644592285\n",
            "\ttr_loss: 5.759152631759644\n",
            "Iterations: 8000 (0.5738638830184937 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7722562551498413\n",
            "\ttr_loss: 5.694067087173462\n",
            "Saved model to: checkpoint.8000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7849, device='cuda:0')\n",
            "  tr_loss = tensor(5.2631, device='cuda:0')\n",
            "\n",
            "[8000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  це я першии чоловік !\n",
            "\t==  это я первыи муж !\n",
            "\t>>  и наконец которая и <UNK> и и и и и наконец и\n",
            "\t ru  -->  uk\n",
            "\t<<  это я первыи муж !\n",
            "\t==  це я першии чоловік !\n",
            "\t>>  <UNK> нарешті існує <UNK> <UNK> <UNK> третіи і нарешті третіи\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 8050 (0.6884377813339233 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773459644317627\n",
            "\ttr_loss: 5.690124940872193\n",
            "Iterations: 8100 (0.5855966520309448 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7715445137023926\n",
            "\ttr_loss: 5.719134178161621\n",
            "Iterations: 8150 (0.5800027465820312 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772614388465881\n",
            "\ttr_loss: 5.61566577911377\n",
            "Iterations: 8200 (0.5861362504959107 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.774086284637451\n",
            "\ttr_loss: 5.672643117904663\n",
            "Iterations: 8250 (0.5932546997070313 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7708742666244506\n",
            "\ttr_loss: 5.657025623321533\n",
            "Iterations: 8300 (0.5959636926651001 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771182813644409\n",
            "\ttr_loss: 5.67592643737793\n",
            "Iterations: 8350 (0.5758081102371215 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772455959320068\n",
            "\ttr_loss: 5.5996501255035405\n",
            "Iterations: 8400 (0.5762294864654541 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7719260358810427\n",
            "\ttr_loss: 5.586896867752075\n",
            "Iterations: 8450 (0.5690369701385498 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773252468109131\n",
            "\ttr_loss: 5.56247760772705\n",
            "Iterations: 8500 (0.5483216714859008 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771930775642395\n",
            "\ttr_loss: 5.588845958709717\n",
            "Saved model to: checkpoint.8500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7640, device='cuda:0')\n",
            "  tr_loss = tensor(6.2591, device='cuda:0')\n",
            "\n",
            "[8500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  але в нас немає кіно .\n",
            "\t==  но у нас нет кино .\n",
            "\t>>  и третии даже и <UNK> <UNK> и и наконец и и <UNK>\n",
            "\t ru  -->  uk\n",
            "\t<<  но у нас нет кино .\n",
            "\t==  але в нас немає кіно .\n",
            "\t>>  і третіи не і і і нарешті\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 8550 (0.6622558641433716 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7727540826797483\n",
            "\ttr_loss: 5.571710948944092\n",
            "Iterations: 8600 (0.5634515142440796 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7726365756988525\n",
            "\ttr_loss: 5.47794659614563\n",
            "Iterations: 8650 (0.5718885278701782 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7736060190200806\n",
            "\ttr_loss: 5.501972160339355\n",
            "Iterations: 8700 (0.5704421520233154 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7721601629257204\n",
            "\ttr_loss: 5.498723134994507\n",
            "Iterations: 8750 (0.5644026517868042 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771717281341553\n",
            "\ttr_loss: 5.466266822814942\n",
            "Iterations: 8800 (0.5624846935272216 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7740195894241335\n",
            "\ttr_loss: 5.422270097732544\n",
            "Iterations: 8850 (0.5834166336059571 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7730715894699096\n",
            "\ttr_loss: 5.384688949584961\n",
            "Iterations: 8900 (0.5464129066467285 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7721015644073486\n",
            "\ttr_loss: 5.5276913070678715\n",
            "Iterations: 8950 (0.5693395805358886 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7727013206481934\n",
            "\ttr_loss: 5.460853614807129\n",
            "Iterations: 9000 (0.553064923286438 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7745697498321533\n",
            "\ttr_loss: 5.404910583496093\n",
            "Saved model to: checkpoint.9000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7592, device='cuda:0')\n",
            "  tr_loss = tensor(5.3091, device='cuda:0')\n",
            "\n",
            "[9000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  тоді я пішов .\n",
            "\t==  тогда я пошел .\n",
            "\t>>  и и музыку <UNK> <UNK> и\n",
            "\t ru  -->  uk\n",
            "\t<<  тогда я пошел .\n",
            "\t==  тоді я пішов .\n",
            "\t>>  і що і <UNK> <UNK> <UNK> і <UNK> <UNK> <UNK> <UNK>\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 9050 (0.6611811542510986 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7720880699157715\n",
            "\ttr_loss: 5.433636445999145\n",
            "Iterations: 9100 (0.5689319372177124 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7729492664337156\n",
            "\ttr_loss: 5.403016004562378\n",
            "Iterations: 9150 (0.5687915277481079 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7717415285110474\n",
            "\ttr_loss: 5.4102373218536375\n",
            "Iterations: 9200 (0.5740753602981568 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7717950773239135\n",
            "\ttr_loss: 5.389501600265503\n",
            "Iterations: 9250 (0.5787440109252929 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.774225354194641\n",
            "\ttr_loss: 5.375108661651612\n",
            "Iterations: 9300 (0.5703747224807739 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7721406841278076\n",
            "\ttr_loss: 5.327800245285034\n",
            "Iterations: 9350 (0.5830664777755737 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771122221946716\n",
            "\ttr_loss: 5.303065242767334\n",
            "Iterations: 9400 (0.5764303636550904 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7742364263534545\n",
            "\ttr_loss: 5.312798995971679\n",
            "Iterations: 9450 (0.5750352287292481 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7731245613098143\n",
            "\ttr_loss: 5.299746875762939\n",
            "Iterations: 9500 (0.5915185594558716 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7716085052490236\n",
            "\ttr_loss: 5.3219139099121096\n",
            "Saved model to: checkpoint.9500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7781, device='cuda:0')\n",
            "  tr_loss = tensor(5.6710, device='cuda:0')\n",
            "\n",
            "[9500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  от це и відбулося .\n",
            "\t==  вот это и произошло .\n",
            "\t>>  <UNK> наконец которая <UNK>\n",
            "\t ru  -->  uk\n",
            "\t<<  вот это и произошло .\n",
            "\t==  от це и відбулося .\n",
            "\t>>  <UNK> нарешті що <UNK> <UNK>\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 9550 (0.6828573751449585 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.775049548149109\n",
            "\ttr_loss: 5.237572660446167\n",
            "Iterations: 9600 (0.5898678493499756 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.770063905715942\n",
            "\ttr_loss: 5.206565284729004\n",
            "Iterations: 9650 (0.5745215129852295 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7712681531906127\n",
            "\ttr_loss: 5.309641551971436\n",
            "Iterations: 9700 (0.579191403388977 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773038554191589\n",
            "\ttr_loss: 5.220220985412598\n",
            "Iterations: 9750 (0.560330843925476 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771086587905884\n",
            "\ttr_loss: 5.270243654251098\n",
            "Iterations: 9800 (0.5742554998397827 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.774368929862976\n",
            "\ttr_loss: 5.2104754066467285\n",
            "Iterations: 9850 (0.5528860092163086 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7724322366714476\n",
            "\ttr_loss: 5.22744909286499\n",
            "Iterations: 9900 (0.5903146409988403 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7702278661727906\n",
            "\ttr_loss: 5.179818449020385\n",
            "Iterations: 9950 (0.5642442989349366 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7715382862091062\n",
            "\ttr_loss: 5.2121689510345455\n",
            "Iterations: 10000 (0.5792158603668213 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7724125671386717\n",
            "\ttr_loss: 5.1251567840576175\n",
            "Saved model to: checkpoint.10000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7787, device='cuda:0')\n",
            "  tr_loss = tensor(5.2024, device='cuda:0')\n",
            "\n",
            "[10000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  ви иого отримали .\n",
            "\t==  вы его получили .\n",
            "\t>>  <UNK> наконец которая <UNK>\n",
            "\t ru  -->  uk\n",
            "\t<<  вы его получили .\n",
            "\t==  ви иого отримали .\n",
            "\t>>  <UNK> нарешті існує <UNK>\n",
            "BLEU: 0.01\n",
            "\n",
            "Iterations: 10050 (0.661544098854065 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771441822052002\n",
            "\ttr_loss: 5.130658864974976\n",
            "Iterations: 10100 (0.5726466369628906 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.770775375366211\n",
            "\ttr_loss: 5.139678602218628\n",
            "Iterations: 10150 (0.5616106557846069 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7731979656219483\n",
            "\ttr_loss: 5.179492607116699\n",
            "Iterations: 10200 (0.5687174892425537 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7719700860977174\n",
            "\ttr_loss: 5.153550386428833\n",
            "Iterations: 10250 (0.566520266532898 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.768679184913635\n",
            "\ttr_loss: 5.123899393081665\n",
            "Iterations: 10300 (0.5630198574066162 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7739806842803953\n",
            "\ttr_loss: 5.141582860946655\n",
            "Iterations: 10350 (0.5643256044387818 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7715730047225953\n",
            "\ttr_loss: 5.174538583755493\n",
            "Iterations: 10400 (0.5779190874099731 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773429579734802\n",
            "\ttr_loss: 5.137653007507324\n",
            "Iterations: 10450 (0.581694016456604 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7700177669525146\n",
            "\ttr_loss: 5.109048547744751\n",
            "Iterations: 10500 (0.5806373023986816 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7725085735321047\n",
            "\ttr_loss: 5.13632908821106\n",
            "Saved model to: checkpoint.10500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7769, device='cuda:0')\n",
            "  tr_loss = tensor(5.1536, device='cuda:0')\n",
            "\n",
            "[10500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  що роблять тут ці люди ?\n",
            "\t==  что здесь делает человек ?\n",
            "\t>>  \n",
            "\t ru  -->  uk\n",
            "\t<<  что здесь делает человек ?\n",
            "\t==  що роблять тут ці люди ?\n",
            "\t>>  і нарешті себе і нарешті <UNK> нарешті <UNK> <UNK> нарешті нарешті і нарешті і <UNK> і <UNK> і <UNK> і <UNK> <UNK> <UNK> <UNK> і\n",
            "BLEU: 0.02\n",
            "\n",
            "Iterations: 10550 (0.6863563251495362 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7722629165649413\n",
            "\ttr_loss: 5.120859451293946\n",
            "Iterations: 10600 (0.5867807388305664 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7715685510635377\n",
            "\ttr_loss: 5.0669572639465335\n",
            "Iterations: 10650 (0.5763610601425171 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7711097145080568\n",
            "\ttr_loss: 5.0800769329071045\n",
            "Iterations: 10700 (0.5772795915603638 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771778826713562\n",
            "\ttr_loss: 5.034601554870606\n",
            "Iterations: 10750 (0.5740739345550537 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7711015367507934\n",
            "\ttr_loss: 4.998854246139526\n",
            "Iterations: 10800 (0.5781908226013184 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7706859159469603\n",
            "\ttr_loss: 5.009757804870605\n",
            "Iterations: 10850 (0.5474696111679077 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7720295429229735\n",
            "\ttr_loss: 5.084781904220581\n",
            "Iterations: 10900 (0.581964373588562 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7722536516189575\n",
            "\ttr_loss: 5.011133613586426\n",
            "Iterations: 10950 (0.5856588077545166 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7697911930084227\n",
            "\ttr_loss: 4.926136960983277\n",
            "Iterations: 11000 (0.579051423072815 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771503520011902\n",
            "\ttr_loss: 4.995422105789185\n",
            "Saved model to: checkpoint.11000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7835, device='cuda:0')\n",
            "  tr_loss = tensor(5.0148, device='cuda:0')\n",
            "\n",
            "[11000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  а якщо ні ?\n",
            "\t==  а если нет ?\n",
            "\t>>  и и бессмысленно наконец <UNK> <UNK> наконец и <UNK> и <UNK> наконец <UNK>\n",
            "\t ru  -->  uk\n",
            "\t<<  а если нет ?\n",
            "\t==  а якщо ні ?\n",
            "\t>>  <UNK> і такого\n",
            "BLEU: 0.01\n",
            "\n",
            "Iterations: 11050 (0.6917129564285278 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773272647857666\n",
            "\ttr_loss: 4.934945106506348\n",
            "Iterations: 11100 (0.5578584909439087 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772491216659546\n",
            "\ttr_loss: 4.9662614345550535\n",
            "Iterations: 11150 (0.5656635284423828 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7742274093627928\n",
            "\ttr_loss: 5.0118372440338135\n",
            "Iterations: 11200 (0.5639729928970337 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773159871101379\n",
            "\ttr_loss: 4.982205657958985\n",
            "Iterations: 11250 (0.571127552986145 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7683792066574098\n",
            "\ttr_loss: 4.925279731750488\n",
            "Iterations: 11300 (0.5825376796722412 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.774265751838684\n",
            "\ttr_loss: 4.897200365066528\n",
            "Iterations: 11350 (0.5851183462142945 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773121700286865\n",
            "\ttr_loss: 4.927956638336181\n",
            "Iterations: 11400 (0.6003932857513428 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771966300010681\n",
            "\ttr_loss: 4.849207201004028\n",
            "Iterations: 11450 (0.580781078338623 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771839389801025\n",
            "\ttr_loss: 4.9290120315551755\n",
            "Iterations: 11500 (0.5867904043197631 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773964076042175\n",
            "\ttr_loss: 4.950868663787841\n",
            "Saved model to: checkpoint.11500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7615, device='cuda:0')\n",
            "  tr_loss = tensor(4.9666, device='cuda:0')\n",
            "\n",
            "[11500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  та ви що ?\n",
            "\t==  да что вы .\n",
            "\t>>  и и музыку и <UNK> <UNK> наконец и и и\n",
            "\t ru  -->  uk\n",
            "\t<<  да что вы .\n",
            "\t==  та ви що ?\n",
            "\t>>  <UNK> і існує <UNK>\n",
            "BLEU: 0.01\n",
            "\n",
            "Iterations: 11550 (0.6995494222640991 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7742046546936034\n",
            "\ttr_loss: 4.891905174255371\n",
            "Iterations: 11600 (0.5802291011810303 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.770612711906433\n",
            "\ttr_loss: 4.896098899841308\n",
            "Iterations: 11650 (0.579527678489685 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772224335670471\n",
            "\ttr_loss: 4.904119081497193\n",
            "Iterations: 11700 (0.5631170940399169 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772887258529663\n",
            "\ttr_loss: 4.828368501663208\n",
            "Iterations: 11750 (0.5606575727462768 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.770913505554199\n",
            "\ttr_loss: 4.898078279495239\n",
            "Iterations: 11800 (0.5527118301391601 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773320937156677\n",
            "\ttr_loss: 4.859690475463867\n",
            "Iterations: 11850 (0.5684442138671875 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7734723234176637\n",
            "\ttr_loss: 4.831680393218994\n",
            "Iterations: 11900 (0.5760168552398681 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7716605997085573\n",
            "\ttr_loss: 4.801304950714111\n",
            "Iterations: 11950 (0.5687737560272217 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7726274871826173\n",
            "\ttr_loss: 4.84567479133606\n",
            "Iterations: 12000 (0.5792672681808472 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7717560958862304\n",
            "\ttr_loss: 4.806429004669189\n",
            "Saved model to: checkpoint.12000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7877, device='cuda:0')\n",
            "  tr_loss = tensor(4.5686, device='cuda:0')\n",
            "\n",
            "[12000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  звідки він знає що тут повинен бути будинок . . .\n",
            "\t==  откуда он знает что тут должен быть дом . . .\n",
            "\t>>  которая название третии тип третии третии третии\n",
            "\t ru  -->  uk\n",
            "\t<<  откуда он знает что тут должен быть дом . . .\n",
            "\t==  звідки він знає що тут повинен бути будинок . . .\n",
            "\t>>  існує назва третіи третіи третіи третіи <UNK> третіи нарешті третіи третіи тип музика тип\n",
            "BLEU: 0.01\n",
            "\n",
            "Iterations: 12050 (0.6818865442276001 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771239790916443\n",
            "\ttr_loss: 4.8185204410552975\n",
            "Iterations: 12100 (0.5658713483810425 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7697054862976076\n",
            "\ttr_loss: 4.873944263458252\n",
            "Iterations: 12150 (0.5647187471389771 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772965974807739\n",
            "\ttr_loss: 4.878096971511841\n",
            "Iterations: 12200 (0.5761269092559814 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772152137756348\n",
            "\ttr_loss: 4.748028984069824\n",
            "Iterations: 12250 (0.5869850444793702 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772689151763916\n",
            "\ttr_loss: 4.733106698989868\n",
            "Iterations: 12300 (0.56925057888031 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7721920680999754\n",
            "\ttr_loss: 4.784591856002808\n",
            "Iterations: 12350 (0.5802160692214966 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.774582142829895\n",
            "\ttr_loss: 4.712984294891357\n",
            "Iterations: 12400 (0.563035922050476 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7726581048965455\n",
            "\ttr_loss: 4.753077659606934\n",
            "Iterations: 12450 (0.5454367160797119 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772618794441223\n",
            "\ttr_loss: 4.767806215286255\n",
            "Iterations: 12500 (0.55032320022583 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771073660850525\n",
            "\ttr_loss: 4.7637583351135255\n",
            "Saved model to: checkpoint.12500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7461, device='cuda:0')\n",
            "  tr_loss = tensor(4.7278, device='cuda:0')\n",
            "\n",
            "[12500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  якщо ви людина то остання в світі .\n",
            "\t==  тогда вы последнии человек .\n",
            "\t>>  тип музыку бессмысленно тип <UNK> явит тип тип тип тип тип наконец третии тип собои <UNK> тип тип тип <UNK> тип наконец тип третии третии явит тип <UNK> тип тип наконец третии явит тип тип тип тип и тип тип тип тип тип третии тип тип третии <UNK> наконец наконец и тип тип третии наконец явит тип третии и и <UNK> и тип третии тип тип <UNK> собои собои <UNK> и тип наконец <UNK> и явит\n",
            "\t ru  -->  uk\n",
            "\t<<  тогда вы последнии человек .\n",
            "\t==  якщо ви людина то остання в світі .\n",
            "\t>>  <UNK> нарешті існує <UNK> <UNK> <UNK>\n",
            "BLEU: 0.01\n",
            "\n",
            "Iterations: 12550 (0.6809043169021607 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7726936817169188\n",
            "\ttr_loss: 4.747776718139648\n",
            "Iterations: 12600 (0.5404693126678467 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7714926910400393\n",
            "\ttr_loss: 4.718731565475464\n",
            "Iterations: 12650 (0.56295729637146 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773927621841431\n",
            "\ttr_loss: 4.7260000610351565\n",
            "Iterations: 12700 (0.555105447769165 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7739622831344604\n",
            "\ttr_loss: 4.659647760391235\n",
            "Iterations: 12750 (0.556725172996521 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771599683761597\n",
            "\ttr_loss: 4.763376111984253\n",
            "Iterations: 12800 (0.5511996698379517 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773987736701965\n",
            "\ttr_loss: 4.659870710372925\n",
            "Iterations: 12850 (0.5561478519439698 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7728895425796507\n",
            "\ttr_loss: 4.71434624671936\n",
            "Iterations: 12900 (0.5716251230239868 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.77270565032959\n",
            "\ttr_loss: 4.667304792404175\n",
            "Iterations: 12950 (0.5780559492111206 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7727572774887084\n",
            "\ttr_loss: 4.610082349777222\n",
            "Iterations: 13000 (0.5920663213729859 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7745350646972655\n",
            "\ttr_loss: 4.661133499145508\n",
            "Saved model to: checkpoint.13000\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7946, device='cuda:0')\n",
            "  tr_loss = tensor(4.4424, device='cuda:0')\n",
            "\n",
            "[13000] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  і я б змогла ?\n",
            "\t==  и я бы смогла ?\n",
            "\t>>  и третии существует и и и и и наконец и и и <UNK> наконец и наконец\n",
            "\t ru  -->  uk\n",
            "\t<<  и я бы смогла ?\n",
            "\t==  і я б змогла ?\n",
            "\t>>  і третіи існує і <UNK> нарешті <UNK> і\n",
            "BLEU: 0.01\n",
            "\n",
            "Iterations: 13050 (0.6725386714935303 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771990346908569\n",
            "\ttr_loss: 4.705193204879761\n",
            "Iterations: 13100 (0.5846722555160523 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.770389003753662\n",
            "\ttr_loss: 4.672773065567017\n",
            "Iterations: 13150 (0.5685074472427368 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772568130493164\n",
            "\ttr_loss: 4.603795471191407\n",
            "Iterations: 13200 (0.5545523166656494 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.772670559883118\n",
            "\ttr_loss: 4.674470596313476\n",
            "Iterations: 13250 (0.5411455583572388 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771772174835205\n",
            "\ttr_loss: 4.67685115814209\n",
            "Iterations: 13300 (0.5368574953079224 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.771314749717712\n",
            "\ttr_loss: 4.657797212600708\n",
            "Iterations: 13350 (0.5660047101974487 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.7717509984970095\n",
            "\ttr_loss: 4.654747905731202\n",
            "Iterations: 13400 (0.5838066864013672 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.773618335723877\n",
            "\ttr_loss: 4.594512042999267\n",
            "Iterations: 13450 (0.5856581497192382 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.769413375854492\n",
            "\ttr_loss: 4.6397130489349365\n",
            "Iterations: 13500 (0.569668755531311 sec/iter)\n",
            "Average losses:\n",
            "\td_loss: 2.77478657245636\n",
            "\ttr_loss: 4.594774503707885\n",
            "Saved model to: checkpoint.13500\n",
            "Last loss:\n",
            "  d_loss = tensor(2.7809, device='cuda:0')\n",
            "  tr_loss = tensor(4.7483, device='cuda:0')\n",
            "\n",
            "[13500] Validation:\n",
            "Sample translations:\n",
            "\t uk  -->  ru\n",
            "\t<<  хто це був ?\n",
            "\t==  кто это был ?\n",
            "\t>>  и явит <UNK> <UNK> <UNK> <UNK> наконец <UNK> <UNK> <UNK> наконец\n",
            "\t ru  -->  uk\n",
            "\t<<  кто это был ?\n",
            "\t==  хто це був ?\n",
            "\t>>  і і це і\n",
            "BLEU: 0.01\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-94d00736f01d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Computed encoder and decoder gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;31m# Undo the changes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}