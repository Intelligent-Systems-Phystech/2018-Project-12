\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\NOREVIEWERNOTES
\begin{document}

\title
    [Обучение машинного перевода без параллельных текстов] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Обучение машинного перевода без параллельных текстов}
\author
    [Строганов~А.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Строганов~А.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление
    [Строганов~А.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Работа выполнена при финансовой поддержке РФФИ, проект \No\,00-00-00000.
   Научный руководитель:  Стрижов~В.\,В.
   Авторы: А.В. Грабовой, О.Ю. Бахтеев, В.В. Стрижов, Eric Gaussier, координатор Малиновский Г.С.
    Консультант:  Бахтеев~О.\,Ю.}
%\email
%    {author@site.ru}
\organization
     {$^1$Московский физико-технический институт\par
      $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}

\abstract
    {Данная работа посвящена исследованию машинного перевода без использования параллельных текстов. Исследование сконцентрировано на использовании нейросети с несколькими моделями Seq2seq для перевода с одного языка на другой и обратно. Особенностью данных моделей является то, что исходный текст кодируется во внутреннее представление модели, а затем декодируется в текст на другом языке. Две модели Seq2seq имеют общее скрытое пространство. Примером, иллюстрирующим работоспособность данного алгортма, будет использован эксперимент по взаимному переводу с двух похожих языков -- русского и украинского.
    
    \bigskip
    \textbf{Ключевые слова}: \emph {машинный перевод, нейросеть, Seq2seq}.}
\maketitle
\section{Введение}
Благодаря недавним достижениям в области глубокого обучения и наличию крупномасштабных параллельных корпусов, машинный перевод достиг впечатляющей производительности на нескольких языковых парах. Тем не менее, эти модели работают очень хорошо, только если они снабжены огромным количеством параллельных данных в порядке миллионов параллельных предложений.
К сожалению, параллельные корпуса стоят дорого\cite{koehn2009statistical}, поскольку они требуют специализированного опыта и часто не существуют для языков с низким уровнем ресурсов. 

Есть несколько подходов к построению оптимального метода обучения. Предлагается использовать рекуррентные нейронные сети c короткой и долгой памятью и нейронные сети, в которых реализовано внимание. В других методах используются нейронные сети, которые осуществляют перевод в два этапа. Такой метода называется Seq2Seq\cite{weiss2017sequence}.

Данная работа посвящена последнему методу последовательного перевода. Предлагается с помощью первой рекуррентной нейронной сети, основанной на долгой памяти перевести входящую последовательность в вектор, а с помощью второй перевести этот вектор в выходную последовательность на нужном нам языке\cite{cho2014properties}. Данный метод позволяет
гораздо быстрее обучить нейронную сеть переводу с одного языка на другой, в связи с использованием ей предыдущего опыта и наличию у нее памяти и внимания. Проверка и анализ метода проводятся с помощью алгоритма BLEU(Bilingual evaluation understudy) для проверки качества текста, переведенного с одного языка на другой на паре языков русский-украинский.


\section{Постановка задачи}
Во время обучения нет параллельных пар предложений. Предполагаем, что нам подойдет модель, отображающая предложения из обоих языков в одно общеее векторное пространство.

Мы будем использовать модель, в которой используются главных юнита: encoder $f$ и decoder $g$. $f$ и $g$  в нашей конкретной модели -  две рекуррентные нейронные сети. Задача $f$  -  отображать предложения в  латентное пространство(сразу для двух языков) и $g$  -  отображать из латентного пространства в предложения(первого языка и второго соответственно).  Введем обозначения: $D^{src} = [s_1^{src},...,s_{m_{src}}^{src}]$, $D^{tgt} = [s_1^{tgt},...,s_{m_{tgt}}^{tgt}]$.

Для реализации этого метода определим функционалы, которые будут минимизироваться. Чтобы модель не обучилась возвращать в конце цикла исходные данные, необходимо зашумить исходные предложения. Пусть $\sigma(x)$ - результат наложения шума на слово x.  Оптимизировать будем  следующую функцию:

$$L_{AE} = ||d(e(\sigma(x)))-x||^2$$

Пусть дана какая-то модель слабого перевода $\hat{g}$. На втором шаге  функция потерь будет иметь вид:

$$L_{TR} = ||d(e(\hat{g}(e(x))) - x||^2$$

Пусть дана модель D, различающая скрытые представления векторов предложений из двух языков. На последнем шаге оптимизируем дискримантор, чтобы он различал представления векторов разных языков в скрытом пространстве:

$$L_{ADV} = \log p(lang = src| Encoder(x)) + \log p(lang = tgt|Encoder(y))$$

В итоге нужно минимизировать следующую функцию:

$$L = a*L_{AE} + b*L_{TR}+c*L{ADV} \longrightarrow min$$

здесь $a,b,c$ - калибруемые гиперпараметры.






\bibliography{biblioteka}
\bibliographystyle{plain}


%\linenumbers

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
