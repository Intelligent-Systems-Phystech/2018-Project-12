\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\bibliographystyle{utf8gost705u}
%\NOREVIEWERNOTES
\title
    [Обучение машинного перевода без параллельных текстов] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Обучение машинного перевода без параллельных текстов}
\author
    [Иванов~А.\,В. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
{Иванов~А.\,В. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление
[Иванов~А.\,В. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Научный руководитель:  Стрижов~В.\,В. 
   Авторы: А.В. Грабовой, О.Ю. Бахтеев, В.В. Стрижов, Eric Gaussier, координатор Малиновский~Г.\,С.
   Консультант:  Бахтеев~О.\,Ю.}
\email
    {ivanov.aleksandr@phystech.edu}
\organization
{$^1$Московский физико-технический институт\par
	$^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}
\abstract
{Большая часть моделей машинного обучения, используемых для перевода, в процессе обучения используют параллельные тексты на различных языках. В случае некоторых пар языков не удается построить тренировочную выборку достаточного размера. Для решения этой проблемы были предложены способы тренировки моделей, основанные на автокодировщиках. Каждому предложению из обоих языков ставится в соотвествие некоторый "смысловой" вектор в скрытом пространстве. Модель учится переводить предложения с одного языка на другой при попытках восстановления изначальных предложений по их скрытому представлению. В данной работе проводится исследование предложенного подхода для перевода между двумя близкими языками: русским и украиским.
	\bigskip
	\textbf{Ключевые слова}: \emph {перевод, автокодировщик}.
}

\begin{document}
\maketitle

\section{Введение}

Выбор модели машинного обучения, используемой для перевода, зависит от спецефических особенностей пары языков. Так, использование глубоких нейронных сетей приводит к качественным результатам, но только в том случае, если количество параллельных предложений в обучающей выборке достаточно велико. Были достигнуты результаты уже для миллионной выборки. \cite{zou2013bilingual},\cite{cho2014properties}. 

В случае, когда размер обучающей выборки не достаточен, для ее пополнения может быть использован результат предыдущих итераций обучаемой нейронной сети. Данный результат представлен в \cite{bertoldi2009domain}.

Во многих работах представлено рещение задачи машинного перевода в том случае, когда количества параллельных предложений не достаточно для обучения полноценного обучения глубокой сети \cite{wu2016google}, \cite{sutskever2014sequence}, \cite{bahdanau2014neural}. В данном подходе используются 2 типа автокодировщивок. Первый - рекуррентные нейронные сети LSTM (\cite{graves2005framewise}) используются для перевода слов изначального языка в скрытое векторное пространство. Второй - дискриминатор, восстанавливающий исходное предложение по скрытому внутреннему представлению, являющемуся результатом работы первой сети.

Оптимизация автокодировщиков производится таким образом, чтобы их результирующие представления совпадали. В некотором смысле, это означает искуственное создание пробле для дискриминатора чтобы он не мог с достаточной точностью определить язык по скрытому представлению некоторого предложения. Для борьбы в с переобучением добавляется шум, который не дает возможности абсолютно точного восстановления текста по его представлению после обработки автокодировщиком. Шаг оптимизации состоит из двух частей. Изначально выбирается случайное предложение из исходного языка и кодируется с добавлением шума \cite{kimimproving}) после чего подаётся на вход дискриминатору.Далее аналогичные действия повторяются со случайным предложением из второго языка. На второй стадии выбирается произвольное предложение из исходного языка, переводится текущей моделью на конечный язык. На результат накладывается шум, и для зашумленного предложения выполняется полный обратный перевод, после чего вычисляется функция потерь. Затем аналогичные действия проводятся с произвольно выбранным предложением конечного языка.

Для перевода с русского на французский языки и обратно такой подход был продемонстрирован в \cite{lample2017unsupervised}.

В качестве эксперимента производится перевод предложений с русского языка на украинский. Качество результата оценивается с помощью метрики BLEU \cite{papineni2002bleu}.

\section{Постановка задачи}

Обозначим части модели: $f$ - кодировщик и $g$ - декодировщик, используемые в модели. И $f$ и $g$ представляют собой рекуррентные нейронные сети и отвечают за отображения между предложениями в двух языках и их скрытыми представлениями. 

Рассматриваемая модель состоит из кодировщика $f$ и декодировщика $g$, и отвечающих соответственно за отображение предложений из обоих языков в латентное пространство и обратное отображение из латентного пространства в предложения первого или второго языка. Кодировщик и декодировщик реализованы в виде рекуррентных нейронных сетей. Так как процедуры перевода из разных языков требуют разные словари, будем обозначать это индексами: $f^{\text{src}}$ и $g^{\text{src}}$ для автокодировщика первого языка и $f^{\text{tgt}}$ и $g^{\text{tgt}}$ для автокодировщика первого языка. Будут рассматриваться следующие входные данные: $\mathfrak{D}^{\text{src}} = [\mathbf{s}_1^{\text{src}}, \dots, \mathbf{s}_{m_\text{src}}^{\text{src}}]$ - набор предложений из первого языка, $\mathfrak{D}^{\text{tgt}} = [\mathbf{s}_1^{\text{tgt}}, \dots, \mathbf{s}_{m_\text{tgt}}^{\text{tgt}}]$ - набор предложений второго языка. Два предыдущих набора не обязаны быть параллельными. $\mathfrak{D}^{valid} = \{(\mathbf{s}_1^{\text{src}}, \mathbf{s}_1^{\text{tgt}}), \dots, (\mathbf{s}_{m_\text{valid}}^{\text{src}}, \mathbf{s}_{m_\text{valid}}^{\text{tgt}})\}$ - валидационная выборка, являющаяся параллельным набором предложений.

Далее определеним функционалы, которые будут подвергнуты минимизации. Пусть $\sigma(t)$ - функция зашумления аргумента, применяемая перед началом обратного перевода модели. Понятие нормы в данном случае строго не определено, но будем считать это некоторым растоянием.

$$L_{FULL} = ||g(f(\sigma(x)))-x||$$

При пословном переводе функция потерь будет иметь вид

$$L_{WORD} = ||g(f(g^{-1}(f(x))) - x||$$

И самое сложное - оптимизация дискриминатора для того, чтобы он мог отличать представления различных языков в скрытом пространстве:

$$L_{G} = \log \mathbb{P}(\textbf{lang} = 1| g(x)) + \log \mathbb{P}(\textbf{lang} = 2|g(y))$$

Итоговая функция ошибки принимает следующий вид:
$$L = (L_{FULL}, L_{WORD}, L_{G})^T \cdot w \to \text{min}$$

$w$ рассматривается как вектор весов "значимости" штрафа.



\bibliography{references}

%\linenumbers

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
