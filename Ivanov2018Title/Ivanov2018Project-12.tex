\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\bibliographystyle{utf8gost705u}
%\NOREVIEWERNOTES
\title
    [Обучение машинного перевода без параллельных текстов] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Обучение машинного перевода без параллельных текстов}
\author
    [Иванов~А.\,В. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
{Иванов~А.\,В. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление
[Иванов~А.\,В. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Научный руководитель:  Стрижов~В.\,В. 
   Авторы: А.В. Грабовой, О.Ю. Бахтеев, В.В. Стрижов, Eric Gaussier, координатор Малиновский~Г.\,С.
   Консультант:  Бахтеев~О.\,Ю.}
\email
    {ivanov.aleksandr@phystech.edu}
\organization
{$^1$Московский физико-технический институт\par
	$^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}
\abstract
{Решается задача машинного перевода между двумя языками. В случае некоторых пар языков не удается построить тренировочную выборку из параллельных текстов достаточного размера. Для решения этой проблемы были предложены способы оптимизации моделей, основанные на автокодировщиках. Каждому предложению из обоих языков из параллельных текстов ставится в соотвествие некоторый "смысловой" вектор в скрытом пространстве. Модель оптимизируется для перевода с одного языка на другой при попытках восстановления изначальных предложений по их скрытому представлению. В данной работе проводится исследование предложенного подхода для перевода между двумя близкими языками: русским и украиским.
	\bigskip \newline
	\textbf{Ключевые слова}: \emph {перевод, автокодировщик}.
}

\begin{document}
\maketitle

\section{Введение}

Выбор модели машинного обучения, используемой для перевода, зависит от спецефических особенностей пары языков. Так, использование глубоких нейронных сетей приводит к качественным результатам, но только в том случае, если количество параллельных предложений в обучающей выборке достаточно велико. В работах  \cite{zou2013bilingual, cho2014properties} были достигнуты результаты для милионной выборки. 

В случае, когда размер обучающей выборки не достаточен, для ее пополнения может быть использован результат предыдущих итераций обучаемой нейронной сети. Данный результат представлен в \cite{bertoldi2009domain}.

Во многих работах представлено решение задачи машинного перевода в том случае, когда количества параллельных предложений не достаточно для построения и оптимизации  глубокой сети \cite{wu2016google, sutskever2014sequence, bahdanau2014neural}. В данном подходе используются 2 типа моделей машинного обучения. Первый - рекуррентные нейронные сети LSTM \cite{graves2005framewise} используются для перевода слов изначального языка в скрытое векторное пространство. Второй - дискриминатор, восстанавливающий исходное предложение по скрытому внутреннему представлению, являющемуся результатом работы первой сети.

Оптимизация проводится в состязательном режиме. Дискриминатор модели минимизирует разницу между скрытыми представлениями предложений из двух языков. Для борьбы с переобучением добавляется шум, который не дает возможности абсолютно точного восстановления текста по его представлению после обработки автокодировщиком. Шаг оптимизации состоит из двух частей. Изначально выбирается случайное предложение из исходного языка и кодируется с добавлением шума \cite{kimimproving} после чего подаётся на вход дискриминатору. Далее аналогичная процедура со случайным предложением из второго языка. На второй стадии выбирается произвольное предложение из исходного языка, переводится текущей моделью на конечный язык. На результат накладывается шум, после чего для него считавыются показания дискриминатора. После шага оптимизации предложение переводится обратно в исходный язык и вычисляется занчение функции потерь. После этого аналогичные действия посторяются со случайным предложением второго языка.

Для перевода с русского на французский языки и обратно такой подход был продемонстрирован в \cite{lample2017unsupervised}.

В качестве эксперимента производится перевод предложений с русского языка на украинский. Качество результата оценивается с помощью метрики BLEU \cite{papineni2002bleu}.

\section{Постановка задачи}

Рассматриваемая модель состоит из кодировщика $\mathbb{f}$ и декодировщика $\mathbb{g}$, и отвечающих соответственно за отображение предложений из обоих языков в латентное пространство и обратное отображение из латентного пространства в предложения первого или второго языка. Кодировщик и декодировщик реализованы в виде рекуррентных нейронных сетей. Будем рассматривать модели кодировщиков и декодировщиков для обоих языков: $f^\text{src}$, $f^\text{tgt}$ и $g^\text{src}$, $g^\text{tgt}$ соответственно. Модели имеют общие параметры, но отличаются входными словарями, разными для каждого языка. Пусть заданы следующие выборки: $\mathfrak{D}^{\text{src}} = [\mathbf{s}_1^{\text{src}}, \dots, \mathbf{s}_{m_\text{src}}^{\text{src}}]$ - набор предложений из первого языка, $\mathfrak{D}^{\text{tgt}} = [\mathbf{s}_1^{\text{tgt}}, \dots, \mathbf{s}_{m_\text{tgt}}^{\text{tgt}}]$ - набор предложений второго языка. Два предыдущих набора не обязаны быть параллельными. Так же есть валидационная выборка, являющаяся набором параллельных предложений. $\mathfrak{D}^{valid} = \{(\mathbf{s}_1^{\text{src}}, \mathbf{s}_1^{\text{tgt}}), \dots, (\mathbf{s}_{m_\text{valid}}^{\text{src}}, \mathbf{s}_{m_\text{valid}}^{\text{tgt}})\}$.

Далее определеним функционалы, которые будут подвергнуты минимизации, которые и будут представлять собой функцию потерь для модели.  Пусть $\sigma$ - функция зашумления аргумента, применяемая перед началом обратного перевода модели. В качестве нормы может быть использована кросс-энтропия или стандартная $L_2$ норма..

$$L_{\text{sent}} = ||\mathbf{g}(\mathbf{f}(\sigma(x)))-x||$$

При пословном переводе функция потерь будет иметь вид

$$L_{\text{word}} = ||\mathbf{g}(\mathbf{f}(\mathbf{g}^{-1}(\mathbf{f}(x))) - x||$$

Оптимизация дискриминатора для того, чтобы он мог отличать представления различных языков в скрытом пространстве: 

$$L_{G} = \log \mathbb{P}(\mathbf{f}_{\text{lang}} = 1| \mathbf{g}(x)) + \log \mathbb{P}(\mathbf{f}_{\text{lang}} = 2|\mathbf{g}(y))$$

Итоговая функция ошибки принимает следующий вид:
$$L = (L_{sent}, L_{word}, L_{G})^T \cdot \mathbf{w} \to \text{min}$$

$\mathbf{w}$ рассматривается как вектор весов "значимости" штрафа.



\bibliography{references}

%\linenumbers

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
