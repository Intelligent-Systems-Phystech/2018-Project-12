\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\NOREVIEWERNOTES
\title
    [Обучение машинного перевода без параллельных текстов] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Обучение машинного перевода без параллельных текстов}
\author
    [Скиднов~Е.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Скиднов~Е.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление
    [Скиднов~Е.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Работа выполнена при финансовой поддержке РФФИ, проект \No\,00-00-00000.
   Научный руководитель:  Стрижов~В.\,В.
   Авторы: А.В. Грабовой, О.Ю. Бахтеев, В.В. Стрижов, Eric Gaussier, координатор Малиновский Г.С.
    Консультант:  Бахтеев~О.\,Ю.}
%\email
%    {author@site.ru}
\organization
     {$^1$Московский физико-технический институт\par
      $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}

\abstract
    {Данная задача посвящена исследованию алгоритма обучения машинного перевода без параллельных предложений. Использование параллельных текстов для задачи машинного перевода требует слишком большой базы предложений всех переводимых языков, что является ресурсоемкой задачей для некоторых пар непохожих языков. Особенностью исследуемого алгоритма является то, что для перевода используется кодировние и декодирование текста во внутреннем представлении. Данный алгоритм использует 2 модели нейронной сети Seq2Seq для перевода с одного языка на другой и обратно. Цель данного исследования заключается в том, чтобы сделать векторы скрытых пространств этих двух моделей как можно более близкими. Для демонстрации работоспособности метода используется вычислительный эксперимент машинного перевода между двумя похожими языками: русским и украинским.

    \bigskip
    \textbf{Ключевые слова}: \emph {машинный перевод, нейросеть, Seq2Seq}.}
\begin{document}
\maketitle
\section{Введение}
    Благодаря недавним достижениям в области глубокого обучения и наличию крупномасштабных параллельных корпусов, машинный перевод достиг впечатляющей производительности на нескольких языковых парах. Тем не менее, эти модели работают очень хорошо, только если они снабжены огромным количеством параллельных данных в порядке миллионов параллельных предложений. К сожалению, параллельные корпуса стоят дорого\cite{koehn2009statistical}, поскольку они требуют специализированного опыта и часто не существуют для языков с низким уровнем ресурсов.

    Есть несколько подходов к построению оптимального метода обучения. Предлагается использовать рекуррентные нейронные сети c короткой и долгой памятью и нейронные сети, в которых реализовано внимание. В других методах используются нейронные сети, которые осуществляют перевод в два этапа. Такой метода называется Seq2Seq\cite{weiss2017sequence}.

    Данная работа посвящена последнему методу последовательного перевода. Предлагается с помощью первой рекуррентной нейронной сети, основанной на долгой памяти перевести входящую последовательность в вектор, а с помощью второй перевести этот вектор в выходную последовательность на нужном нам языке\cite{cho2014properties}. Данный метод позволяет гораздо быстрее обучить нейронную сеть переводу с одного языка на другой, в связи с использованием ей предыдущего опыта и наличию у нее памяти и внимания. Проверка и анализ метода проводятся с помощью алгоритма BLEU(Bilingual evaluation understudy) для проверки качества текста, переведенного с одного языка на другой на паре языков русский-украинский.


\section{Постановка задачи}
    Во время обучения нет параллельных пар предложений. Предполагаем, что нам подойдет модель, отображающая предложения из обоих языков в одно общеее векторное пространство.

    Мы будем использовать модель, в которой используются главных юнита: encoder $f$ и decoder $g$. $f$ и $g$  в нашей конкретной модели -  две рекуррентные нейронные сети. Задача $f$  -  отображать предложения в  латентное пространство(сразу для двух языков) и $g$  -  отображать из латентного пространства в предложения(первого языка и второго соответственно).  Введем обозначения: $D^{src} = [s_1^{src},...,s_{m_{src}}^{src}]$, $D^{tgt} = [s_1^{tgt},...,s_{m_{tgt}}^{tgt}]$.

    Для реализации этого метода определим функционалы, которые будут минимизироваться. Чтобы модель не обучилась возвращать в конце цикла исходные данные, необходимо зашумить исходные предложения. Пусть $\sigma(x)$ - результат наложения шума на слово x.  Оптимизировать будем  следующую функцию:

    $$L_{AE} = ||d(e(\sigma(x)))-x||^2$$

    Пусть дана какая-то модель слабого перевода $\hat{g}$. На втором шаге  функция потерь будет иметь вид:

    $$L_{TR} = ||d(e(\hat{g}(e(x))) - x||^2$$

    Пусть дана модель D, различающая скрытые представления векторов предложений из двух языков. На последнем шаге оптимизируем дискримантор, чтобы он различал представления векторов разных языков в скрытом пространстве:

    $$L_{ADV} = \log p(lang = src| Encoder(x)) + \log p(lang = tgt|Encoder(y))$$

    В итоге нужно минимизировать следующую функцию:

    $$L = a*L_{AE} + b*L_{TR}+c*L{ADV} \longrightarrow min$$

    здесь $a,b,c$ - калибруемые гиперпараметры.

\subsection{Гипотеза}
   Пусть $\mathcal{S}^{l}$ -- множество всех предложений на языке $l \in \{\text{src}, \text{tgt}\}$. Выдвигается гипотеза о том, что существует единое латентное пространство $\mathcal{L}$ и отображения $\textbf{\textit{f}}^{l}: \mathcal{S}^{l} \to \mathcal{L}$ и $\textbf{\textit{g}}^{l}: \mathcal{L} \to \mathcal{S}^{l}$ такие, что
   \begin{itemize}
      \item для $\forall l_1, l_2 \in \{\text{src}, \text{tgt}\}$ и $\forall s^{l_1} \in \mathcal{S}^{l_1}$ $\textbf{\textit{g}}^{l_2}(\textbf{\textit{f}}^{l_1}(s^{l_1}))$ совпадает с $s^{l_1}$, если $l_1$ совпадает с $l_2$, и является корректным переводом $s^{l_1}$, если $l_1$ и $l_2$ различаются
      %\item для $\forall l \in \{\text{src}, \text{tgt}\}$ и $\forall \mathbf{s}^{l} \in \mathcal{S}^{l}$ $\textbf{\textit{g}}^{l}(\textbf{\textit{f}}^{l}(\mathbf{s}^{l})) = \mathbf{s}^{l}$

      %\item для $\forall (l_1, l_2) \in \{(\text{src}, \text{tgt}), (\text{tgt}, \text{src})\}$ $\forall \mathbf{s}^{l_1} \in \mathcal{S}^{l_1}$ $\textbf{\textit{g}}^{l_2}(\textbf{\textit{f}}^{l_1}(\mathbf{s}^{l_1}))$ является корректным переводом $s^{l_1}$ на язык $l_2$

      \item распределения образов $\textbf{\textit{f}}^{\text{src}}(\mathcal{S}^{\text{src}})$ и $\textbf{\textit{f}}^{\text{tgt}}(\mathcal{S}^{\text{tgt}})$ совпадают
   \end{itemize}

\subsection{Описание метода}
   Предлагаемый метод заключается в том, чтобы для каждого $l \in \{\text{src}, \text{tgt}\}$  моделировать отображения $\textbf{\textit{f}}^{l}$ и $\textbf{\textit{g}}^{l}$ кодировщиком $\textbf{f}^{l}$ и декодировщиком $\textbf{g}^{l}$ соответственно. Таким образом, моделью перевода является композиция $\textbf{g}^{\text{tgt}} \circ \textbf{f}^{\text{src}}$.

   Оптимизация проводится следующим образом. Функция ошибки содержит три слагаемых, которые соответствуют сделанным предположениям об отображениях $\textbf{\textit{f}}^{l}$ и $\textbf{\textit{g}}^{l}$.
   \paragraph{Ошибка восстановления} Для каждого $l \in \{\text{src}, \text{tgt}\}$ рассматривается входное предложение $\mathbf{s}^{l}$ на языке $l$. Ошибки считаются между $\mathbf{s}^{l}$ и его образом при отображении $\textbf{g}^{l} \circ \textbf{f}^{l}$.

   \paragraph{Ошибка перевода} Рассматривается предложение $\mathbf{s}^{\text{src}}$ без ограничения общности на языке src. В качестве входного предложения используется $\mathbf{s}^{\text{tgt}}$ -- перевод $\mathbf{s}^{\text{src}}$, полученный с помощью некоторой слабой модели перевода $M^0$. Ошибка перевода с языка tgt на язык src считается между $\mathbf{s}^{\text{src}}$ и образом $\mathbf{s}^{\text{tgt}}$ при отображении $\textbf{g}^{\text{src}} \circ \textbf{f}^{\text{tgt}}$. Аналогичным образом считается ошибка перевода с языка src на язык tgt. В качестве $M^0$ используется пословный переводчик на основе предобученных векторных представлений слов $\mathcal{E}^{\text{src}} = \{\textbf{x}_i\}_{i=1}^{n_{\text{src}}}$ и $\mathcal{E}^{tgt} = \{\textbf{y}_i\}_{i=1}^{n_{\text{tgt}}}$. Переводом слова $\textbf{x} \in \mathcal{E}^{\text{src}}$ является $\textbf{y} = {\arg\min}_{\textbf{y} \in  \mathcal{E}^{\text{tgt}}} \rho(\textbf{x}, \textbf{y})$, где $\rho$ -- косинусное расстояние (перевод слов языка $\text{tgt}$ осуществляется аналогично).

   \paragraph{Штраф за различие распределений} Вводится дискриминатор $\textbf{d}$, который решает задачу классификации векторов $\textbf{h}^l$ латентного пространства $\mathcal{L}$ на классы $\{0, 1\}$: $\textbf{h}^l \in 0 \Leftrightarrow l = \text{src}$. Векторы $\textbf{h}^l$ получаются кодировщиком $\textbf{f}^{l}$, в качестве промежуточного результата при отображениях входных предложений. В функцию ошибки добавляется штраф за то, что дискриминатор точно определяет язык входного предложения. Таким образом параметры модели оптимизируются таким образом, чтобы усложнить задачу дискриминатору. В свою очередь параметры дискриминатора оптимизируются параллельно параметрам модели. Соревновательный процесс оптимизации мотивирован желанием добиться сходства распределений латентных векторов для разных языков.

\subsection{Детали метода}
   Введем словарь $V^{\text{src}}$, содержащий проиндексированные слова, встречающиеся в предложениях из $\mathfrak{D}^{\text{src}}$ и переводах предложений из  $\mathfrak{D}^{\text{tgt}}$, получаемых с помощью $M^0$. Аналогично введем $V^{\text{tgt}}$.

   Предложения на разных языках, использующиеся при оптимизации кодируются с помощью соответствующих словарей. Входные предложения зашумляются преобразованием $\sigma$: сначала с некоторой вероятностью $q$ из них удаляется каждое слово, а затем производится случайная перестановка оставшихся слов с условием, что слово не может оказаться дальше чем на $k$ позиций от своей начальной позиции ($q$, $k$ -- гиперпараметры).

   Кодировщик $\textbf{f}^{l}$ включает в себя слой векторного представления слов $\textbf{e}^{l}$, параметры которого инициализируются векторами $\mathcal{E}^{l}$, и реккурентную нейронную сеть $\textbf{r}^{\text{enc}}$. Декодировщик $\textbf{g}^{l}$ включает в себя $\textbf{e}^{l}$, $\textbf{r}^{\text{dec}}$ и классификатор $\textbf{c}^{l}$, который решает задачу многоклассовой классификации выходов $\textbf{r}^{\text{dec}}$ на классы, соотвествующие словам в словаре $V^{l}$ (отображает выходы $\textbf{r}^{\text{dec}}$ в векторы вероятностей размерности $|V^{l}|$). Параметры $\textbf{r}^{\text{enc}}$ и $\textbf{r}^{\text{dec}}$ общие для $\textbf{f}^{\text{src}}$, $\textbf{f}^{\text{tgt}}$ и $\textbf{g}^{\text{src}}$, $\textbf{g}^{\text{tgt}}$ соответственно.

   В качестве меры ошибок классификации используется кросс-энтропия $CE$. Итоговый вид функции ошибки

   $$
   L_{\text{tran}} = w_1 \cdot \sum_{l \in \\ \{src, tgt\}} CE(\textbf{g}^{l}(\textbf{f}^{l}(\sigma(\mathbf{s}^{l}))), \mathbf{s}^{l})\ +
   $$
   $$
   +\ w_2 \cdot \sum_{l_1 \ne l_2 \in \\ \{src, tgt\}} CE(\textbf{g}^{l_2}(\textbf{f}^{l_1}(\sigma(\mathbf{s}^{l_1}))), \mathbf{s}^{l2}) +
   w_3 \cdot \sum_{l \in \\ \{src, tgt\}} CE(\textbf{d}(\textbf{f}^{l}(\sigma(\mathbf{s}^{l}))), \mathbb{I}[l = \text{src}])
   $$

   $$
   L_{\text{disc}} = \sum_{l \in \{src, tgt\}} CE(\textbf{d}(\textbf{f}^{l}(\sigma(\mathbf{s}^{l}))), \mathbb{I}[l = \text{tgt}]).
   $$

\bibliography{references}
\bibliographystyle{plain}

%\linenumbers

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
