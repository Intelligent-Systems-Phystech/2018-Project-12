\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\NOREVIEWERNOTES
\title
    [Обучение машинного перевода без параллельных текстов] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Обучение машинного перевода без параллельных текстов}
\author
    [Скиднов~Е.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Скиднов~Е.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление
    [Скиднов~Е.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Работа выполнена при финансовой поддержке РФФИ, проект \No\,00-00-00000.
   Научный руководитель:  Стрижов~В.\,В.
   Авторы: А.В. Грабовой, О.Ю. Бахтеев, В.В. Стрижов, Eric Gaussier, координатор Малиновский Г.С.
    Консультант:  Бахтеев~О.\,Ю.}
%\email
%    {author@site.ru}
\organization
     {$^1$Московский физико-технический институт\par
      $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}

\abstract
    {Данная задача посвящена исследованию алгоритма обучения машинного перевода без параллельных предложений. Использование параллельных текстов для задачи машинного перевода требует слишком большой базы предложений всех переводимых языков, что является ресурсоемкой задачей для некоторых пар непохожих языков. Особенностью исследуемого алгоритма является то, что для перевода используется кодировние и декодирование текста во внутреннем представлении. Данный алгоритм использует 2 модели нейронной сети Seq2Seq для перевода с одного языка на другой и обратно. Цель данного исследования заключается в том, чтобы сделать векторы скрытых пространств этих двух моделей как можно более близкими. Для демонстрации работоспособности метода используется вычислительный эксперимент машинного перевода между двумя похожими языками: русским и украинским.

    \bigskip
    \textbf{Ключевые слова}: \emph {машинный перевод, нейросеть, Seq2Seq}.}
\begin{document}
\maketitle
\section{Введение}
    Благодаря недавним достижениям в области глубокого обучения и наличию крупномасштабных параллельных корпусов, машинный перевод достиг впечатляющей производительности на нескольких языковых парах. Тем не менее, эти модели работают очень хорошо, только если они снабжены огромным количеством параллельных данных в порядке миллионов параллельных предложений. К сожалению, параллельные корпуса стоят дорого\cite{koehn2009statistical}, поскольку они требуют специализированного опыта и часто не существуют для языков с низким уровнем ресурсов.

    Есть несколько подходов к построению оптимального метода обучения. Предлагается использовать рекуррентные нейронные сети c короткой и долгой памятью и нейронные сети, в которых реализовано внимание. В других методах используются нейронные сети, которые осуществляют перевод в два этапа. Такой метода называется Seq2Seq\cite{weiss2017sequence}.

    Данная работа посвящена последнему методу последовательного перевода. Предлагается с помощью первой рекуррентной нейронной сети, основанной на долгой памяти перевести входящую последовательность в вектор, а с помощью второй перевести этот вектор в выходную последовательность на нужном нам языке\cite{cho2014properties}. Данный метод позволяет гораздо быстрее обучить нейронную сеть переводу с одного языка на другой, в связи с использованием ей предыдущего опыта и наличию у нее памяти и внимания. Проверка и анализ метода проводятся с помощью алгоритма BLEU(Bilingual evaluation understudy) для проверки качества текста, переведенного с одного языка на другой на паре языков русский-украинский.


\section{Постановка задачи}
    Во время обучения нет параллельных пар предложений. Предполагаем, что нам подойдет модель, отображающая предложения из обоих языков в одно общеее векторное пространство.

    Мы будем использовать модель, в которой используются главных юнита: encoder $f$ и decoder $g$. $f$ и $g$  в нашей конкретной модели -  две рекуррентные нейронные сети. Задача $f$  -  отображать предложения в  латентное пространство(сразу для двух языков) и $g$  -  отображать из латентного пространства в предложения(первого языка и второго соответственно).  Введем обозначения: $D^{src} = [s_1^{src},...,s_{m_{src}}^{src}]$, $D^{tgt} = [s_1^{tgt},...,s_{m_{tgt}}^{tgt}]$.

    Для реализации этого метода определим функционалы, которые будут минимизироваться. Чтобы модель не обучилась возвращать в конце цикла исходные данные, необходимо зашумить исходные предложения. Пусть $\sigma(x)$ - результат наложения шума на слово x.  Оптимизировать будем  следующую функцию:

    $$L_{AE} = ||d(e(\sigma(x)))-x||^2$$

    Пусть дана какая-то модель слабого перевода $\hat{g}$. На втором шаге  функция потерь будет иметь вид:

    $$L_{TR} = ||d(e(\hat{g}(e(x))) - x||^2$$

    Пусть дана модель D, различающая скрытые представления векторов предложений из двух языков. На последнем шаге оптимизируем дискримантор, чтобы он различал представления векторов разных языков в скрытом пространстве:

    $$L_{ADV} = \log p(lang = src| Encoder(x)) + \log p(lang = tgt|Encoder(y))$$

    В итоге нужно минимизировать следующую функцию:

    $$L = a*L_{AE} + b*L_{TR}+c*L{ADV} \longrightarrow min$$

    здесь $a,b,c$ - калибруемые гиперпараметры.

\bibliography{references}
\bibliographystyle{plain}

%\linenumbers

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
