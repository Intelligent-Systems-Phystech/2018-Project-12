\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\usepackage{amsmath}

%\NOREVIEWERNOTES
\title[Обучение машинного перевода без параллельных текстов]{
   Обучение машинного перевода без параллельных текстов
}

\author[Гончаров$^1$~М.\,Ю., Бахтеев$^1$~О.\,Ю., Стрижов$^2$~В.\,В.]{
   Гончаров$^1$~М.\,Ю., Бахтеев$^1$~О.\,Ю., Стрижов$^2$~В.\,В.
}[Гончаров$^1$~М.\,Ю., Бахтеев$^1$~О.\,Ю., Стрижов$^2$~В.\,В.]
 
\thanks{
   Научный руководитель:  Стрижов~В.\,В. 
   Авторы: А.В. Грабовой, О.Ю. Бахтеев, В.В. Стрижов, Eric Gaussier, координатор Малиновский~Г.\,С.
   Консультант:  Бахтеев~О.\,Ю.
}
   
\email{
   goncharov.myu@phystech.edu
}

\organization{
   $^1$Московский физико-технический институт \par
   $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН
}

\abstract{
    В данной работе исследуется метод обучения без учителя для решения задачи машинного перевода. Метод основан на нейросетевой модели seq2seq, в которой кодировщик отображает предложение в вектор латентного пространства, а декодировщик, используя этот вектор, восстанавливает исходное предложение на любом из двух языков. Процесс оптимизации параметров нейросети построен таким образом, чтобы избежать необходимости использовать параллельные данные. Качество работы метода проверяется на паре языков русский-украинский.
    
   \bigskip
   \textbf{Ключевые слова}: \emph {машинный перевод, seq2seq, обучение без учителя}.
}

%\abstract{
%   В последние годы с помощью нейросетевых методов были достигнуты существенные результаты в решении задачи машинного перевода. До недавнего времени все методы были применимы при наличии большого числа параллельных данных. В этом году был предложен метод, в котором процедура оптимизации параметров нейросети использует только данные из произвольных корпусов текста на каждом из языков. Метод основан на модели seq2seq, в которой кодировщик отображает предложение в вектор скрытого пространства, а декодировщик восстанавливает исходное предложение на любом из двух языков. В данной работе проверяется качество работы этого метода на паре языков русский-украинский.
%    
%   \bigskip
%   \textbf{Ключевые слова}: \emph {машинный перевод, модель seq2seq, обучение без учителя}.
%}

\begin{document}
\maketitle

\section{Введение}
   %Задача машинного перевода заключается в том, чтобы автоматически переводить текст с одного языка на другой. Существует большое количество подходов к решению этой задачи. Идея статистического подхода состоит в том, чтобы считать $\textbf{y} =  {\arg\max}_{\textbf{y}}\ p(\textbf{y} \mid \textbf{x})$ правильным переводом предложения $\textbf{x}$, где $p$ -- неизвестное распределение, которое может быть смоделированно распределением $p_{\theta}$ из некоторого параметрического семейства, параметры которого можно найти методом наибольшего правдоподобия при наличии выборки из параллельных предложений $\{\textbf{x}_i, \textbf{y}_i\}_{i=1}^n$
   %\begin{equation} \label{mle}
   %\hat{\theta} = \underset{\theta}{\arg\max} \sum_{i =1}^n \log p_{\theta}(\textbf{y}_i \mid \textbf{x}_i).
   %\end{equation}
   %Нейросетевой машинный перевод использует для аппроксимации неизвестного распределения нейросетевую модель seq2seq, которая чаще всего состоит из кодировщика и декодировщика, которые, в свою очередь, в большинстве случаев представляют из себя реккурентные нейронные сети \cite{sutskever2014sequence, cho2014properties}. Кодировщик получает на вход предложение на одном из языков и отображает его в вектор латентного пространства. Декодировщик, получая на вход этот вектор, пошагово восстанавливает исходное предложение на другом языке. В последние годы, после появления механизма attention \cite{bahdanau2014neural, luong2015effective}, с помощью модели seq2seq были достигнуты существенные результаты для пар языков, для которых есть достаточное количество параллельных данных (порядка миллиона предложений). К сожалению, для того чтобы собрать такое количество качественных размеченных данных требуется большое количество экспертов, что не всегда возможно. Поэтому были предложены методы, позволяющие полностью или частично отказаться от использования параллельных данных. Одним из них является метод, предложенный Lample и др. \cite{lample2017unsupervised}. Его идея заключается в следующем. Предположим, что мы имеем выборку $\{\textbf{y}_i\}_{i=1}^n$ из качественных предложений на одном из языков. Тогда в качестве $\textbf{x}_i$ в \eqref{mle} будем использовать некачественный перевод предложения $\textbf{y}_i$, полученный с помощью простого переводчика (например обученного без учителя пословного переводчика \cite{conneau2017word}). Таким образом, нейросеть будет решать задачу обратного перевода с исправлением ошибок. В данной работе изложены детали этого метода и проводится вычислительный эксперимент, в ходе которого проверяется применимость метода к паре языков русский-украинский. Качество перевода оценивается с помощью метрики BLEU \cite{papineni2002bleu}.
   Задача машинного перевода заключается в том, чтобы автоматически переводить текст с одного языка на другой. Существует ряд подходов к решению этой задачи. Идея статистического подхода состоит в том, чтобы рассматривать задачу перевода как задачу машинного обучения. Перевод предложения рассматривается как случайная величина с неизвестным распределением, которое моделируется распределением из некоторого параметрического семейства, параметры которого можно найти методом наибольшего правдоподобия при наличии выборки из параллельных предложений. Нейросетевой машинный перевод использует для аппроксимации неизвестного распределения нейросетевые модели, которые чаще всего состоят из кодировщика и декодировщика, которые в большинстве случаев представляют из себя реккурентные нейронные сети \cite{sutskever2014sequence, cho2014properties}. Кодировщик получает на вход предложение на одном из языков и отображает его в вектор латентного пространства. Декодировщик, получая на вход этот вектор, пошагово восстанавливает исходное предложение на другом языке. Такая модель получила название seq2seq. В последние годы, после появления механизма attention, с помощью модели seq2seq были достигнуты существенные результаты для пар языков, для которых есть достаточное количество параллельных данных (порядка миллиона предложений) \cite{bahdanau2014neural, luong2015effective}. 
   
   К сожалению, для некоторых пар языков, например когда один из них относительно мало используется, собрать такое количество качественных размеченных данных может быть сложно или невозможно. Поэтому были предложены методы, позволяющие полностью или частично отказаться от использования параллельных данных. Одним из них является метод, предложенный в \cite{lample2017unsupervised}. Его идея заключается в следующем. Предположим, что мы имеем выборку из качественных предложений на одном из языков. Тогда при оптимизации параметров нейросети будем использовать предложения из выборки, в качестве целевой переменной, а на вход подавать их некачественные переводы, полученные с помощью простого переводчика, например обученного без учителя пословного переводчика \cite{conneau2017word}. Таким образом, нейросеть будет решать задачу обратного перевода с исправлением ошибок.
   
   В данной работе изложены детали этого метода и проводится вычислительный эксперимент, в ходе которого проверяется применимость метода к паре языков русский-украинский. Качество перевода оценивается с помощью метрики BLEU \cite{papineni2002bleu}.

\section{Постановка задачи}
   Даны обучающая выборка $\mathfrak{D}^{\text{src}} = \{\mathbf{s}_1^{\text{src}}, \dots, \mathbf{s}_{m_\text{src}}^{\text{src}}\}$, $\mathfrak{D}^{\text{tgt}} = \{\mathbf{s}_1^{\text{tgt}}, \dots, \mathbf{s}_{m_\text{tgt}}^{\text{tgt}}\}$, которая состоит из двух корпусов произвольных предложений для каждого языка; и валидационная выборка $\mathfrak{D}^{valid} = \{(\mathbf{\tilde{s}}_1^{\text{src}}, \mathbf{\tilde{s}}_1^{\text{tgt}}), \dots, (\mathbf{\tilde{s}}_{m_{\text{valid}}}^{\text{src}}, \mathbf{\tilde{s}}_{m_{\text{valid}}}^{\text{tgt}})\}$, которая состоит из пар параллельных предложений.
   
   Задача состоит в том, чтобы, используя обучающую выборку, построить модель $M$ перевода предложений c языка src на язык tgt и выбрать ее параметры таким образом, чтобы максимизировать среднее значение метрики BLEU на валидационной выборке 
   $$
   \frac{1}{m_{\text{valid}}} \sum_{i=1}^{m_{\text{valid}}} \text{BLEU}(M(\mathbf{\tilde{s}}_i^{\text{src}}), \mathbf{\tilde{s}}_i^{\text{tgt}}) \to \max_{M}
   $$ 

\subsection{Гипотеза}
   Пусть $\mathcal{S}^{l}$ -- множество всех предложений на языке $l \in \{\text{src}, \text{tgt}\}$. Выдвигается гипотеза о том, что существует единое латентное пространство $\mathcal{L}$ и отображения $\textbf{\textit{f}}^{l}: \mathcal{S}^{l} \to \mathcal{L}$ и $\textbf{\textit{g}}^{l}: \mathcal{L} \to \mathcal{S}^{l}$ такие, что 
   \begin{itemize}
      \item для $\forall l_1, l_2 \in \{\text{src}, \text{tgt}\}$ и $\forall s^{l_1} \in \mathcal{S}^{l_1}$ $\textbf{\textit{g}}^{l_2}(\textbf{\textit{f}}^{l_1}(s^{l_1}))$ совпадает с $s^{l_1}$, если $l_1$ совпадает с $l_2$, и является корректным переводом $s^{l_1}$, если $l_1$ и $l_2$ различаются
      %\item для $\forall l \in \{\text{src}, \text{tgt}\}$ и $\forall \mathbf{s}^{l} \in \mathcal{S}^{l}$ $\textbf{\textit{g}}^{l}(\textbf{\textit{f}}^{l}(\mathbf{s}^{l})) = \mathbf{s}^{l}$
      
      %\item для $\forall (l_1, l_2) \in \{(\text{src}, \text{tgt}), (\text{tgt}, \text{src})\}$ $\forall \mathbf{s}^{l_1} \in \mathcal{S}^{l_1}$ $\textbf{\textit{g}}^{l_2}(\textbf{\textit{f}}^{l_1}(\mathbf{s}^{l_1}))$ является корректным переводом $s^{l_1}$ на язык $l_2$
      
      \item распределения образов $\textbf{\textit{f}}^{\text{src}}(\mathcal{S}^{\text{src}})$ и $\textbf{\textit{f}}^{\text{tgt}}(\mathcal{S}^{\text{tgt}})$ совпадают
   \end{itemize}

\subsection{Описание метода}
   Предлагаемый метод заключается в том, чтобы для каждого $l \in \{\text{src}, \text{tgt}\}$  моделировать отображения $\textbf{\textit{f}}^{l}$ и $\textbf{\textit{g}}^{l}$ кодировщиком $\textbf{f}^{l}$ и декодировщиком $\textbf{g}^{l}$ соответственно. Таким образом, моделью перевода является композиция $\textbf{g}^{\text{tgt}} \circ \textbf{f}^{\text{src}}$. 
   
   Оптимизация проводится следующим образом. Функция ошибки содержит три слагаемых, которые соответствуют сделанным предположениям об отображениях $\textbf{\textit{f}}^{l}$ и $\textbf{\textit{g}}^{l}$.
   \paragraph{Ошибка восстановления} Для каждого $l \in \{\text{src}, \text{tgt}\}$ рассматривается входное предложение $\mathbf{s}^{l}$ на языке $l$. Ошибки считаются между $\mathbf{s}^{l}$ и его образом при отображении $\textbf{g}^{l} \circ \textbf{f}^{l}$.
   
   \paragraph{Ошибка перевода} Рассматривается предложение $\mathbf{s}^{\text{src}}$ без ограничения общности на языке src. В качестве входного предложения используется $\mathbf{s}^{\text{tgt}}$ -- перевод $\mathbf{s}^{\text{src}}$, полученный с помощью некоторой слабой модели перевода $M^0$. Ошибка перевода с языка tgt на язык src считается между $\mathbf{s}^{\text{src}}$ и образом $\mathbf{s}^{\text{tgt}}$ при отображении $\textbf{g}^{\text{src}} \circ \textbf{f}^{\text{tgt}}$. Аналогичным образом считается ошибка перевода с языка src на язык tgt. В качестве $M^0$ используется пословный переводчик на основе предобученных векторных представлений слов $\mathcal{E}^{\text{src}} = \{\textbf{x}_i\}_{i=1}^{n_{\text{src}}}$ и $\mathcal{E}^{tgt} = \{\textbf{y}_i\}_{i=1}^{n_{\text{tgt}}}$. Переводом слова $\textbf{x} \in \mathcal{E}^{\text{src}}$ является $\textbf{y} = {\arg\min}_{\textbf{y} \in  \mathcal{E}^{\text{tgt}}} \rho(\textbf{x}, \textbf{y})$, где $\rho$ -- косинусное расстояние (перевод слов языка $\text{tgt}$ осуществляется аналогично).
   
   \paragraph{Штраф за различие распределений} Вводится дискриминатор $\textbf{d}$, который решает задачу классификации векторов $\textbf{h}^l$ латентного пространства $\mathcal{L}$ на классы $\{0, 1\}$: $\textbf{h}^l \in 0 \Leftrightarrow l = \text{src}$. Векторы $\textbf{h}^l$ получаются кодировщиком $\textbf{f}^{l}$, в качестве промежуточного результата при отображениях входных предложений. В функцию ошибки добавляется штраф за то, что дискриминатор точно определяет язык входного предложения. Таким образом параметры модели оптимизируются таким образом, чтобы усложнить задачу дискриминатору. В свою очередь параметры дискриминатора оптимизируются параллельно параметрам модели. Соревновательный процесс оптимизации мотивирован желанием добиться сходства распределений латентных векторов для разных языков.
   
\subsection{Детали метода}   
   Введем словарь $V^{\text{src}}$, содержащий проиндексированные слова, встречающиеся в предложениях из $\mathfrak{D}^{\text{src}}$ и переводах предложений из  $\mathfrak{D}^{\text{tgt}}$, получаемых с помощью $M^0$. Аналогично введем $V^{\text{tgt}}$. 
   
   Предложения на разных языках, использующиеся при оптимизации кодируются с помощью соответствующих словарей. Входные предложения зашумляются преобразованием $\sigma$: сначала с некоторой вероятностью $q$ из них удаляется каждое слово, а затем производится случайная перестановка оставшихся слов с условием, что слово не может оказаться дальше чем на $k$ позиций от своей начальной позиции ($q$, $k$ -- гиперпараметры).
   
   Кодировщик $\textbf{f}^{l}$ включает в себя слой векторного представления слов $\textbf{e}^{l}$, параметры которого инициализируются векторами $\mathcal{E}^{l}$, и реккурентную нейронную сеть $\textbf{r}^{\text{enc}}$. Декодировщик $\textbf{g}^{l}$ включает в себя $\textbf{e}^{l}$, $\textbf{r}^{\text{dec}}$ и классификатор $\textbf{c}^{l}$, который решает задачу многоклассовой классификации выходов $\textbf{r}^{\text{dec}}$ на классы, соотвествующие словам в словаре $V^{l}$ (отображает выходы $\textbf{r}^{\text{dec}}$ в векторы вероятностей размерности $|V^{l}|$). Параметры $\textbf{r}^{\text{enc}}$ и $\textbf{r}^{\text{dec}}$ общие для $\textbf{f}^{\text{src}}$, $\textbf{f}^{\text{tgt}}$ и $\textbf{g}^{\text{src}}$, $\textbf{g}^{\text{tgt}}$ соответственно.
   
   В качестве меры ошибок классификации используется кросс-энтропия $CE$. Итоговый вид функции ошибки
   
   $$
   L_{\text{tran}} = w_1 \cdot \sum_{l \in \\ \{src, tgt\}} CE(\textbf{g}^{l}(\textbf{f}^{l}(\sigma(\mathbf{s}^{l}))), \mathbf{s}^{l})\ +
   $$
   $$
   +\ w_2 \cdot \sum_{l_1 \ne l_2 \in \\ \{src, tgt\}} CE(\textbf{g}^{l_2}(\textbf{f}^{l_1}(\sigma(\mathbf{s}^{l_1}))), \mathbf{s}^{l2}) +
   w_3 \cdot \sum_{l \in \\ \{src, tgt\}} CE(\textbf{d}(\textbf{f}^{l}(\sigma(\mathbf{s}^{l}))), \mathbb{I}[l = \text{src}])
   $$
   
   $$
   L_{\text{disc}} = \sum_{l \in \{src, tgt\}} CE(\textbf{d}(\textbf{f}^{l}(\sigma(\mathbf{s}^{l}))), \mathbb{I}[l = \text{tgt}]).
   $$
   
    
   
   
   
    


  %Даны обучающая выборка и некоторая стартовая модель $M^0$, с помощью которой можно осуществлять перевод с любого языка. Обучающая выборка состоит из двух корпусов произвольных предложений для каждого языка: $\mathfrak{D}^{\text{src}} = [\mathbf{s}_1^{\text{src}}, \dots, \mathbf{s}_{m_\text{src}}^{\text{src}}]$, $\mathfrak{D}^{\text{tgt}} = [\mathbf{s}_1^{\text{tgt}}, \dots, \mathbf{s}_{m_\text{tgt}}^{\text{tgt}}]$. В качестве $M^0$ мы использовали пословный переводчик на основе предобученных векторных представлений слов $E^{src} = \{x_i\}_{i=1}^{n_{\text{src}}}$ и $E^{tgt} = \{y_i\}_{i=1}^{n_{\text{tgt}}}$. Переводом слова $x \in E^{\text{src}}$ является $y = {\arg\min}_{y \in  E^{\text{tgt}}} \rho(x, y)$, где $\rho$ -- косинусное расстояние. Перевод с языка $\text{tgt}$ осуществляется аналогично. 
  
  %Введем словарь $V^{\text{src}}$, содержащий проиндексированные слова, встречающиеся в предложениях из $\mathfrak{D}^{\text{src}}$ и переводах предложений из  $\mathfrak{D}^{\text{tgt}}$, получаемых с помощью $M^0$, а также слова <<{src-<sos>}>> и <<{src-<eos>}>>. Аналогично введем $V^{\text{tgt}}$. 
  
  %Оптимизируемая модель состоит из четырех seq2seq моделей для переводов с любого языка на любой:  src2src, tgt2tgt, src2tgt и tgt2src; и дискриминатора $\textbf{d}$. Каждая seq2seq модель $\text{lan}_12\text{lan}_2$ состоит из кодировщика $\textbf{f}^{\text{lan}_1}$ и декодировщика $\textbf{g}^{\text{lan}_2}$, где $\text{lan}_1, \text{lan}_2 \in \{\text{src}, \text{tgt}\}$. Кодировщик $\textbf{f}^{\text{lan}}$ включает в себя эмбеддинг-слой $\textbf{e}^{\text{lan}}$, параметры которого инициализируются векторами $E^{\text{lan}}$, и реккурентную нейронную сеть $\textbf{r}^{\text{enc}}$; декодировщик $\textbf{g}^{\text{lan}}$ включает в себя $\textbf{e}^{\text{lan}}$, $\textbf{r}^{\text{dec}}$ и классификатор $\textbf{c}^{\text{lan}}$, который отображает выходы декодировщика в векторы вероятностей размерности  $|V^{\text{lan}}|$, где $\text{lan} \in \{\text{src}, \text{tgt}\}$. Параметры $\textbf{r}^{\text{enc}}$ и $\textbf{r}^{\text{dec}}$ общие для $\textbf{f}^{\text{src}}$, $\textbf{f}^{\text{tgt}}$ и $\textbf{g}^{\text{src}}$, $\textbf{g}^{\text{tgt}}$ соответственно. 
  
  %Процесс оптимизации построен следующим образом. Для пары языков $\text{lan}_1, \text{lan}_2 \in \{\text{src}, \text{tgt}\}$ и предложения $s^{\text{lan}_2}$ из обучающей выборки, которое выступает в роли целевой переменной, рассматривается входное предложение $s^{\text{lan}_1}$, которое или совпадает с целевым в случае, если $\text{lan}_1$ совпадает с $\text{lan}_2$ или получается из него с помощью $M^0$, если языки различаются. Входное и целевое предложения кодируются с помощью соответствующих словарей. Входное предложение зашумляется преобразованием $\sigma$: сначала с некоторой вероятностью $p$ из него удаляется каждое слово, а затем производится случайная перестановка оставшихся слов с условием, что слово не может оказаться дальше чем на $k$ позиций от своей начальной позиции ($p$, $k$ -- гиперпараметры). Зашумленное входное предложение подается на вход кодировщику $\textbf{f}^{\text{lan}_1}$, который преобразует его в последовательность скрытых состояний. Она подается на вход дискриминатору $\textbf{d}$, который решает задачу классификации входного языка. Идея использовать дискриминатор заключается в том, чтобы оптимизировать параметры кодировщиков, таким образом чтобы увеличить ошибку дискриминатора, что влечет сближение распределений латентных векторов для разных языков. Декодировщик $\textbf{g}^{\text{lan}_2}$ генерирует выходное предложение пошагово. На первом шаге он получает на вход последнее скрытое состояние кодировщика и индекс слова <<{$\text{lan}_2$-<sos>}>>. На последующих шагах он получает на вход предыдущее скрытое состояние и с вероятностями $q$ и $1-q$ индекс предыдущего сгенерированного слова или индекс соответствующего слова из целевого предложения ($q$ -- гиперпараметр). На этапе оптимизации количество шагов определяется длиной целевого предложения, а после окончания оптимизации процесс генерации происходит до тех пор, пока не будет сгенерирован индекс слова <<{$\text{lan}_2$-<eos>}>>. На каждом шаге декодировщик использует все скрытые состояния кодировщика в механизме attention и возвращает новое скрытое состояние и вектор вероятностей для нового слова. В качестве лосс-функций для seq2seq моделей и для дискриминатора используется кросс-энтропия. 
  
  %Таким образом, слагаемое в лосс-функции для seq2seq моделей, соответствующее переводу с $\text{lan}_1$ на $\text{lan}_2$
  %$$
  %L^{\text{lan}_12\text{lan}_2}_{\text{main}} = CE(\textbf{g}^{\text{lan}_2}(\textbf{f}^{\text{lan}_1}(\sigma(s^{\text{lan}_1}))), s^{\text{lan}_2}) + CE(\textbf{d}(\textbf{f}^{\text{lan}_1}(\sigma(s^{\text{lan}_1}))), \text{lan}_2),
  %$$
  %а слагаемое в лосс-функции для дискриминатора
  %$$
  %L^{\text{lan}_12\text{lan}_2}_{\text{disc}} = CE(\textbf{d}(\textbf{f}^{\text{lan}_1}(\sigma(s^{\text{lan}_1}))), \text{lan}_1).
  %$$
  %Итоговые лосс-функции
  %$$
  %L_{\text{main}} = \sum_{\text{lan}_1, \text{lan}_2 \in \\ \{src, tgt\}} L^{\text{lan}_12\text{lan}_2}_{\text{main}}, \quad
  %L_{\text{disc}} = \sum_{\text{lan}_1, \text{lan}_2 \in \{src, tgt\}} L^{\text{lan}_12\text{lan}_2}_{\text{disc}}.
  %$$
  
  
   

\bibliography{references}
\bibliographystyle{ieeetr}

%\linenumbers

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
