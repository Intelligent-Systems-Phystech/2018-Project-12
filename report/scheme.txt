1. Какие выборки у нас есть. D_src, D_tgt --- неразмеченные  D^valid_src, D^valid_src --- есть информация о переводе из одного пердложения в другой.
   1. X - предложения из первого языка
   2. Y - предложения из второго языка


1. Модель состоит из двух рекуррентных сетей (энкодер, декодер)
2. Хотим минимизировать ошибку на валидацинной выборке:
   1. Сумма по всем парам предложений 
   2. Сумма по всем словам внутри предложений
   3. Ошбика предсказания (смотри accuracy).
      1. (не для постановки: вид ошибки спорный, в то же время при минимизации ошибки шумного перевода в Seq2Seq мы используем именно такой вид ошибки)
1. Пишем, что у нас во время обучения нет параллельных пар предложений
2. Выдвиггаем гипотезу о том, что подойдет модель, отображающая предложения из обоих языков в одно общеее векторное пространство
3. Пишем требования к отображению  в общее пространство (в статье есть три слагаемых, приблизительно расписываем):
   1. Пусть sigma --- зашумление предложение (назвать более акукратно). 
      1. L_AE = ||Decoder(Encoder(Sigma(x))) - x||^2 
   1. Пусть дана какая-то модель слабого перевода \hat{g}.
      1. L_TR = ||Decoder(Encoder(hat{g}(Encoder(x)))) -  x) - x||^2 (проверить по статье, что Олег не соврал)
   1. Пусть дана модель D, различающая скрытые представления векторов предложений из двух языков.
      1. L_ADV = log p (язык|Encoder(x)) = log p(язык = src | Encoder(x)) + log p(язык = tgh | Encoder(y))
1. Итоговая функция Ошибки :
   1. L = a * L_AE + b * L_TR + c * L_ADV -> min, где a,b,c --- калибруемые гиперпараметры.


https://github.com/Intelligent-Systems-Phystech/2018-Project-12/blob/master/report/task12_notes.pdf