\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\bibliographystyle{utf8gost705u}
%\NOREVIEWERNOTES
\title
    [Обучение машинного перевода без параллельных текстов] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Обучение машинного перевода без параллельных текстов}
\author
    [Ярошенко~А.\,М. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
{Ярошенко~А.\,М. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление
[Ярошенко~А.\,М. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Научный руководитель:  Стрижов~В.\,В. 
   Авторы: А.В. Грабовой, О.Ю. Бахтеев, В.В. Стрижов, Eric Gaussier, координатор Малиновский~Г.\,С.
   Консультант:  Бахтеев~О.\,Ю.}
%\email
%    {author@site.ru}
\organization
{$^1$Московский физико-технический институт\par
	$^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}
\abstract
{В данной работе исследуется задача машинного перевода между двумя языками. Для решения часто используются параллельные предложения, то есть совпадающие по смыслу фразы на двух языках. В работе рассматривается альтернативная модель, не требующая большого количества параллельных предложений. Она состоит из двух нейросетей типа Seq2Seq, имеющих общее скрытое пространство. [Тут добавится что-то от меня]. Для проверки качества модели проводится вычислительный эксперимент по переводу предложений между близкими языками, такими как русский и украинский.

\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, машинный перевод, автокодировщики}.}
\begin{document}
\maketitle

\section{Введение}

В зависимости от специфики пары языков выделяют несколько подходов к машинному переводу. При наличии достаточного числа параллельных предложений(порядка миллиона) использование глубоких нейронных сетей привело к тотальному коллапсу(\cite{zou2013bilingual},\cite{cho2014properties}). 

Но для многих пар языков нет достаточной базы примеров. Одним из подходов на основе параллельных предложений является пополнение обучающей выборки переводами с предыдущих итераций работы нейронной сети(\cite{bertoldi2009domain}). Ниже представлено решение задачи машинного перевода при отсутствии достаточного количества параллельных предложений (\cite{wu2016google}, \cite{sutskever2014sequence}, \cite{bahdanau2014neural}). 
 

Такой подход был использован в \cite{lample2017unsupervised} для пары языков французский-русский. В данной работе будет проведен схожий эксперимент для перевода с русского на украинский. В модели используются 2 типа автокодировщиков: LSTM (\cite{gers1999learning}, \cite{graves2005framewise}), которые реализуют перевод слов в скрытое пространство, и сеть-дискриминатор, определяющая по векторному представлению язык исходного предложения. Сети LSTM тренируются так, чтобы представление одного и того же предложения на разных языках совпадало в скрытом пространстве, то есть, чтобы дискриминатору было сложнее определить язык, к которому относится вектор. Обучение состоит из двух фаз. На первой обучается дискриминатор: предложение кодируется с добавлением шума (\cite{kimimproving}) и подаётся на вход и происходит перераспределение весов. На второй стадии происходит перераспределение весов уже у сетей-кодировщиков. После обновления весов  вычисляется значение функции потерь. Качество переводчика в работе оценивается с помощью метрики BLEU(\cite{papineni2002bleu}).



\bibliography{references}

%\linenumbers

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
