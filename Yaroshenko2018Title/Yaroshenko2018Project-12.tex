\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\bibliographystyle{utf8gost705u}
%\NOREVIEWERNOTES
\title
    [Обучение машинного перевода без параллельных текстов] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Обучение машинного перевода без параллельных текстов}
\author
    [Ярошенко~А.\,М. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
{Ярошенко~А.\,М. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление
[Ярошенко~А.\,М. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Научный руководитель:  Стрижов~В.\,В. 
   Авторы: А.В. Грабовой, О.Ю. Бахтеев, В.В. Стрижов, Eric Gaussier, координатор Малиновский~Г.\,С.
   Консультант:  Бахтеев~О.\,Ю.}
%\email
%    {author@site.ru}
\organization
{$^1$Московский физико-технический институт\par
	$^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}
\abstract
{В данной работе исследуется задача машинного перевода между двумя языками. Для решения часто используются параллельные предложения, то есть совпадающие по смыслу фразы на двух языках. В работе рассматривается альтернативная модель, не требующая большого количества параллельных предложений. Она использует нейронную сеть типа Seq2Seq, имеющую скрытое пространство. [Тут добавится что-то от меня]. Для проверки качества модели проводится вычислительный эксперимент по переводу предложений между близкими языками, такими как русский и украинский.

\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, машинный перевод, автокодировщики}.}
\begin{document}
\maketitle

\section{Введение}

В зависимости от специфики пары языков выделяют несколько подходов к машинному переводу. При наличии достаточного числа параллельных предложений(порядка миллиона) использование глубоких нейронных сетей привело к получению хороших результатов \cite{zou2013bilingual},\cite{cho2014properties}. 

Но для многих пар языков нет достаточной базы примеров. Одним из подходов на основе параллельных предложений является пополнение обучающей выборки переводами с предыдущих итераций работы нейронной сети \cite{bertoldi2009domain}. 

Ниже представлено решение задачи машинного перевода при отсутствии достаточного количества параллельных предложений \cite{wu2016google}, \cite{sutskever2014sequence}, \cite{bahdanau2014neural}. В модели используются 2 типа автокодировщиков: рекуррентные нейронные сети \cite{gers1999learning}, \cite{graves2005framewise}, которые реализуют перевод слов в скрытое пространство, и сеть-дискриминатор, определяющая по векторному представлению язык исходного предложения. Сети-энкодеры оптимизируются так, чтобы представление одного и того же предложения на разных языках совпадало в скрытом пространстве, то есть, чтобы дискриминатору было сложнее определить язык, к которому относится вектор. Обучение состоит из двух фаз. На первой оптимизируется работа дискриминатора: предложение кодируется с добавлением шума \cite{kimimproving} и подаётся на вход и происходит перераспределение параметров. На второй стадии происходит перераспределение парамтеров уже у сетей-кодировщиков. После проведение этих шагов вычисляется значение функции потерь. 
 
Такой подход был использован в \cite{lample2017unsupervised} для пары языков французский-английский. В данной работе будет проведен схожий эксперимент для перевода с русского на украинский. Качество переводчика в работе оценивается с помощью метрики BLEU \cite{papineni2002bleu}.

\section{Постановка задачи}

В данной задаче в качестве обучающей выборки используются несопоставленные друг другу предложения на обоих языках $D^{src} = [s_1^{src},...,s_{m_{src}}^{src}]$, $D^{tgt} = [s_1^{tgt},...,s_{m_{tgt}}^{tgt}]$, по которым нужно предоставить перевод на другой язык. Также предоставлен корпус параллельных предложений $D^{valid} = \{(s_1^{src},s_1^{tgt}),...,(s_{m_{valid}}^{src},s_{m_{valid}}^{tgt})\}$ для проверки качества перевода.


Предлагается решение в виде модели $\mathbf{M}$ из двух рекуррентых нейронных сетей для реализации декодера $\mathbf{g}$ и энкодера $\mathbf{f}$ и из двуслойного персептрона $\mathbf{discr}$ в качестве дискриминатора. 

В качестве метрики между словами используется NLL. Метрикой качества модели является среднее значение BLEU на валидационной выборке:

$$\dfrac{1}{m_{valid}}\sum_{i=1}^{m_\text{valid}}BLEU(M(s_i^{src}),s_i^{tgt})$$

Так как у нас нет достаточно большого корпуса из параллельных предложений, мы будем использовать следующую схему построения модели. Используется два словаря $V^{src}$ и $V^{tgt}$, которые сопосталяют словам из обоих языков численные векторы. Предварительно по выборке строится нулевое приближение модели - пословный перевод. Для этого каждому слову $\mathbf{x}$ ставится в соответствие его ближайший по косинусной метрике сосед  $\mathbf{y} = argmin_{z}\rho(x,z)$ из векторного представления MUSE.

 При последующих итерациях энкодер будет переводить исходное предложение в скрытое пространство $Z$, общее для обоих языков, где дискриминатор будет по вектору определять, какому языку он принадлежал. Параметры оптимизируются так, чтобы дискриминатор по представлению предложения из языка source определял его как target, то есть чтобы латентные пространства языков были достаточно близки и усложнялась работа дискриминатора. Декодер же преобразует этот вектор из $Z$ в матрицу вероятностей, где для каждого слова $\mathbf{x}$ будет определен вектор вероятностей $\vec{p(x)}$ нехождения на этой позиции того или иного слова из словаря $V^{tgt}$. Параметры у энкодеров и декодеров для обоих языков являются общими, для этих моделей отличается только генерация матрицы вероятностей. 

Помимо этого для избежания переобучения к каждому предложению добавляется шум $\mathbf{\sigma}$: некоторые слова удаляются, а остальные переставляются, но так чтобы результирующая позиция отличалась от исходной не более, чем на фиксированную константу k.

Для реализации этого метода определим функционалы, которые будут минимизироваться. Во-первых, это ошибка восстановления зашумленного предложения в исходный язык, которая считается между входным предложением $\mathbf{s^{src}}$ и его образом $g^{src}(f^{src}(\mathbf{s^{src}}))$. На этом шаге оптимизации будет минимизироваться следующая функция:

$$L_{AE} = \sum\limits_{i=1}^{|s^{src}|}NLL(g^{src}(f^{src}(\sigma(\mathbf{s^{src}})))[i],\mathbf{s^{src}}[i])$$

Далее рассматривается ошибка перевода уже на другой язык:

$$L_{TR} = \sum\limits_{i=1}^{|s^{src}|}NLL(g^{tgt}(f^{src}(\sigma(\mathbf{s^{src}})))[i],\mathbf{s^{src}}[i])$$

И последний этап - оптимизация дискримантора для схожести латентных представлений одного и того же предложения, закодированного с разных языков:

$$L_{ADV} = \log\mathbb{P}{discr(f^{src}(s^{src})) = src}$$

Таким образом, имеем задачу оптимизации:

$$L = a*L_{AE} + b*L_{TR}+c*L_{ADV} \longrightarrow min$$

где a,b,c калибруемые гиперпараметры.

\bibliography{references}

%\linenumbers

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
